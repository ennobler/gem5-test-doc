[
{
	"uri": "http://localhost/getting-started/intro/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " gem5 is a modular discrete event driven computer system simulator platform. That means that:\n gem5\u0026rsquo;s components can be rearranged, parameterized, extended or replaced easily to suit your needs. It simulates the passing of time as a series of discrete events. Its intended use is to simulate one or more computer systems in various ways. It\u0026rsquo;s more than just a simulator; it\u0026rsquo;s a simulator platform that lets you use as many of its premade components as you want to build up your own simulation system.  gem5 is written primarily in C++ and python and most components are provided under a BSD style license. It can simulate a complete system with devices and an operating system in full system mode (FS mode), or user space only programs where system services are provided directly by the simulator in syscall emulation mode (SE mode). There are varying levels of support for executing Alpha, ARM, MIPS, Power, SPARC, and 64 bit x86 binaries on CPU models including two simple single CPI models, an out of order model, and an in order pipelined model. A memory system can be flexibly built out of caches and crossbars. Recently the Ruby simulator has been integrated with gem5 to provide even more flexible memory system modeling. There are many components and features not mentioned here, but from just this partial list it should be obvious that gem5 is a sophisticated and capable simulation platform. Even with all gem5 can do today, active development continues through the support of individuals and some companies, and new features are added and existing features improved on a regular basis.\nCapabilities out of the box gem5 is designed for use in computer architecture research, but if you\u0026rsquo;re trying to research something new and novel it probably won\u0026rsquo;t be able to evaluate your idea out of the box. If it could, that probably means someone has already evaluated a similar idea and published about it. To get the most out of gem5, you\u0026rsquo;ll most likely need to add new capabilities specific to your project\u0026rsquo;s goals. gem5\u0026rsquo;s modular design should help you make modifications without having to understand every part of the simulator. As you add the new features you need, please consider contributing your changes back to gem5. That way others can take advantage of your hard work, and gem5 can become an even better simulator.\n"
},
{
	"uri": "http://localhost/contributing/high-level/",
	"title": "High-level flow",
	"tags": [],
	"description": "",
	"content": "\n+-------------+ | Make change | +------+------+ | | v +------+------+ | Post review | +------+------+ | v +--------+---------+ | Wait for reviews | \u0026lt;--------+ +--------+---------+ | | | | | v | +----+----+ No +------+------+ |Reviewers+---------\u0026gt;+ Update code | |happy? | +------+------+ +----+----+ ^ | | | Yes | v | +----+-----+ No | |Maintainer+----------------+ |happy? | +----+-----+ | | Yes v +------+------+ | Submit code | +-------------+   After creating your change to gem5, you can post a review on our Gerrit code-review site: https://gem5-review.googlesource.com. Before being able to submit your code to the mainline of gem5, the code is reviewed by others in the community. Additionally, the maintainer for that part of the code must sign off on it.\n"
},
{
	"uri": "http://localhost/docs/build-system/",
	"title": "Build System",
	"tags": [],
	"description": "",
	"content": " Build System gem5\u0026rsquo;s build system is based on SCons, an open source build system implemented in Python. You can find more information about scons at http://www.scons.org. The main scons file is called SConstruct and is found in the root of the source tree. Additional scons files are named SConscript and are found throughout the tree, usually near the files they\u0026rsquo;re associated with.\nBuild targets In gem5, scons build targets are of the form //. For example:\nscons build/ARM/gem5.opt\nThe part of the target is a directory path that ends in \u0026ldquo;build\u0026rdquo;. Typically this is simply \u0026ldquo;build\u0026rdquo; by itself (as in the example), but you can specify a directory called \u0026ldquo;build\u0026rdquo; located somewhere else instead. All targets under the same build dir are assumed to be compiled for the same host platform, and share the same global variable settings. These settings are stored in the build/variables.global file.\nThe  part (\u0026ldquo;ARM\u0026rdquo; in the example above) selects a set of compile-time build options that control simulator functionality, such as the ISA used, which CPU models are included, which Ruby coherence protocol to use, etc. These per configuration variables are stored in separate files in the build/variables directory. This location makes it possible to clean out a build by deleting the configuration directory completely (e.g., using \u0026lsquo;rm -rf\u0026rsquo;) without losing the variable settings.\nThe first time a particular configuration is built, the configuration name is also used to determine the default per-configuration variable settings by looking for a matching name in the build_opts directory. If you would like to create a new configuration with a name not found there, use the \u0026ndash;default flag to tell scons which file in build_opts or build/variables to use for the default.\nThe build target (\u0026ldquo;gem5.opt\u0026rdquo; in the example above) is typically the name of the gem5 binary to build, which specifies the set of compiler flags used. Currently supported versions are gem5.debug, gem5.opt, gem5.fast, gem5.prof and gem5.perf.\n gem5.debug has optimizations turned off. This ensures that variables won\u0026rsquo;t be optimized out, functions won\u0026rsquo;t be unexpectedly inlined, and control flow will not behave in surprising ways. That makes this version easier to work with in tools like gdb, but without optimizations this version is significantly slower than the others. You should choose it when using tools like gdb and valgrind and don\u0026rsquo;t want any details obscured, but other wise more optimized versions are recommended.   gem5.opt has optimizations turned on and debugging functionality like asserts and DPRINTFs left in. This gives a good balance between the speed of the simulation and insight into what\u0026rsquo;s happening in case something goes wrong. This version is best in most circumstances.   gem5.fast has optimizations turned on and debugging functionality compiled out. This pulls out all the stops performance wise, but does so at the expense of run time error checking and the ability to turn on debug output. This version is recommended if you\u0026rsquo;re very confident everything is working correctly and want to get peak performance from the simulator.   gem5.prof is similar to gem5.fast but also includes instrumentation that allows it to be used with the gprof profiling tool. This version is not needed very often, but can be used to identify the areas of gem5 that should be focused on to improve performance.   gem5.perf also includes instrumentation, but does so using google perftools, allowing it to be profiled with google-pprof. This profiling version is complementary to gem5.prof, and can probably replace it for all Linux-based systems.  These versions are summarized in the following table.\n   Binary name Optimizations Run time debugging support Profiling support     gem5.debug  X    gem5.opt X X    gem5.fast X     gem5.prof X  X   gem5.perf X  X    Command line options Scons will recognize the following command line options specific to gem5.\n   Option Effect     --colors Turn on colorized output   --no-colors Turn off colorized output   --default Override which existing build configuration or build_opts file to use for defaults   --ignore-style Disable style checking hooks   --update-ref Update test reference outputs   --verbose Print full tool command lines    Environment variables The following environment variables are imported from the host environment for use in scons:\n   Variable Use     AS Assembler command   AR Archive tool command   CC C compiler command   CXX C++ compiler command   HOME User\u0026rsquo;s home directory   LD_LIBRARY_PATH Path to search for library files at loading time   LIBRARY_PATH Path to search for library files at linking time   PATH Path to search for programs   PROTOC protobuf compiler command   PYTHONPATH Path to search for python files   RANLIB Ranlib command   M5_CONFIG Where to look for the special \u0026ldquo;.m5\u0026rdquo; directory   M5_DEFAULT_BINARY The default build target which overrides the default default build/ALPHA/gem5.debug    Configuration variables These configuration variables are used to control the way gem5 is built. Some are global, affecting all configurations in a build directory, and some only affect the configuration being built. Unlike the command line options, these variables retain their value between invocations of scons.\nGlobal    Variable Description Default     CC C Compiler CC environment variable or value determined by scons   CXX C++ Compiler CXX environment variable or value determined by scons   BATCH Use batch pool for build and tests False   BATCH_CMD Batch pool submission command qdo   M5_BUILD_CACHE Cache built objects in this directory False   EXTRAS Add extra directories to the compilation     Per Configuration    Variable Description Default Exported as config/*.hh     CP_ANNOTATE Enable critical path annotation capability False X   CPU_MODELS CPU Models AtomicSimpleCPU,InOrderCPU,O3CPU,TimingSimpleCPU    PROTOCOL Coherence protocol for Ruby MI_example X   SS_COMPATIBLE_FP Make floating-point results compatible with SimpleScalar False X   TARGET_ISA Target ISA: ALPHA, ARM, MIPS, POWER, X86 alpha X   USE_CHECKER Use checker for detailed CPU models False X   USE_FENV Use \u0026lt;fenv.h\u0026gt; IEEE mode control whether fenv.h was found on this host X   USE_POSIX_CLOCK Use POSIX Clocks whether posix clocks are available on this host X    Setting configuration variable values The first way you set configuration variable values is through the configuration name you choose as part of the build target. This file is loaded from build_opts and contains preset values for some of these variables which configures the build as the file name suggests.\nIt is important to note that the values in the file corresponding to the configuration you picked are -default- values and are only used if no directory already exists with its own values already in place. Those files are for defining reasonable starting points to configure gem5 to behave the way you want it to, and are not intended to actively configure a particular build.\nIf you want to change a value after your build and configuration directory is already created, or if you want to override a value as it\u0026rsquo;s created, you can specify the new values on the command line. The syntax is similar to setting environment variables at a shell prompt, but these go after the scons command. For example, to build MESI_CMP_directory protocol for an existing ALPHA build, you could use the following command.\nscons PROTOCOL=MESI_CMP_directory build/ALPHA/gem5.opt\nIt\u0026rsquo;s often a good idea to add \u0026ndash;help to the scons command line which will print out all of the configuration variables and what their values are. This way you can make sure everything is set up like you want, and that you don\u0026rsquo;t have any typos in any variable names. If everything is as you expect, you can remove \u0026ndash;help to actually start the build.\nRunning regressions / Testing Your Build gem5\u0026rsquo;s regression system is built into SCons. That ensures that the gem5 binary is automatically rebuilt if necessary, and that tests are only rerun if they might have different results.\nThe regression targets are all found under \u0026ldquo;tests\u0026rdquo; in the build directory. The components of the path to a tests output files determine which test to run and how to run it. These components are as follows:\ntests///////\nYou can leave out components of the path farther down, and scons will automatically build all available tests that match the components that were specified. To run all \u0026ldquo;quick\u0026rdquo; tests on the \u0026ldquo;opt\u0026rdquo; version of ALPHA configuration of gem5, you can run the following command:\nscons build/ALPHA/tests/opt/quick\nThe regression framework is integrated into the scons build process, so the command above will (re)build ALPHA/gem5.opt if necessary before running the tests. Also thanks to scons\u0026rsquo;s dependence tracking, tests will be re-run only if the binary has been rebuilt since the last time the test was run. If the previous test run is still valid (as far as scons can tell), only a brief pass/fail message will be printed out based on the result of that previous test, rather than the full output and statistics diff that is printed when the test is actually executed.\nRegression tests are subdivided into two categories, \u0026ldquo;quick\u0026rdquo; and \u0026ldquo;long\u0026rdquo;, based on runtime. You can run only the tests in a particular category by adding that category name to the target path, e.g.:\n% scons build/ALPHA/tests/opt/long\nSpecific tests can be run by appending the test name:\n% scons build/ALPHA/tests/opt/quick/fs/10.linux-boot\nFor more details, see Regression Tests.\nA few of the \u0026ldquo;quick\u0026rdquo; category of tests should run without any additional setup. Some tests depend on EIO support which is provided in the \u0026ldquo;encumbered\u0026rdquo; repository and which has to be built into gem5 using the EXTRAS mechanism. Other tests depend on system files like particular disk images and kernels to be found in particular places on your system. These files are frequently available, and with a little effort and those files you should be able to get those tests running. Other tests, typically based on SPEC benchmarks and not part of \u0026ldquo;quick\u0026rdquo;, require input files which have a restrictive license and can\u0026rsquo;t be distributed by us. You won\u0026rsquo;t be able to run these tests unless you have a license and can get these files somehow.\nAdding source files and debug flags Files are added to the build by declaring them inside SConscript files as instances of certain python classes. The build system knows how to handle those files based on what particular class was used. For instance, to add a C++ source file foo.cc to the build, you could add the following line to the SConscript in the same directory as foo.cc:\nSource('foo.cc')\nThe build system finds and processes SConscript files automatically, so you can create one near the files your adding or extend one that\u0026rsquo;s already there. The following table shows what types of source files there are and what they\u0026rsquo;re for.\n   Source file type Description Parameters     Name Description    PySource Add a python source file to the named package package   source The name of the python source file    SimObject Add a SimObject python file as a python source object and add it to a list of sim object modules source   Source Add a c/c++ source file to the build source   Werror Whether to compile with -Werror. Defaults to True.    main This file is part of main() for the gem5 binary. Defaults to False.    skip_lib Whether this file should be excluded from the library version of gem5. Defaults to False.    UnitTest Add a unit test to the build sources    A similar mechanism is used to define debug/trace flags. These look very similar but don\u0026rsquo;t actually refer to a file. A compound trace flag is a flag which controls a group of normal trace flags.\n   Trace flag type Description Parameters     Name Description    DebugFlag Add an individual trace flag to the build. name   desc A description for this flag. This is optional but recommended.    CompoundFlag Add a compound trace flag to the build. name   flags A list or tuple of the names of trace flags which this new compound trace flag will control.    desc A description for this flag. This is optional but recommended.     Using EXTRAS The EXTRAS scons variable can be used to build additional directories of source files into gem5 by setting it to a colon delimited list of paths to these additional directories. EXTRAS is a handy way to build on top of the gem5 code base without mixing your new source with the upstream source. You can then manage your new body of code however you need to independently from the main code base.\n"
},
{
	"uri": "http://localhost/docs/debugging/",
	"title": "Debugging",
	"tags": [],
	"description": "",
	"content": " Introduction The simplest method of debugging is to have gem5 print out traces of what it\u0026rsquo;s doing. The simulator contains many DPRINTF statements that print trace messages describing potentially interesting events. Each DPRINTF is associated with a debug flag (e.g., Bus, Cache, Ethernet, Disk, etc.). To turn on the messages for a particular flag, use the --debug-flags command line argument. Multiple flags can be specified by giving a list of strings, e.g.:\nbuild/ALPHA/gem5.opt --debug-flags=Bus,Cache configs/examples/fs.py  Groups of trace flags can be turned on all at once using composite trace flags. To turn off a trace flag, put a minus in front of it. Negative trace flags can be combined with composite flags to turn on all the flags in a group except with exceptions. For\nexample: build/ALPHA/gem5.opt --debug-flags=Exec,-ExecTicks configs/examples/fs.py  would turn on a group of debug flags related to instruction execution but leave out Tick (timing) information. This is useful if you want to compare execution between two runs where the same instructions execute but at different rates.\nNote that the gem5.fast binary does not support tracing; part of what makes it faster than gem5.opt is that the DPRINTF code is compiled out.\nThe --debug-flags command line option should come after the gem5 executable but before the simulation script. This is because debug flags are handled by gem5 itself, and whether command line options are before or after the simulation script determine if they\u0026rsquo;re for gem5 or the script.\nDebugging Options ----------------- --debug-break=TIME[,TIME] Tick to create a breakpoint --debug-help Print help on debug flags --debug-flags=FLAG[,FLAG] Sets the flags for debug output (-FLAG disables a flag) --debug-start=TIME Start debug output at TIME (must be in ticks) --debug-file=FILE Sets the output file for debug [Default: cout] --debug-ignore=EXPR Ignore EXPR sim objects  The complete list of debug/trace flags can be seen by running gem5 with the --debug-help option.\nIf you find that events of interest are not being traced, feel free to add DPRINTFs yourself. You can add new debug flags simply by adding DebugFlag() command to any SConscript file (preferably the one nearest where you are using the new flag). If you use a debug flag in a C++ source file, you would need to include the header file \u0026quot;debug/.hh\u0026quot; in that file.\nFor more complex bugs, the trace can be useful in simply identifying points in the simulation where more in-depth investigation is needed. The --debug-break option lets you re-run your simulation under a debugger and stop on a particular tick as identified by the trace. You can also schedule breakpoints and enable or disable debug flags from within the debugger itself. See the page on Debugger Based Debugging for more information.\nThe Exec debug flag The Exec compound debug flag is very useful because it turns on instruction tracing in gem5. It makes the simulator print a disassembled version of each instruction as it finishes executing, along with other useful information like the time, pc, the address if it was a memory instruction, etc. These individual pieces of information can be turned on and off with the base debug flags Exec controls. For example, you can disable the use of function symbol names in place of absolute PC addresses (if they\u0026rsquo;re available) by turning off the ExecSymbol flag (e.g., --debug-flags=Exec,-ExecSymbol).\nIf some supposedly innocuous change has caused gem5 to stop working correctly, you can compare trace outputs from before and after the change using the tracediff script in the src/util directory. Comments in the script describe how to use it.\nReducing trace file size Trace file can become very large very quickly, but they also compress very well (e.g. about 90%). If you\u0026rsquo;d like to make gem5 output a compressed trace, just add a .gz extension to the output file name. For example --debug-file=trace.out will produce an uncompressed file as normal, but --debug-file=trace.out.gz will produce a gzip compressed file. You can use the zcat program and pipes to process the output. The editor vim also can uncompress gzip compressed files in memory.\nThe tracediff and rundiff utilities tracediff and rundiff utilities allow the simple diffing of two streams of trace data from gem5 to find any differences. It\u0026rsquo;s very handy for debugging why regression tests fail, figuring out why your minor code change seems to cause some unrelated execution problem, or comparing the execution of CPU models.\nBoth utilities are found in the util directory. rundiff is a simple diff-like program. Unlike regular diff, this script does not read in the entire input before comparing its inputs, so it can be used on lengthy outputs piped from other programs (e.g., gem5 traces). tracediff is a front end for rundiff that provides an easy way to run two similar copies of gem5 and diff their outputs. It takes a common gem5 command line with embedded alternatives and executes the two alternative commands in separate subdirectories with output piped to rundiff.\nScript arguments are handled uniformly as follows:\n If the argument does not contain a \u0026lsquo;|\u0026rsquo; character, it is appended to both command lines. If the argument has a \u0026lsquo;|\u0026rsquo; character in it, the text on either side of the \u0026lsquo;|\u0026rsquo; is appended to the respective command lines. Note that you\u0026rsquo;ll have to quote the arg or escape the \u0026lsquo;|\u0026rsquo; with a backslash so that the shell doesn\u0026rsquo;t think you\u0026rsquo;re doing a pipe or put quotes around it. Arguments with \u0026lsquo;#\u0026rsquo; characters are split at those characters, processed for alternatives (\u0026lsquo;|\u0026rsquo;s) as independent terms, then pasted back into a single argument (without the \u0026lsquo;#\u0026rsquo;s). (Sort of inspired by the C preprocessor \u0026lsquo;##\u0026rsquo; token pasting operator.)  In other words, the arguments should look like the command line you want to run, with \u0026ldquo;|\u0026rdquo; used to list the alternatives for the parts that you want to differ between the two runs.\nFor example:\n % tracediff m5.opt --opt1 '--opt2|--opt3' --opt4 # would compare these two runs: m5.opt --opt1 --opt2 --opt4 m5.opt --opt1 --opt3 --opt4\n% tracediff 'path1|path2#/m5.opt' --opt1 --opt2 # would compare these two runs: path1/m5.opt --opt1 --opt2 path2/m5.opt --opt1 --opt2\nIf you want to add arguments to one run only, just put a \u0026lsquo;|\u0026rsquo; in with text only on one side (\u0026lsquo;\u0026ndash;onlyOn1|\u0026rsquo;). You can do this with multiple arguments together too (\u0026lsquo;|-a -b -c\u0026rsquo; adds three args to the second run only).\nThe \u0026lsquo;-n\u0026rsquo; argument to tracediff allows you to preview the two generated command lines without running them.\nFor tracediff to be useful some trace flags must be enabled. The most common trace flags to use with tracediff are --debug-flags=Exec,-ExecTicks which removes the timestamp from each trace making it suitable to diff when slight timing variations are present.\nTracediff is also useful for comparing CPU models when one fails and the other doesn\u0026rsquo;t. In this case it\u0026rsquo;s best to create a checkpoint before the problem occurs (this can be done by just creating a bunch of checkpoints and finding one that fails). If the failure occurs in kernel code, use the -ExecUser debug flag, on the other hand if it occurs in user code try the -ExecKernel debug flag to isolate user code in the trace. You can then compare the traces and see when the execution diverges.\nComparing traces across machines Sometimes gem5 executions differ inexplicably across different environments, and you\u0026rsquo;d like to use rundiff to help pinpoint where they diverge. Rather than try and reproduce those environments on the same machine, you can use netcat with rundiff to compare traces from gem5 instances running on separate systems across the network.\nFirst, start rundiff running on one machine, configured to compare the trace output from a local instance of gem5 with the output of a netcat \u0026ldquo;server\u0026rdquo;. Since the network is likely to be the bottleneck, we\u0026rsquo;ll compress the trace going across netcat, which means we need to uncompress it as it arrives. For example (choosing port number 33335 arbitrarily):\n util/rundiff 'gem5.opt --debug-flag=Exec  |' 'nc -d -l 33335 | gunzip -c |' \u0026gt;\u0026amp; tracediff.out \u0026amp;\nNow go to the second machine, start a copy of gem5 there, and ship its compressed trace output to the netcat instance running on the first machine. For example:\n gem5.opt --debug-flag=Exec  |\u0026amp; gzip -c |\u0026amp; nc  33335\nInternal Exec tracing implementation (InstTracer) The \u0026ldquo;Trace-based debugging\u0026rdquo; section above talked about how to use the Exec trace flag to print information about each instruction as it completes. That functionality is actually implemented by an InstTracer object which collects information about instructions as the execute. These objects can be swapped out, and different objects can do different things with the information they collect. For instance, the IntelTrace object prints out a trace in a different format which is compatible with an external tool. The objects can also do more than just print a trace. NativeTrace objects send information about architectural state over a socket to the statetrace tool (described below) instruction by instruction to validate execution. InstTracer objects are SimObjects which are assigned to the \u0026ldquo;tracer\u0026rdquo; parameter of each CPU. If you want to install a different tracer, just assign it to that parameter on the CPU of interest.\nWhen writing your own InstTracer, you\u0026rsquo;ll write at least two different classes, one which inherits from InstTracer and one that inherits from InstRecord. The InstTracer class\u0026rsquo;s main responsibility is to generate InstRecord objects which are associated with a particular instruction. By subclassing InstTracer, you\u0026rsquo;ll be able to return your own specialized version of InstRecord which is the class that really does most of the work.\nThe InstRecord class have a number of fields which hold information about the history of an instruction. For instance, InstRecord records the instruction\u0026rsquo;s PC, what address it used if it accessed memory, a \u0026ldquo;data\u0026rdquo; value which it produced (multiple data values aren\u0026rsquo;t handled), etc. The InstRecord function also has a pointer to a ThreadContext which can be used to read out architectural state. When an instruction is finished executing, the InstRecord\u0026rsquo;s \u0026ldquo;dump()\u0026rdquo; virtual function is called to process the record. For the default InstTracer, this is where the instruction\u0026rsquo;s assembly language form, etc., is printed which is the output you see when you turn on Exec. For NativeTrace, this is where architectural state is gathered up to send to statetrace.\nComparing traces with a real machine The statetrace tool runs alongside gem5 and compares execution of a workload on a real machine with execution in gem5. In the simulator and the real system, the workload is allowed to run one instruction at a time. After each instruction, architectural state is collected and compared and any differences are reported. It can be tricky to get it set up and producing useful results (described below), but it\u0026rsquo;s an extremely valuable tool for debugging because it tends to quickly pinpoint exactly where a problem is coming from, likely saving many hours of painful debugging per bug.\nNative Trace In gem5, a NativeTrace InstTracer object (described above) needs to be installed on the CPU that will run the workload of interest. When execution starts, the tracer will wait for the state trace utility to connect to it. Then, after each instruction executes, it uses the ThreadContext pointer in the InstRecord object to gather architectural state from the currently running process. It also reads in architectural state gather by state trace through the connection they established. The two versions of state are compared, and any meaningful differences are reported. The exact makeup of the state and how it should be compared is very ISA dependent on ISA, so each ISA defines its own version of NativeTrace. These specialized classes can handle things like expected differences when registers may become undefined, or situations where execution skips ahead for one reason or another.\nstatetrace utility The statetrace utility is found in the util directory and is responsible for running the workload on the real machine. It uses the ptrace mechanism provided by the Linux kernel to single step the target process and to access its state. It uses scons, but is independent of scons as used by the rest of gem5. To build a version of statetrace suitable for a particular ISA, use the \u0026ldquo;build/${ARCH}/statetrace\u0026rdquo; target where ${ARCH} is replaced by the ISA of interest. Currently recognized values for ${ARCH} are \u0026ldquo;amd64\u0026rdquo;, \u0026ldquo;arm\u0026rdquo;, \u0026ldquo;i686\u0026rdquo;, and \u0026ldquo;sparc\u0026rdquo;. You can override the compiler used for any ISA using the CXX scons argument, and the compiler used for a particular ISA with ${ARCH}CXX. For instance, to build an arm version of statetrace, you could run\ncd util/statetrace scons ARMCXX=arm-softfloat-linux-gnueabi-g++ build/arm/statetrace  statetrace accepts four flags, -h to print the help, \u0026ndash;host to specify what ip and port gem5 is listening at, -i to print out what\u0026rsquo;s on the initial stack frame, and -nt to disable tracing. -nt is typically used with -i to get information about a processes initial stack without running it. The end of the command line options is marked with two dashes. Next, put the command line you want statetrace to run.\nThe exact text of the program name and arguments matters because these will be passed to the process on its stack. Longer values take up more room on the stack, that displaces other items to different addresses, and statetrace clog up with lots of unimportant differences. For instance, if you need to run a program found in your home directory in a gem5 subdirectory and you run this command:\nstatetrace -- ~/gem5/my_benchmark arg1 arg2  You must also override arg0 in gem5 to be ~/gem5/my_benchmark.\nTuning statetrace is a very sensitive system, and any minor difference between simulated execution and real execution could produce lots and lots of spurious differences. In order to get useful information from statetrace you\u0026rsquo;ll need to adjust the real system and gem5 so that everything lines up perfectly. I normally create a patch which has all the modifications I\u0026rsquo;ve made to gem5 for statetrace. Then I can easily remove them or reapply them for as I find and fix problems. Mercurial queues is useful for managing that patch and patches for my fixes. The following is an incomplete list of the differences you may have to correct.\nAddress randomization. To improve security, Linux will randomize the address space of processes, moving around their stack and heap areas. This makes it harder for an attacker to predict what memory will look like, but it also thoroughly defeats statetrace. To disable it, echo \u0026ldquo;0\u0026rdquo; into /proc/sys/kernel/randomize_va_space. You\u0026rsquo;ll almost certainly need root permissions to do that.\nargv values. Be sure to use -exactly- the same text for each argument to your program in gem5 and on the real system. This includes arg0, the program name.\nFile block size. Glibc uses the block size associated with a file to decide how to buffer it. Different behavior will throw off execution and prevent statetrace from working. You can change the block size gem5 reports in the convertStatBuf and convertStat64Buf functions in src/sim/syscall_emul.hh.\nInitial stack contents. Depending on your version of Linux, the contents of the initial stack may be different. You can use the -i and -nt options to print out the content of the initial stack on the real machine. statetrace attempts to interpret the initial stack so you can more easily see what\u0026rsquo;s on it. You\u0026rsquo;ll need to adjust how gem5 sets up the stack to match your real system. This code is typically in a file called process.cc in the appropriate arch directory. gem5\u0026rsquo;s code has been painstakingly constructed so that it sets up a stack as identically to Linux as possible, but the underlying mechanism would change. Also, Linux puts a collection of auxiliary vectors on the initial stack. These are type, value pairs which let the kernel provide extra information to the process as it starts. From time to time Linux introduces a new type of auxiliary vector and adds it to the stack. You may need to dig into the Linux source and emulate any new entries.\nCaveats Because statetrace is very sensitive to any changes in execution, it can\u0026rsquo;t be used with programs that don\u0026rsquo;t behave in very predictable ways. For instance, if a program reads in a random value from /dev/random and uses that in a calculation (or worse in control flow) then that program can\u0026rsquo;t be used. Less obviously, if the program relies on the system time which is unpredictable, it also can\u0026rsquo;t be used. Generally speaking, many benchmarks try to be very deterministic so that they can be used to generate reproducible data. That makes them work well with statetrace.\nStatetrace can\u0026rsquo;t be used at the operating system level for at least two main reasons. First, no system is implemented or will be implemented in the foreseeable future for single stepping an operating system. Second, real operating systems are not determinstic. Interrupts from hardware devices will almost certainly come in at unpredictable times, some devices will return unpredictable data, and gem5 is much less likely to exactly match the behavior of a system at that level where firmware and other implementation details are non longer abstracted away. Second the amount of state that\u0026rsquo;s relevant at the system level is typically larger than at the user level, especially in complex ISAs like x86. Gathering, comparing, and transporting all that extra state would significantly impact performance.\nNot all implementations of ptrace actually work properly. For instance when I last used statetrace with ARM, certain functions called into a region of memory set up by the kernel which had kernel specific implementations of for various operations. Ptrace relied on software breakpoints which work by replacing the next instruction in the program with one that will trap. Because the region of memory really belonged to the kernel, ptrace couldn\u0026rsquo;t modify it to install a breakpoint. The process \u0026ldquo;escaped\u0026rdquo; single stepped execution and quickly ran to completion, leaving gem5 waiting for an update that never came.\nstatetrace isn\u0026rsquo;t able to track changes to memory. Because memory is very large and there isn\u0026rsquo;t a convenient way to detect modifications to it, statetrace only tracks register based architectural state. If an instruction changes registers correctly but stores the wrong value to memory and/or to the wrong address, that problem may not be detected for many instructions. Fortunately, those sorts of errors are the exception.\nTo compare execution to a real machine, you ideally need to have a real machine at your disposal. It\u0026rsquo;s still quite possible, however, to run statetrace inside an emulator like qemu. That\u0026rsquo;s likely a little slower and compares execution against the emulator and not real hardware, but it can still help identify bugs.\nISA support Currently SPARC, ARM, and x86 support state. ARM\u0026rsquo;s support is currently the most sophisticated, only sending differences in state across the connection which improves performance, and only printing when differences start or stop which reduces output and improves readability. Those features are planned to be ported to the other ISAs. Hopefully that code can be factored out and put into the base NativeTrace class so that all ISAs can use it easily.\n"
},
{
	"uri": "http://localhost/tutorials/isca-2018/",
	"title": "ISCA 2018",
	"tags": [],
	"description": "",
	"content": " AMD gem5 APU Simulator: Modeling GPUs Using the Machine ISA  Held in conjunction with [ISCA 2018](http://iscaconf.org/isca2018/). June 2nd, 2018.  Important Dates The tutorial will be held on day one of the conference - June 2nd, 2018\nISCA 2018 early registration and hotel reservation deadline - April 16th, 2018\nAbstract AMD Research has developed an APU (Accelerated Processing Unit) model that extends gem5 [1] with a GPU timing model that executes the GCN (Graphics Core Next) generation 3 machine ISA [2, 3]. In addition to supporting a modern machine ISA, the model supports running the open-source Radeon Open Compute platform (ROCm) stack without modification. This allows users to run a wide variety of applications written in several high-level languages, including C++, HIP, OpenMP, and OpenCL. This provides researchers the ability to evaluate many different types of workloads, from traditional compute applications to emerging modern GPU workloads, such as task parallel and machine learning applications. The resulting AMD gem5 APU simulator is a cycle-level, flexible research model that is capable of representing many different APU configurations, on-chip cache hierarchies, and system designs. Our APU extensions allow researchers to model both CPU and GPU memory requests and the interactions between them. In particular, the model uses SLICC and Ruby to implement a wide variety of coherence and synchronization solutions, which is a critical research area in heterogeneous computing. The model has been used in several top-tier computer architecture publications in the last several years [MICRO 2013, HPCA 2014, ASPLOS 2014, ISCA 2014, HPCA 2015, ASPLOS 2015, MICRO 2016, HPCA 2017, ISCA 2017, HPCA 2018].\nIn this tutorial, we will describe the capabilities of the AMD gem5 APU simulator that will be publically released with a liberal BSD license before ISCA 2018. We will detail the simulated APU architecture, review the execution flow, and describe how the simulator has been used. The presentation will also discuss key design decisions and tradeoffs. For example, we use the system-call emulation mode to avoid running a full OS and kernel driver, therefore we will describe the simulator’s system-call emulation interface, and how the ROCm runtime and user space drivers interact with it. Also, our GPU model now directly executes native machine ISA instructions rather than the HSAIL intermediate language representation. Previously relying on executing the intermediate language simplified workload compilation, but was less accurate when modeling hardware behavior. In this tutorial, we will highlight many of the improvements enabled by executing the GCN3 ISA.\n[1]. Nathan Binkert et al. The gem5 Simulator. In SIGARCH Computer Architecture News, vol. 39, no. 2, pp. 1-7, Aug. 2011.\n[2]. AMD. AMD GCN3 ISA Architecture Manual\n[3]. Anthony Gutierrez et al. Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level. In HPCA 2018.\nSlides Schedule    Topic Presenter Time     Background Tony 8:00-8:15 am   ROCm Stack, GCN3 ISA, and uArch Tony 8:15-9:15 am   HSA Queuing Sooraj 9:15-10:00 am   Break 10:00-10:30 am    Ruby and GPU Protocol Tester Tuan 10:30-11:15 am   Demo/Workloads and Q+A TBD 11:15-12:00 pm    Presenters Tony Gutierrez (AMD Research)\nSooraj Puthoor (AMD Research)\nBrad Beckmann (AMD Research)\nTuan Ta (Cornell)\n"
},
{
	"uri": "http://localhost/benchmarks/spec-2000/",
	"title": "SPEC2000",
	"tags": [],
	"description": "",
	"content": " This is a work in-progress. Everyone should feel free to extend this page with their experiences to help new users get started.\n cpu2000.py Input sets and Binaries Several of the cpu2000 benchmarks for our regression tests. Unfortunately because of licensing restrictions we can\u0026rsquo;t provide the binaries or input files, however to make this a bit easier we have created cpu2000.py. Currently the script is tailored to our particular organization of the binaries and input files. To make the python work for you you\u0026rsquo;ll minimally have to change spec_dist to point to wherever you keep your cpu2000 binaries/input sets. We have our binaries and input sets organized in the following directory structure:\ncpu2000/binaries/ARCH/OPSYS/BENCHMARK cpu2000/data/BENCHMARK/INPUTSET/FILES\nWhere ARCH is alpha or sparc, OPSYS is linux or tru64, BENCHMARK is the name of the spec binary (e.g gzip), INPUTSET is the input files (e.g. smred), and FILES are the specific input files. If you can\u0026rsquo;t create this structure you\u0026rsquo;ll have to mess with cpu2000.py to change how it finds files.\nHow to use it The cpu2000.py configuration file takes this data and creates an m5 workload parameter based on benchmark name, isa, operating system, and input set. If you take a look at tests/long/00.gzip/test.py you can see an example of this, but in brief:\nfrom cpu2000 import gzip_log workload = gzip_log('alpha', 'tru64', 'smred') root.system.cpu.workload = workload.makeLiveProcess()  Assuming you have a machine configured normally above that blob would correctly run the gzip log spec2000 benchmark for alpha/tru64 with the smred input set. NOTOC\nSPEC2K Command Lines (Syscall Emulation) If you would like to run SPEC2K using syscall-emulation mode, a good reference for the correct command line options can be found here:\n SPEC2K INT Command Lines SPEC2K FP Command Lines  (Note: these example command lines aren\u0026rsquo;t for reduced/minimized input sets.)\nThen, if you are unable to use some of the aforementioned cpu2000.py scripts, you could try something like this to run statically-linked, ALPHA, eon\nbinary: $ build/ALPHA/gem5.debug configs/example/se.py --cmd=eon00 --options=\u0026quot;chair.control.cook chair.camera chair.surfaces chair.cook.ppm ppm pixels_out.cook\u0026quot;  "
},
{
	"uri": "http://localhost/arch/x86/address_space/",
	"title": "Address space",
	"tags": [],
	"description": "",
	"content": "X86_64 is defined to support physical memory addresses up to 52 bits long. Because M5 uses 64 bit integers for addresses, every physical address has 12 extra bits which aren\u0026rsquo;t directly accessible to the software running on the simulated CPU. M5 uses those bits to differentiate between different physical address spaces which are allocated for various purposes. In order to maximize the space given to the actual address portion of the address, the bits that select the address space grown down from the MSB as they\u0026rsquo;re allocated. In effect, the index of the address space is just written into the most significant bits of the address in reverse, with it\u0026rsquo;s least significant bit in the addresses most significant bit. The currently defined address spaces are the following:\n 0x0000000000000000 - Physical memory. 0x8000000000000000 - IO ports. 0xC000000000000000 - PCI config space. 0x2000000000000000 - The registers of all the local APICs. Each one gets a 2 page chunk of this space. 0xA000000000000000 - Interrupt messages. Each APIC gets a portion of this space as well. The various addresses work like memory mapped registers and allow the APICs to send each other IPIs.  "
},
{
	"uri": "http://localhost/tutorials/dist-gem5/",
	"title": "ISCA2017 - distributed gem5",
	"tags": [],
	"description": "",
	"content": " \n\nTitle: dist-gem5: Modeling and Simulating a Distributed Computer System Using Multiple Simulation\nSunday, June 25, 9:00 to 12:30\n44thInternationalSymposiumonComputer Architecture,June24-28,2017,Toronto,ON, Canada\n\n\nTOC\nList of organisers/presenters  Nam Sung Kim, University of Illinois, Urbana-Champaign Mohammad Alian, University of Illinois, Urbana-Champaign Nikos Nikoleris, ARM Ltd. Radhika Jagtap, ARM Ltd. Gabor Dozsa, ARM Ltd. Stephan Diestelhorst, ARM Ltd.  Abstract The single-thread performance improvement of processors has been sluggish for the past decade as Dennard’s scaling is approaching its fundamental physical limit. Thus, the importance of efficiently running applications on a parallel/distributed computer system has continued to increase and diverse applications based on parallel/distributed computing models such as MapReduce and MPI have thrived.\nIn a parallel/distributed computing system, the complex interplay amongst processor, node, and network architectures strongly affects the performance and power efficiency. In particular, we observe that all the hardware and software aspects of the network, which encompasses interface technology, switch/router capability, link bandwidth, topology, traffic patterns, and protocols, significantly impact the processor and node activities. Therefore, to maximize performance and power efficiency, it is critical to develop various optimization strategies cutting across processor, node, and network architectures, as well as their software stacks, necessitating full-system simulation. However, our community lacks a proper research infrastructure to study the interplay of these subsystems. Facing such a challenge, we have released a gem5-based simulation infrastructure dubbed dist-gem5 to support full-system simulation of a parallel/distributed computer system using multiple simulation host. This tutorial will cover an introduction to dist-gem5 including relevant background knowledge.\nObjectives More specifically, the tutorial will provide the following.\n Introduction of parallel/distributed system architecture. Details of enhanced gem5 components to enable simulation of a parallel/distributed computer system.  Network interface and switch models to connect multiple simulated nodes (as shown in the Figure). Synchronization amongst multiple simulated nodes running across multiple simulation hosts. Simulating a region of interest of a given benchmark using check-point creation/restoration enhanced for simulating multiple simulated nodes using multiple simulation hosts.  Examples of modeling parallel/distributed computer systems using a few network topologies.           09:00 – 10:00 Introduction (60 min)   10:00 – 10:15 Break (15 min)   10:15 – 11:15 dist-gem5 deep dive (60 min)   11:15 – 11:30 Break (15 min)   11:30 – 12:00 dist-gem5 examples (30 min)    Program for the tutorial\nSlides  The slides from the tutorial can be downloaded here.  Publications  Mohammad Alian, Gabor Dozsa, Umur Darbaz, Stephan Diestelhorst, Daehoon Kim, and Nam Sung Kim. “dist-gem5: Distributed Simulation of Computer Clusters”, IEEE International Symposium on Performance Analysis of Systems (ISPASS), April 2017 (Nominated for the Best Paper Award)   Mohammad Alian, Daehoon Kim, and Nam Sung Kim. “pd-gem5: Simulation Infrastructure for Parallel/Distributed Computer Systems”, IEEE Computer Architecture Letters (CAL), Jan 2016 paper   dist-gem5 website  Pre-requisites  Basic knowledge of computer architecture No prior experience with simulators is required  Previous tutorials  dist-gem5 tutorial at MICRO 2015 gem5 tutorial at ASPLOS 2017  "
},
{
	"uri": "http://localhost/arch/arm/",
	"title": "Arm",
	"tags": [],
	"description": "",
	"content": " Supported features and modes The ARM Architecture models within gem5 support an ARMv8-A profile of the ARM® architecture allowing for multi-processor simulation of 64-bit ARM (AArch64) cores. Additionally, gem5 still support ARMv7-a profile of the ARM® architecture with multi-processor extensions for 32-bit simulation. Specifically, this include support for Thumb®, Thumb-2, VFPv3 (32 double register variant), NEON™, and Large Physical Address Extensions (LPAE). Optional features of the architecture that are not currently supported are TrustZone®, ThumbEE, Jazelle®, and Virtualization.\nPertinent Non-supported Features Currently in ARMv8-A implementation in gem5, there isn\u0026rsquo;t support for interworking between AArch32 and AArch64 execution. This limits the ability to run some OSes that expect to execute both 32-bit and 64-bit code, but is expected to be fixed in the short term. Additionally, there has been limited testing of EL2 and EL3 modes in the implementation.\nConditional Execution Support Many instructions within the ARM architecture are predicated. To handle the predication within the gem5 framework and not have to generate N varieties of each instruction for every condition code, the instructions constructors determine which, if any, conditional execution flags are set and then conditionally read the condition codes or a \u0026ldquo;zero register\u0026rdquo; which is always available and doesn\u0026rsquo;t insert any dependencies in the dynamic execution of instructions.\nSpecial PC management The PCState object used for ARM® encodes additional execution state information so facilitate the use of the generic gem5 CPU components. In addition to the standard program counter, the Thumb® vs. ARM® instruction state is included as well as the ITSTATE (predication within Thumb® instructions).\nBoot loader A simple bootloader for ARM is in the source tree under system/arm/. Two boot loaders exist, one for AArch64 (aarch64_bootloader) and another for AArch32 (simple_bootloader).\nFor the AArch64 bootloader: The initial conditions of the boot loader are the same as those for Linux, r0 = device tree blob address; r6 = kernel start address.The boot loader starts the kernel with CPU 0 and places the other CPUs in a WFE spin-loop until the kernel starts them later.\nFor the AArch32 boot loader: The initial conditions of the bootloader running are the same as those ffor Linux, r0 = 0; r1 = machine number; r2 = atags ptr; and some special registers for the boot loader to use r3 = start address of kernel; r4 = address of GIC; r5 = adderss of flags register. The bootloader works by reading the MPIDR register to determine the CPU number. CPU0 jumps immediately to the kernel while CPUn enables their interrupt interface and and wait for an interrupt. When CPU0 generates an IPI, CPUn reads the flags register until it is non-zero and then jumps to that address.\n"
},
{
	"uri": "http://localhost/gem5-101/hello_world/",
	"title": "Hello world",
	"tags": [],
	"description": "",
	"content": "In part I, you will first learn to download and build gem5 correctly, create a simple configuration script for a simple system, write a simple C program and run a gem5 simulation. You will then introduce a two-level cache hierarchy in your system (fun stuff). Finally, you get to view the effect of changing system parameters such as memory types, processor frequency and complexity on the performance of your simple program.\n"
},
{
	"uri": "http://localhost/contributing/cloning/",
	"title": "Cloning the repo to contribute",
	"tags": [],
	"description": "",
	"content": " If you plan on contributing, it is strongly encouraged for you to clone the repository directly from our gerrit instance at https://gem5.googlesource.com/.\nTo clone the master gem5 repository:\n git clone https://gem5.googlesource.com/public/gem5  Other gem5 repositories There are a few repositories other than the main gem5 development repository.\n public/m5threads: The code for a pthreads implementation that works with gem5\u0026rsquo;s syscall emulation mode.  Other gem5 branches None right now.\n"
},
{
	"uri": "http://localhost/getting-started/dependencies/",
	"title": "Dependencies",
	"tags": [],
	"description": "",
	"content": "Building gem5 requires the following components to be installed. Ubuntu 16.04 and newer includes the required versions:\n git g++ or clang scons python swig protbuf  sudo apt install git build-essential scons python-dev swig sudo apt install libprotobuf-dev python-protobuf protobuf-compiler libgoogle-perftools-dev  While gem5 runs on Linux and Mac OS X, these instructions are written assuming Ubuntu 16.04 or newer. For installing the required dependencies for other platforms see Dependencies.\n "
},
{
	"uri": "http://localhost/getting-started/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "The gem5 simulator is distributed as source code the following pages will describe how to download, build, and run the simulator.\n"
},
{
	"uri": "http://localhost/cpu-models/inorder/",
	"title": "In Order",
	"tags": [],
	"description": "",
	"content": " Overview The InOrder CPU model was designed to provide a generic framework to simulate in-order pipelines with an arbitrary ISA and with arbitrary pipeline descriptions. The model was originally conceived by closely mirroring the O3CPU model to provide a simulation framework that would operate at the \u0026ldquo;Tick\u0026rdquo; granularity. We then abstract the individual stages in the O3 model to provide generic pipeline stages for the InOrder CPU to leverage in creating a user-defined amount of pipeline stages. Additionally, we abstract each component that a CPU might need to access (ALU, Branch Predictor, etc.) into a \u0026ldquo;resource\u0026rdquo; that needs to be requested by each instruction according to the resource-request model we implemented. This will potentially allow for researchers to model custom pipelines without the cost of designing the complete CPU from scratch.\nFor more information, please check the following documentation about the InOrder model, browse the code, and also access the gem5-users@m5sim.org (standard usage) or gem5-dev@m5sim.org (for developer) mailing lists:\n Pipeline Stages Resource-Request Modeling Instruction Schedules \u0026amp; Pipeline Descriptions A Day in the Life of an Instruction in the InOrderCPU model (Not Completed) Other Links:  Soumyaroop Roy has been kind enough to provide a [http://www.csee.usf.edu/~sroy/techres/m5_tests/\u0026ldquo;test-status-page\u0026ldquo;] of the M5-Inorder model in the work he has been doing.   Current Development Latest versions of the InOrderCPU model can be found in the gem5-dev repository\n InOrder ToDo List  "
},
{
	"uri": "http://localhost/benchmarks/spec-2006/",
	"title": "SPECcpu2006",
	"tags": [],
	"description": "",
	"content": " This is a work in-progress. Everyone should feel free to extend this page with their experiences to help new users get started.\n Input sets and Binaries We can\u0026rsquo;t provide the binaries or input files because of licensing restrictions, but It\u0026rsquo;s not hard to build the binaries by yourself. In this short article, we will share our experiences about what we have done so far.\nBuild the cross-compiler for alpha machine It is suggested that you use crosstool-ng available here, a new tool based on Dan Kegel\u0026rsquo;s cross tool that is more up to date. Follow the instructions available on that page for building the cross-compiler.\nIf you do not wish to build this tool yourself, you may be able to use one of the pre-compiled cross-compilers available on the M5 Download Page.\nOr, you can use the older crosstool. Download the crosstool-0.43.tar.gz from http://kegel.com/crosstool and modify these three lines in the demo-alpha.sh :\nRESULT_TOP=where_you_want_to_put_the_compiler GCC_LANGUAGES=\u0026quot;c,c++,fortran\u0026quot; eval `cat alpha.dat gcc-4.1.0-glibc-2.3.6.dat` sh all.sh --notest  Then follow the steps in the crosstool-howto page to build the cross compiler.\nBuild the SPEC CPU2006 alpha binaries Install the SPEC CPU2006 from DVD and modify the CC, CXX, and FC in config/alpha.cfg. If alpha.cfg is not in the config/ directory, you can use linux32-i386-gcc42.cfg and modify the CC, CXX, and FC variables.\nFor example: CC = /home/mjwu/crosstool/gcc-4.1.0-glibc-2.3.6/alpha/bin/alpha-gcc CXX = /home/mjwu/crosstool/gcc-4.1.0-glibc-2.3.6/alpha/bin/alpha-g++ FC = /home/mjwu/crosstool/gcc-4.1.0-glibc-2.3.6/alpha/bin/alpha-gfortran  Then follow the instructions in the ./Docs/install-guide-unix.html to build the binaries\nFor example: runspec --config=alpha.cfg --action=build --tune=base bzip2  Create the SPEC CPU2006 processes for M5 SE mode A good reference for the correct command line options can be found here: SPEC_CPU2006_Commands.\nFor your convenience, here is our benchmark python file for the M5 SE mode.\n#Mybench.py #400.perlbench perlbench = LiveProcess() perlbench.executable = binary_dir+'400.perlbench_base.alpha-gcc' perlbench.cmd = [perlbench.executable] + ['-I./lib', 'attrs.pl'] perlbench.output = 'attrs.out' #401.bzip2 bzip2 = LiveProcess() bzip2.executable = binary_dir+'401.bzip2_base.alpha-gcc' data=data_dir+'401.bzip2/data/all/input/input.program' bzip2.cmd = [bzip2.executable] + [data, '1'] bzip2.output = 'input.program.out' #403.gcc gcc = LiveProcess() gcc.executable = binary_dir+'403.gcc_base.alpha-gcc' data=data_dir+'403.gcc/data/test/input/cccp.i' output='/import/home1/mjwu/work_spec2006/403.gcc/m5/cccp.s' gcc.cmd = [gcc.executable] + [data]+['-o',output] gcc.output = 'ccc.out' #410.bwaves bwaves = LiveProcess() bwaves.executable = binary_dir+'410.bwaves_base.alpha-gcc' bwaves.cmd = [bwaves.executable] #416.gamess gamess=LiveProcess() gamess.executable = binary_dir+'416.gamess_base.alpha-gcc' gamess.cmd = [gamess.executable] gamess.input='exam29.config' gamess.output='exam29.output' #429.mcf mcf = LiveProcess() mcf.executable = binary_dir+'429.mcf_base.alpha-gcc' data=data_dir+'429.mcf/data/test/input/inp.in' mcf.cmd = [mcf.executable] + [data] mcf.output = 'inp.out' #433.milc milc=LiveProcess() milc.executable = binary_dir+'433.milc_base.alpha-gcc' stdin=data_dir+'433.milc/data/test/input/su3imp.in' milc.cmd = [milc.executable] milc.input=stdin milc.output='su3imp.out' #434.zeusmp zeusmp=LiveProcess() zeusmp.executable = binary_dir+'434.zeusmp_base.alpha-gcc' zeusmp.cmd = [zeusmp.executable] zeusmp.output = 'zeusmp.stdout' #435.gromacs gromacs = LiveProcess() gromacs.executable = binary_dir+'435.gromacs_base.alpha-gcc' data=data_dir+'435.gromacs/data/test/input/gromacs.tpr' gromacs.cmd = [gromacs.executable] + ['-silent','-deffnm',data,'-nice','0'] #436.cactusADM cactusADM = LiveProcess() cactusADM.executable = binary_dir+'436.cactusADM_base.alpha-gcc' data=data_dir+'436.cactusADM/data/test/input/benchADM.par' cactusADM.cmd = [cactusADM.executable] + [data] cactusADM.output = 'benchADM.out' #437.leslie3d leslie3d=LiveProcess() leslie3d.executable = binary_dir+'437.leslie3d_base.alpha-gcc' stdin=data_dir+'437.leslie3d/data/test/input/leslie3d.in' leslie3d.cmd = [leslie3d.executable] leslie3d.input=stdin leslie3d.output='leslie3d.stdout' #444.namd namd = LiveProcess() namd.executable = binary_dir+'444.namd_base.alpha-gcc' input=data_dir+'444.namd/data/all/input/namd.input' namd.cmd = [namd.executable] + ['--input',input,'--iterations','1','--output','namd.out'] namd.output='namd.stdout' #445.gobmk gobmk=LiveProcess() gobmk.executable = binary_dir+'445.gobmk_base.alpha-gcc' stdin=data_dir+'445.gobmk/data/test/input/capture.tst' gobmk.cmd = [gobmk.executable]+['--quiet','--mode','gtp'] gobmk.input=stdin gobmk.output='capture.out' #447.dealII dealII=LiveProcess() dealII.executable = binary_dir+'447.dealII_base.alpha-gcc' dealII.cmd = [gobmk.executable]+['8'] dealII.output='log' #450.soplex soplex=LiveProcess() soplex.executable = binary_dir+'450.soplex_base.alpha-gcc' data=data_dir+'450.soplex/data/test/input/test.mps' soplex.cmd = [soplex.executable]+['-m10000',data] soplex.output = 'test.out' #453.povray povray=LiveProcess() povray.executable = binary_dir+'453.povray_base.alpha-gcc' data=data_dir+'453.povray/data/test/input/SPEC-benchmark-test.ini' #povray.cmd = [povray.executable]+['SPEC-benchmark-test.ini'] povray.cmd = [povray.executable]+[data] povray.output = 'SPEC-benchmark-test.stdout' #454.calculix calculix=LiveProcess() calculix.executable = binary_dir+'454.calculix_base.alpha-gcc' data='/import/RaidHome/mjwu/work_spec2006/454.calculix/m5/beampic' calculix.cmd = [calculix.executable]+['-i',data] calculix.output = 'beampic.log' #456.hmmer hmmer=LiveProcess() hmmer.executable = binary_dir+'456.hmmer_base.alpha-gcc' data=data_dir+'456.hmmer/data/test/input/bombesin.hmm' hmmer.cmd = [hmmer.executable]+['--fixed', '0', '--mean', '325', '--num', '5000', '--sd', '200', '--seed', '0', data] hmmer.output = 'bombesin.out' #458.sjeng sjeng=LiveProcess() sjeng.executable = binary_dir+'458.sjeng_base.alpha-gcc' data=data_dir+'458.sjeng/data/test/input/test.txt' sjeng.cmd = [sjeng.executable]+[data] sjeng.output = 'test.out' #459.GemsFDTD GemsFDTD=LiveProcess() GemsFDTD.executable = binary_dir+'459.GemsFDTD_base.alpha-gcc' GemsFDTD.cmd = [GemsFDTD.executable] GemsFDTD.output = 'test.log' #462.libquantum libquantum=LiveProcess() libquantum.executable = binary_dir+'462.libquantum_base.alpha-gcc' libquantum.cmd = [libquantum.executable],'33','5' libquantum.output = 'test.out' #464.h264ref h264ref=LiveProcess() h264ref.executable = binary_dir+'464.h264ref_base.alpha-gcc' data=data_dir+'464.h264ref/data/test/input/foreman_test_encoder_baseline.cfg' h264ref.cmd = [h264ref.executable]+['-d',data] h264ref.output = 'foreman_test_encoder_baseline.out' #470.lbm lbm=LiveProcess() lbm.executable = binary_dir+'470.lbm_base.alpha-gcc' data=data_dir+'470.lbm/data/test/input/100_100_130_cf_a.of' lbm.cmd = [lbm.executable]+['20', 'reference.dat', '0', '1' ,data] lbm.output = 'lbm.out' #471.omnetpp omnetpp=LiveProcess() omnetpp.executable = binary_dir+'471.omnetpp_base.alpha-gcc' data=data_dir+'471.omnetpp/data/test/input/omnetpp.ini' omnetpp.cmd = [omnetpp.executable]+[data] omnetpp.output = 'omnetpp.log' #473.astar astar=LiveProcess() astar.executable = binary_dir+'473.astar_base.alpha-gcc' astar.cmd = [astar.executable]+['lake.cfg'] astar.output = 'lake.out' #481.wrf wrf=LiveProcess() wrf.executable = binary_dir+'481.wrf_base.alpha-gcc' wrf.cmd = [wrf.executable]+['namelist.input'] wrf.output = 'rsl.out.0000' #482.sphinx sphinx3=LiveProcess() sphinx3.executable = binary_dir+'482.sphinx_livepretend_base.alpha-gcc' sphinx3.cmd = [sphinx3.executable]+['ctlfile', '.', 'args.an4'] sphinx3.output = 'an4.out' #483.xalancbmk xalancbmk=LiveProcess() xalancbmk.executable = binary_dir+'483.Xalan_base.alpha-gcc' xalancbmk.cmd = [xalancbmk.executable]+['-v','test.xml','xalanc.xsl'] xalancbmk.output = 'test.out' #998.specrand specrand_i=LiveProcess() specrand_i.executable = binary_dir+'998.specrand_base.alpha-gcc' specrand_i.cmd = [specrand_i.executable] + ['324342','24239'] specrand_i.output = 'rand.24239.out' #999.specrand specrand_f=LiveProcess() specrand_f.executable = binary_dir+'999.specrand_base.alpha-gcc' specrand_f.cmd = [specrand_i.executable] + ['324342','24239'] specrand_f.output = 'rand.24239.out'  M5 python configure script Here is our system configuration python file for the M5 SE mode.\n#cmp.py # Simple configuration script import m5 from m5.objects import * import os, optparse, sys m5.AddToPath('./configs') import Simulation from Caches import * import Mybench # Get paths we might need. It's expected this file is in m5/configs/example. config_path = os.path.dirname(os.path.abspath(__file__)) print config_path config_root = os.path.dirname(config_path)+\u0026quot;/configs\u0026quot; print config_root m5_root = os.path.dirname(config_root) print m5_root parser = optparse.OptionParser() # Benchmark options parser.add_option(\u0026quot;-b\u0026quot;, \u0026quot;--benchmark\u0026quot;, default=\u0026quot;\u0026quot;, help=\u0026quot;The benchmark to be loaded.\u0026quot;) parser.add_option(\u0026quot;-c\u0026quot;, \u0026quot;--chkpt\u0026quot;, default=\u0026quot;\u0026quot;, help=\u0026quot;The checkpoint to load.\u0026quot;) execfile(os.path.join(config_root, \u0026quot;configs\u0026quot;, \u0026quot;Options.py\u0026quot;)) (options, args) = parser.parse_args() if args: print \u0026quot;Error: script doesn't take any positional arguments\u0026quot; sys.exit(1) if options.benchmark == 'perlbench': process = Mybench.perlbench elif options.benchmark == 'bzip2': process = Mybench.bzip2 elif options.benchmark == 'gcc': process = Mybench.gcc elif options.benchmark == 'bwaves': process = Mybench.bwaves elif options.benchmark == 'gamess': process = Mybench.gamess elif options.benchmark == 'mcf': process = Mybench.mcf elif options.benchmark == 'milc': process = Mybench.milc elif options.benchmark == 'zeusmp': process = Mybench.zeusmp elif options.benchmark == 'gromacs': process = Mybench.gromacs elif options.benchmark == 'cactusADM': process = Mybench.cactusADM elif options.benchmark == 'leslie3d': process = Mybench.leslie3d elif options.benchmark == 'namd': process = Mybench.namd elif options.benchmark == 'gobmk': process = Mybench.gobmk; elif options.benchmark == 'dealII': process = Mybench.dealII elif options.benchmark == 'soplex': process = Mybench.soplex elif options.benchmark == 'povray': process = Mybench.povray elif options.benchmark == 'calculix': process = Mybench.calculix elif options.benchmark == 'hmmer': process = Mybench.hmmer elif options.benchmark == 'sjeng': process = Mybench.sjeng elif options.benchmark == 'GemsFDTD': process = Mybench.GemsFDTD elif options.benchmark == 'libquantum': process = Mybench.libquantum elif options.benchmark == 'h264ref': process = Mybench.h264ref elif options.benchmark == 'tonto': process = Mybench.tonto elif options.benchmark == 'lbm': process = Mybench.lbm elif options.benchmark == 'omnetpp': process = Mybench.omnetpp elif options.benchmark == 'astar': process = Mybench.astar elif options.benchmark == 'wrf': process = Mybench.wrf elif options.benchmark == 'sphinx3': process = Mybench.sphinx3 elif options.benchmark == 'xalancbmk': process = Mybench.xalancbmk elif options.benchmark == 'specrand_i': process = Mybench.specrand_i elif options.benchmark == 'specrand_f': process = Mybench.specrand_f if options.chkpt != \u0026quot;\u0026quot;: process.chkpt = options.chkpt (CPUClass, test_mem_mode, FutureClass) = Simulation.setCPUClass(options) CPUClass.clock = '1.0GHz' #np = options.num_cpus np = 1 system = System(cpu = [CPUClass(cpu_id=i) for i in xrange(np)], physmem = PhysicalMemory(range=AddrRange(\u0026quot;4096MB\u0026quot;)), membus = Bus(), mem_mode = 'timing') system.physmem.port = system.membus.port for i in xrange(np): if options.caches: system.cpu[i].addPrivateSplitL1Caches(L1Cache(size = '64kB'), L1Cache(size = '64kB')) if options.l2cache: system.l2 = L2Cache(size='2MB') system.tol2bus = Bus() system.l2.cpu_side = system.tol2bus.port system.l2.mem_side = system.membus.port system.cpu[i].connectMemPorts(system.tol2bus) else: system.cpu[i].connectMemPorts(system.membus) system.cpu[i].workload = process[i] root = Root(system = system) Simulation.run(options, root, system, FutureClass)  The SPEC CPU2006 testing dataset results We use the quard-core Xeon 2.5GHz with 16G memory machine. The operation system is 64bits CentOS 5.2. The timing results are from the simple cpu model and SPEC CPU2006 testing data set.\n              benchmark datatype language input data number of instructions host seconds comment   400.perlbench integer C attrs.out - - fatal: fault (unalign) detected @ PC 0x12009cedc   401.bzip2 integer C input.program 3171671617 1353.56 o.k.   403.gcc integer C cccp.i - - never end, but o.k. for smaller input   410.bwaves floating Fortran test 119365801487 51703.94 o.k.   416.gamess floating Fortran exam29 - - abormal exit   429.mcf integer C inp.in 5112705810 3386.09 o.k.   433.milc floating C su3imp.in 38027871822 18402.06 output is different with the reference   434.zeusmp floating Fortran zmp_inp 62107158516 27746.77 output is different with the reference   435.gromacs floating C/Fortran gromacs.tpr 10861507208 4457.33 output is wrong, mremap has problem   436.cactusADM floating C/Fortran benchADM.par - - fatal: fault (unalign) detected @ PC 0x120026614   437.leslie3d floating Fortran leslie3d.in 87402135744 41635.96 o.k.   444.namd floating point C++ namd.input 64449976020 26798.88 o.k   445.gobmk integer C capture.tst 494502991 260.29 o.k.   447.dealII floating C++ 8 - - output is wrong   450.soplex floating C++ test.mps 72422927 31.95 o.k.   453.povray floating point C++ test.ini 3597778011 1737.24 o.k.   454.calculix floating point C beampic.inp 251699786 101.04 o.k.   456.hmmer integer C bombesin.hmm 2386768547 997.97 o.k.   458.sjeng integer C test.txt 21682684235 9406.80 o.k.   459.GemsFDTD floating Fortran test.in 11046857318 5289.88 o.k.   462.libquantum integer C 33 5 292639209 111.64 o.k.   464.h264ref integer C foreman_test_encoder_baseline.cfg 154340641371 67426.00 o.k.   465.tonto floating Fortran - - - compile error   470.lbm floating C 100_100_130_cf_a.of 7058506019 4599.69 o.k.   471.omnetpp integer C++ omnetpp.ini 2450821721 1153.36 o.k.   473.astar integer C++ lake.cfg 35796103621 16433.83 output is different with the reference   481.wrf floating C/Fortran - - - STOP wrf_abort. Need library   482.sphinx3 floating C args.an4 9352006427 4011.67 o.k.   483.xalancbmk integer C++ test.xml 501493417 276.77 o.k.   998.specrand integer C 324342 24239 71348559 32.93 o.k.   999.specrand floating C 324342 24239 71348559 31.58 o.k.             Trouble shooting You may encounter errors while executing the SPEC CPU2006, and these two errors are common on 32-bits machine.\n1. terminate called after throwing an instance of \u0026lsquo;std::bad_alloc\u0026rsquo;:\nThe M5 cannot allocate memory from you system. This happens a lot in the 32-bits machine, because only 3G memory can be used. To make life easier, you need a 64-bits machine.\n2. bus error:\nThe same as above.\n"
},
{
	"uri": "http://localhost/arch/x86/decoder/",
	"title": "Decoder",
	"tags": [],
	"description": "",
	"content": " Requirements The decoder must handle:\n Variable length instructions Misaligned instructions Instructions that span fetch buffer/cache line/page boundaries Microcoded instructions Instructions longer than native data types Self modifying code Externally modified memory and page mappings Instructions beginning \u0026ldquo;inside\u0026rdquo; other instructions  Current Design The decoder process will happen in several stages.\nFirst, the native variable length instructions will be translated into \u0026ldquo;fixed width\u0026rdquo; instructions. These instructions will be too large to represent with native data types, so they will be stored as structures. This translator will be set up as a state machine which operates on one byte at a time. This takes advantage of multiple bytes being ready at once in a buffer for filling in multiple byte immediates, for instance.\nThe fields in the structure will contain:\n A bitfield specifying what prefixes were present What the opcode(s) was/were The ModRM and SIB bytes Any immediate values  Second, this structure will be passed into a decoder generated using the isa_parser. This will require modifying the current decoding structure and isa_parser so that they accept a structure with fields. It may also require being able to swap out parts of the decoder to simulate different decoding \u0026ldquo;modes\u0026rdquo;. This step will produce a static_inst object which has now been fully decoded.\nAt this point, the decoded instruction is passed back to the cpu. A flag in the instruction indicates whether it\u0026rsquo;s implemented by microcode. If it isn\u0026rsquo;t, the instruction is processed by the cpu as is. If it is, the instruction is really a wrapper which contains other static_insts. These internal microops actually implement their containing macroop, and they are what is passed down the pipe. Once the sequence of microops are finished, the macroop is discarded and decoding continues with the next instruction.\n"
},
{
	"uri": "http://localhost/gem5-101/adding_an_instruction/",
	"title": "Adding an instruction",
	"tags": [],
	"description": "",
	"content": "For part II, we had used gem5 capabilities straight out of the box. Now, we will witness the flexibility and usefulness of gem5 by extending the simulator functionality. We walk you through the implementation of an x86 instruction (FSUBR), which is currently missing from gem5. This will introduce you to gem5\u0026rsquo;s language for describing instruction sets, and illustrate how instructions are decoded and broken down into micro-ops which are ultimately executed by the processor.\n"
},
{
	"uri": "http://localhost/contributing/changes/",
	"title": "Making changes to gem5",
	"tags": [],
	"description": "",
	"content": " It is strongly encouraged to use git branches when making changes to gem5. Additionally, keeping changes small and concise and only have a single logical change per commit.\nUnlike our previous flow with Mercurial and patch queues, when using git, you will be committing changes to your local branch. By using separate branches in git, you will be able to pull in and merge changes from mainline and simply keep up with upstream changes.\nRequirements for change descriptions To help reviewers and future contributors more easily understand and track changes, we require all change descriptions be strictly formatted.\nA canonical commit message consists of three parts:\n A short summary line describing the change. This line starts with one or more keywords (found in the MAINTAINERS file) separated by commas followed by a colon and a description of the change. This line should be no more than 65 characters long since version control systems usually add a prefix that causes line-wrapping for longer lines. (Optional, but highly recommended) A detailed description. This describes what you have done and why. If the change isn\u0026rsquo;t obvious, you might want to motivate why it is needed. Lines need to be wrapped to 75 characters or less. Tags describing patch metadata. You are highly recommended to use tags to acknowledge reviewers for their work. Gerrit will automatically add most tags.  Tags are an optional mechanism to store additional metadata about a patch and acknowledge people who reported a bug or reviewed that patch. Tags are generally appended to the end of the commit message in the order they happen. We currently use the following tags:\n Signed-off-by: Added by the author and the submitter (if different). This tag is a statement saying that you believe the patch to be correct and have the right to submit the patch according to the license in the affected files. Similarly, if you commit someone else\u0026rsquo;s patch, this tells the rest of the world that you have have the right to forward it to the main repository. If you need to make any changes at all to submit the change, these should be described within hard brackets just before your Signed-off-by tag. By adding this line, the contributor certifies the contribution is made under the terms of the Developer Certificate of Origin (DCO) [https://developercertificate.org/]. Reviewed-by: Used to acknowledge patch reviewers. It\u0026rsquo;s generally considered good form to add these. Added automatically. Reported-by: Used to acknowledge someone for finding and reporting a bug. Reviewed-on: Link to the review request corresponding to this patch. Added automatically. Change-Id: Used by Gerrit to track changes across rebases. Added automatically with a commit hook by git. Tested-by: Used to acknowledge people who tested a patch. Sometimes added automatically by review systems that integrate with CI systems.  Other than the \u0026ldquo;Signed-off-by\u0026rdquo;, \u0026ldquo;Reported-by\u0026rdquo;, and \u0026ldquo;Tested-by\u0026rdquo; tags, you generally don\u0026rsquo;t need to add these manually as they are added automatically by Gerrit.\nIt is encouraged for the author of the patch and the submitter to add a Signed-off-by tag to the commit message. By adding this line, the contributor certifies the contribution is made under the terms of the Developer Certificate of Origin (DCO) [https://developercertificate.org/].\nIt is imperative that you use your real name and your real email address in both tags and in the author field of the changeset.\nFor significant changes, authors are encouraged to add copyright information and their names at the beginning of the file. The main purpose of the author names on the file is to track who is most knowledgeable about the file (e.g., who has contributed a significant amount of code to the file).\nNote: If you do not follow these guidelines, the gerrit review site will automatically reject your patch. If this happens, update your changeset descriptions to match the required style and resubmit. The following is a useful git command to update the most recent commit (HEAD).\n git commit --amend  "
},
{
	"uri": "http://localhost/gem5-101/",
	"title": "gem5 101",
	"tags": [],
	"description": "",
	"content": " This is a six part course which will help you pick up the basics of gem5, and illustrate some common uses. This course is based around the assignments from a particular offering of architecture courses, CS 752 and CS 757, taught at the University of Wisconsin-Madison.\nCredits A lot of people have been involved over the years in developing the assignments for these courses. If we have missed out on anyone, please add them here.\n Multifacet research group at University of Wisconsin-Madison Profs Mark Hill, David Wood Jason Lowe-Power Nilay Vaish Lena Olson Swapnil Haria Jayneel Gandhi  Any questions or queries regarding this tutorial should be directed towards the gem5-users mailing list, and not the individual contacts listed in the assignment.\n "
},
{
	"uri": "http://localhost/docs/dependencies/",
	"title": "Dependencies",
	"tags": [],
	"description": "",
	"content": " gem5 runs on Linux and Mac OS X, but should be easily portable to other Unix-like OSes. At times in the past gem5 has worked on OpenBSD and Microsoft Windows (under Cygwin), but these platforms are not regularly tested. Cygwin in particular is no longer actively supported; if you must run on a Windows host, we recommend installing Linux (e.g., Ubuntu Server) under a VM and running gem5 there. Free virtualization solutions such as VirtualBox and VMware Player work well for this usage. Cross-endian support has been mostly added, however this has not been extensively tested. Running a program with syscall emulation is supported regardless of host/target endianess, however full-system simulation may have cross-endian issues (ALPHA full-system is known not to work on big endian machines).\nHardware gem5 is largely agnostic about the hardware it runs on. However, there are several considerations to keep in mind when running gem5:\n A 64-bit platform is strongly preferred over a 32-bit platform. Simulating a platform with a significant amount of physical memory will require the ability to address that much memory from within the gem5 process. Specifically, a 32-bit platform will typically be limited to simulating platforms with roughly 1GB of physical memory. Also, many of the ISAs simulated by gem5 are 64 bit (e.g., x86-64, ARM aarch64 and Alpha), so simulating their operation on a 32-bit machine will incur additional slowdowns.\n gem5\u0026rsquo;s ISA support involves some very large auto-generated C++ files, which can require up to 1GB for g++ to compile. If you intend to do parallel builds (using the scons \u0026ldquo;-j\u0026rdquo; flag), you may occasionally see significant slowdowns from paging if your system has less than 1GB per core. This constraint is particularly worth noting if you\u0026rsquo;re configuring a VM to run gem5 under Windows, as suggested above.\n Ideally you should choose a host with the same endianness as the ISA you will be simulating. gem5 does support cross-endian simulation, but this feature is not extensively tested. Cross-endian simulation works best in syscall emulation (SE) mode.\n  External tools and required versions To build gem5, you will need the following software:\n g++ version 4.8 or newer or clang version 3.1 or newer. Python, version 2.6 - 2.7 (we don\u0026rsquo;t support Python 3.X). gem5 links in the Python interpreter, so you need the Python header files and shared library (e.g., /usr/lib/libpython2.6.so) in addition to the interpreter executable. These may or may not be installed by default. For example, on Debian/Ubuntu, you need the \u0026ldquo;python-dev\u0026rdquo; package in addition to the \u0026ldquo;python\u0026rdquo; package. If you need a newer or different Python installation but can\u0026rsquo;t or don\u0026rsquo;t want to upgrade the default Python on your system, see our page on using a non-default Python installation. SCons, version 0.98.1 or newer. SCons is a powerful replacement for make. See here to download SCons. If you don\u0026rsquo;t have administrator privileges on your machine, you can use the \u0026ldquo;scons-local\u0026rdquo; package to install scons in your m5 directory, or install SCons in your home directory using the \u0026lsquo;\u0026ndash;prefix=\u0026rsquo; option. Some scripts require argparse, which is available by default in Python 2.7 and can be installed from PyPi for older versions. zlib, any recent version. For Debian/Ubuntu, you will need the \u0026ldquo;zlib-dev\u0026rdquo; or \u0026ldquo;zlib1g-dev\u0026rdquo; package to get the zlib.h header file as well as the library itself. m4, the macro processor.  The following is optional, but highly recommended:\n protobuf, version 2.1 or newer for trace capture and playback support. pydot, Pythons interface to graphviz, is needed for generation of graphical representations of the simulated system topology.  There a few utility scripts written in Perl, but Perl is not necessary to build or run the simulator.\nIncluded dependencies Some packages which might be difficult to find or which were modified for us in gem5 are included in the ext directory.\n libfdt \u0026ndash; provides support for flattened device tree \u0026ldquo;blob\u0026rdquo; files dnet \u0026ndash; dnet provides a simplified, portable interface to several low-level networking routines. iostream3 \u0026ndash; A C++ stream interface to the zlib library. libelf \u0026ndash; ELF object file access library. PLY \u0026ndash; PLY is an implementation of lex and yacc parsing tools for Python. x11ksyms \u0026ndash; Keycodes from X11 for VNC support. fputils \u0026ndash; Compiler-independent library for 80-bit floating point arithmetic  "
},
{
	"uri": "http://localhost/getting-started/source-code/",
	"title": "Source Code",
	"tags": [],
	"description": "",
	"content": " Official git repository Steps for getting a copy of the source code:\n Install git. This is available in the git package on Ubuntu and Redhat and OS X macports or home-brew.\n Clone the development repository git clone https://gem5.googlesource.com/public/gem5\n After you clone the repository you can update it by typing git pull. Read the documentation on this page for more info on using git.\n  More general Git and Gerrit information:  https://git-scm.com/book https://gerrit-review.googlesource.com/Documentation/index.html  "
},
{
	"uri": "http://localhost/getting-started/config-script/",
	"title": "Configuration Scripts",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost/arch/x86/instruction_decoding/",
	"title": "Instruction decoding",
	"tags": [],
	"description": "",
	"content": " Overview http://m5sim.org/graphics/InstructionExecution.png\nX86 instruction encodings have several unique characteristics which make them harder to deal with than the encodings for the other ISAs M5 supports. Despite that, x86 is decoded using the same basic mechanisms as the other ISAs.\nAt the lowest level, instructions can take any number of bytes (up to a maximum) and can have any alignment. That means that when a CPU brings in bytes of memory to decode, it may contain several instructions, the end of one and the start of the next, the middle of an instruction, etc. It\u0026rsquo;s the predecoder\u0026rsquo;s job to take the incoming stream of bytes and to turn them into a stream of objects, called ExtMachInsts, which represent the encoding of a single instruction from beginning to end in a unique and uniform way. The predecoder also makes the guarantee that if any two instruction representations are the same, the instruction object that implements it is also the same.\nNext, the instruction representations need to be translated into an instruction object which the CPU can execute. This is the job of the x86 decoder which is implemented as ISA description files and automatically generated. In addition to the instruction objects and the decoder itself, a ROM object is defined which holds microops which can be executed independent of any containing instruction.\nFinally, the instruction object is handed back to the CPU. If the object is a regular instruction it can be executed directly. If the object is a macroop, aka a microcoded instruction, the CPU\u0026rsquo;s decode section pulls microops out of it one at a time and actually executes those. Control flow within the microops may move execution out of the current macroop and into the ROM. If that happens, instructions are pulled from the ROM until the next instruction starts.\nPredecoder http://m5sim.org/graphics/X86PredecoderStateMachine.png\nThe diagram above shows the high level state machine that runs the x86 predecoder. The CPU feeds the predcoder bytes in certain sized chunks which, in other ISAs, would correspond with instructions. In x86 these are just bytes and are processed one at a time to move the predecoder through the state machine. The predecoder may not have to consume the entire chunk, or it may go through several before it has an entire instruction. Once it\u0026rsquo;s finished, there should be a completed ExtMachInst representing one instruction ready for the CPU to use.\nReset State Resets internal state to prepare for the next instruction.\nPrefix State Recognizes and records the various prefixes allowed for an instruction. When a byte comes in that is not recognized as a prefix, the machine goes to the Opcode state.\nOpcode State Gathers up all the bytes that represent the instructions opcode. Instructions can have 1, 2 or even 3 byte opcodes. This state also figures out how many bytes of immediate value, if any will be part of this instruction, and what the default address size, operand size, and stack size are. If this instruction uses a modRM byte, the next state is ModRM. Failing that, if there are any immediate bytes to collect, the next state is Imm. If neither of those are true, then the instruction has been gathered and the machine resets.\nModRM State Processes the modRM byte. This includes determining which registers will be used as part of address computation or as operands directly, and how many, if any, bytes will be used for the displacement. This state also determines whether to expect an SIB byte. If so, the next state is SIB. If not and there are displacement bytes to collect, the next state is Disp. If not and there are immediate bytes to collect (as determined in the Opcode state), the next state is Imm. Otherwise, this instruction is finished.\nSIB State Very similar to the ModRM except processing the SIB byte.\nDisp State Collects as many displacement bytes as it can from the bytes provided by the CPU. If the whole displacement has been collected, the state machine continues on to Imm if immediate bytes are needed, or it completes the current instruction.\nImm State The same as Disp except for immediate bytes.\nDecoder generation http://m5sim.org/graphics/X86DecoderGeneration.png\nX86\u0026rsquo;s decoder is generated using an ISA description like all the other ISAs, although how it does that is a bit different. Most of the instructions for most of the other ISAs are defined by passing chunks of code that perform the instruction into an instruction format. The format is basically a template which puts wraps that bit of code in the structure needed to support it and you have your instruction object. Because almost all of the instructions in x86 are microcoded and many can be encoded in multiple ways and hence appear in the decoder more than once, and because the same non-trivial decoding rules apply to many different instructions, X86 uses the decoder as a layer of indirection and defines the majority of its instructions elsewhere.\nX86 almost exclusively uses only two different instruction formats, Inst and MultiInst. MultiInst is just a compact way of describing multiple related Insts. An Inst essentially selects an instruction like XOR and provides a specification for its operands. Inside the instruction format, the instruction name and operand specification are passed to a python function called \u0026ldquo;specializeInst\u0026rdquo; which figures out what to do with it. If the operand specification describes more than one version of the instruction, for instance one that uses memory and one that uses registers, the instruction\u0026rsquo;s information is passed into another funciton, \u0026ldquo;doSplitDecode\u0026rdquo;, which separates out those versions and passes each individually back through the same system. This goes on until the instructions have been fully split out and code has been generated to figure out what version to use. As a nice bonus, the MultiInst format doesn\u0026rsquo;t add much complexity to this model since it can simply jump right into doSplitDecode and continue as normal. There is one additional format for string instructions that works similar to MultiInst, except instead of specializing the instruction based on its operands, it specializes it based on its prefixes. There are also a few instructions that don\u0026rsquo;t use this system, but because there are so few and because they work like the instructions in other ISAs I won\u0026rsquo;t describe them here.\nAt this point, the code for selecting the right version of an instruction is put into the C++ decoder function. Almost all of this function is built this way, with the minor exception of small bits of logic that glue everything together and make large scale distinctions the number of opcode bytes.\nNow that we know -what- instruction we want, we need to actually generate a definition for it. Before this entire process got rolling, a library of macroops and their microcode definitions were assembled using the microcode assembler. The specialized name of the instruction/macroop, for instance XOR_R_R, or XOR with two register operands, is used to look up a python object in that library that describes how to implement XOR_R_R. This object has the smarts to generate the C++ needed to implement itself. That code is generated and stuck into source files, and a small bit of code is added to the decoder so that an instruction of the right type can be allocated and returned.\n"
},
{
	"uri": "http://localhost/gem5-101/cpu_models/",
	"title": "CPU models",
	"tags": [],
	"description": "",
	"content": "From the ISA, we now move on to the processor micro-architecture. Part III introduces the various different cpu models implemented in gem5, and analyzes the performance of a pipelined implementation. Specifically, you will learn how the latency and bandwidth of different pipeline stages affect overall performance. Also, a sample usage of gem5 pseudo-instructions is also included at no additional cost.\n"
},
{
	"uri": "http://localhost/contributing/posting-review/",
	"title": "Posting a review",
	"tags": [],
	"description": "",
	"content": " If you have not signed up for an account on the Gerrit review site (https://gem5-review.googlesource.com), you first have to create an account.\nSetting up an account  Go to https://gem5.googlesource.com/ Click \u0026ldquo;Sign In\u0026rdquo; in the upper right corner. Note: You will need a Google account to contribute. After signing in, click \u0026ldquo;Generate Password\u0026rdquo; and follow the instructions.  Submitting a change In gerrit, to submit a review request, you can simply push your git commits to a special named branch. For more information on git push see https://git-scm.com/docs/git-push.\nThere are three ways to push your changes to gerrit.\nPush change to gerrit review  git push origin HEAD:refs/for/master  Assuming origin is https://gem5.googlesource.com/public/gem5 and you want to push the changeset at HEAD, this will create a new review request on top of the master branch. More generally,\n git push \u0026lt;gem5 gerrit instance\u0026gt; \u0026lt;changeset\u0026gt;:refs/for/\u0026lt;branch\u0026gt;  See https://gerrit-review.googlesource.com/Documentation/user-upload.html for more information.\nPushing your first change The first time you push a change you may get the following error:\n remote: ERROR: [fb1366b] missing Change-Id in commit message footer ...  Within the error message, there is a command line you should run. For every new clone of the git repo, you need to run the following command to automatically insert the change id in the the commit (all on one line).\n curl -Lo `git rev-parse --git-dir`/hooks/commit-msg \\ https://gerrit-review.googlesource.com/tools/hooks/commit-msg ; \\ chmod +x `git rev-parse --git-dir`/hooks/commit-msg  If you receive the above error, simply run this command and then amend your changeset.\n git commit --amend  Push change to gerrit as a draft  git push origin HEAD:refs/drafts/master  Push change bypassing gerrit Only maintainers can bypass gerrit review. This should very rarely be used.\n git push origin HEAD:refs/heads/master  Other gerrit push options There are a number of options you can specify when uploading your changes to gerrit (e.g., reviewers, labels). The gerrit documentation has more information. https://gerrit-review.googlesource.com/Documentation/user-upload.html\n"
},
{
	"uri": "http://localhost/contributing/",
	"title": "Contributing",
	"tags": [],
	"description": "",
	"content": "If you\u0026rsquo;ve made changes to gem5 that might benefit others, we strongly encourage you to contribute those changes to the public gem5 repository. There are several reasons to do this:\n Share your work with others, so that they can benefit from new functionality. Support the scientific principle by enabling others to evaluate your suggestions without having to guess what you did. Once your changes are part of the main repo, you no longer have to merge them back in every time you update your local repo. This can be a huge time saving! Once your code is in the main repo, other people have to make their changes work with your code, and not the other way around. Others may build on your contributions to make them even better, or extend them in ways you did not have time to do. You will have the satisfaction of contributing back to the community.  The main method for contributing code to gem5 is via our code review website: https://gem5-review.googlesource.com/. This documents describes the details of how to create code changes, upload your changes, have your changes reviewed, and finally push your changes to gem5. More information can be found from the following sources:\n http://gem5.org/Submitting_Contributions https://gerrit-review.googlesource.com/Documentation/index.html https://git-scm.com/book  "
},
{
	"uri": "http://localhost/getting-started/build/",
	"title": "Building",
	"tags": [],
	"description": "",
	"content": "Let’s start by building a basic x86 system. Currently, you must compile gem5 separately for every ISA that you want to simulate. Additionally, if using ruby, you have to have separate compilations for every cache coherence protocol.\nTo build gem5, we will use SCons. SCons uses the SConstruct file (gem5/SConstruct) to set up a number of variables and then uses the SConscript file in every subdirectory to find and compile all of the gem5 source.\nSCons automatically creates a gem5/build directory when first executed. In this directory you’ll find the files generated by SCons, the compiler, etc. There will be a separate directory for each set of options (ISA and cache coherence protocol) that you use to compile gem5.\nThere are a number of default compilations options in the build_opts directory. These files specify the parameters passed to SCons when initially building gem5. We’ll use the ARM defaults and specify that we want to compile all of the CPU models.\nscons CPU_MODELS=\u0026quot;AtomicSimpleCPU,MinorCPU,O3CPU,TimingSimpleCPU\u0026quot; build/ARM/gem5.opt -j9\nThe main argument passed to SCons is what you want to build, build/ARM/gem5.opt. In this case, we are building gem5.opt (an optimized binary with debug symbols). We want to build gem5 in the directory build/ARM. Since this directory currently doesn’t exist, SCons will look in build_opts to find the default parameters for ARM. (Note: -j9 is being used to parallelize the build and you should choose an appropriate number for your machine, usually cores+1.)\nThe output should look something like below:\nscons: Reading SConscript files ... Mercurial libraries cannot be found, ignoring style hook. If you are a gem5 developer, please fix this and run the style hook. It is important. Checking for leading underscore in global variables...(cached) no Checking for C header file Python.h... (cached) yes Checking for C library pthread... (cached) yes Checking for C library dl... (cached) yes Checking for C library util... (cached) yes Checking for C library m... (cached) yes Checking for C library python2.7... (cached) yes Checking for accept(0,0,0) in C++ library None... (cached) yes Checking for zlibVersion() in C++ library z... (cached) yes Checking for GOOGLE_PROTOBUF_VERIFY_VERSION in C++ library protobuf... (cached) yes Checking for clock_nanosleep(0,0,NULL,NULL) in C library None... (cached) no Checking for clock_nanosleep(0,0,NULL,NULL) in C library rt... (cached) yes Checking for timer_create(CLOCK_MONOTONIC, NULL, NULL) in C library None... (cached) yes Checking for C library tcmalloc... (cached) yes Checking for C header file fenv.h... (cached) yes Checking for C header file linux/kvm.h... (cached) yes Checking size of struct kvm_xsave ... (cached) yes Checking for member exclude_host in struct perf_event_attr...(cached) yes .... .... \u0026lt;lots of output\u0026gt; .... [ LINK] -\u0026gt; ARM/gem5.opt scons: done building targets.  If you see an error Error: gcc version 4.6 or newer required. you need a newer version of gcc installed.\n If you see an error Error: SWIG version 2.0.4 or newer required. you need a newer version of swig installed.\n If you see an error Error: Can't find version of M4 macro processor. Please install M4 and try again. you need to install m4 and possible autotools: sudo apt install automake\n If you see an error TypeError: 'dict' object is not callable and and are using a non-default version of Python, you may need to force scons to use a specific version of python: python `which scons` build/ARM/gem5.opt\n "
},
{
	"uri": "http://localhost/getting-started/running/",
	"title": "Running",
	"tags": [],
	"description": "",
	"content": " Modes \\= System-call Emulation (SE) mode is one of the two modes of gem5 and the most basic. gem5 simulates your program and traps any system calls made to the host then emulates them. In the older versions of gem5, only statically linked binaries were allowed to be simulated in SE mode however, gem5 2.0 now supports dynamically linked binaries. Not all architecutres support dynamically linked binaries\n "
},
{
	"uri": "http://localhost/arch/x86/micro-code/",
	"title": "Micro Code",
	"tags": [],
	"description": "",
	"content": " Microop Parameter Specialization Microops are not, in general, written in an absolute sense. They are templates which provide an implementation for a macroop, but need to be specialized using the arguments of the original instruction. In hardware, this appears to be handled in two different ways depending on the origin of the microop. In the case of combinationally generated microops, the instructions are actually generated with the correct parameters in place. This is how the macroops in SPARC work. If the microops come from the ROM, they are specilized afterwards by a special hardware unit which operates on them before they pass on to the rest of the pipeline. The \u0026ldquo;emulation environment\u0026rdquo;, aka the particulars of the instruction being emulated by the microop sequence, is stored and is used by this specializer to get the values to substitute in. In hardware, this means recognizing a bit pattern and replacing the value with a different one on a match. In C++, this becomes more complicated. There seem to be three options.\n The instructions could be allocated with the correct parameters, similar to the combinational case. This has the benefit that the instruction is totally specialized for it\u0026rsquo;s environment from the very start. The instructions could be specialized after the fact by adding a new function. This lets you reuse instructions without having to fully reallocate them. The instructions could be specialized by adding in an external blob of data which holds all the relevant parameters.  The simplest approach would be the first, but allocating a new microop for every instruction from the ROM would likely have a fairly high performance overhead. The later two approaches help with that somewhat by letting you skip actually allocating memory, but they prevent you from using more than one version of a microop at a time.\nImmediate values are handled by making ROM based microops generate a few combinational microops before actually going to the ROM. These push the immediate parameters into certain microcode registers which the ROM code is written to use.\nA potential solution is to create an \u0026ldquo;EmulEnv\u0026rdquo; (or similar) class which contains the emulation environment. That\u0026rsquo;s passed into the macroop, and then the pieces of it are used to construct the contained microops using the standard \u0026ldquo;operand\u0026rdquo; mechanism. In the case where there is no containing macroop (which we might not want anyway), the \u0026ldquo;emulation environment\u0026rdquo; will exist only in python and its values will be used to fill in the microops parameters directly.\nMicroop Allocation Microops are allocated in two different ways. When they\u0026rsquo;re generated combinationally, they can be set up by their containing macroop when it gets allocated. Existing mechanisms which cache regular instructions also cache the macroop, and the microops end up being saved as well. With microops that come from the microcode ROM, microops aren\u0026rsquo;t stored with the macroop, and the caching mechanism doesn\u0026rsquo;t automatically work.\nAlso, the microop that should be generated from a particular microopc will depend, probably significantly, on the macroop it was generated for. This ties in with the above topic about parameter specialization, and means that the ROM can\u0026rsquo;t directly hold instatiations of it\u0026rsquo;s microops which would be a handy caching mechanism.\nOne potential solution would be for the ROM to have it\u0026rsquo;s own cache of microop instantiations. This wouldn\u0026rsquo;t be as simple as in the regular decode case because there aren\u0026rsquo;t handy machine code representations to work with, but the micropc might be used instead. Unlike regular instruction memory, the same micropc should always refer to the same instruction. Note that while this might seem like a reasonable assumption, there are situations I can imagine someone at some point might want to have dynamic microcode.\nAnother option would be to actually store the microops in the macroop which requested them. The draw back here is that it\u0026rsquo;s possible that you\u0026rsquo;re macroop might need a non constant and large set of microops to implement itself, and it might be difficult to keep track of what\u0026rsquo;s been instantiated and what hasn\u0026rsquo;t.\nYet another solution would be to try to work the microops into the main cache somehow. This doesn\u0026rsquo;t sound like a great idea since they really aren\u0026rsquo;t the same thing.\nMacroop Argument Specialization This sounds like but is very different from microop parameter specialization. This refers to generating groups of macroops which all perform the same operation, but use different arguments from different places including memory.\nIn order to simplify the system which specifies the microcode which makes up instructions, a preprocessor will need to be implemented. This will probably use syntax mimicking NASM\u0026rsquo;s.\nTo allow the decoder to specify what variant of arguments to use, the system of passing tags into the instruction format will be revamped. It will work essentially the same, but instead of doing replacement on the register arguments, it will need to take advantage of whatever the microop parameter specialization mechanism is to set things up.\n"
},
{
	"uri": "http://localhost/arch/",
	"title": "Supported architectures",
	"tags": [],
	"description": "",
	"content": " gem5 is a flexible architecture simulator that supports a number of ISAs and operating systems for both full-system simulation (booting an entire operating system) and syscall emulation (running one or more applications by emulating syscalls). An overview of the architecture support is given in the table below.\n   ISA Maintainer Level of ISA support Full-system OS support Test coverage Tool chain availability Linux kernel availability     ALPHA None High Linux Medium Low Low   ARM Andreas Sandberg High Linux, BSD, Android High High High   MIPS None Low None Low Medium Medium   POWER None Low None Low Medium Medium   RISC-V Alec Roelke Medium None Low Medium Medium   SPARC Gabe Black Low None Low Low Low   X86 Gabe Black Medium Linux, BSD Medium High High    We should update an arch/mem-system table here too\n Alpha Gem5 models a DEC Tsunami based system. In addition to the normal Tsunami system that support 4 cores, we have an extension which supports 64 cores (a custom PALcode and patched Linux kernel is required). The simulated system looks like an Alpha 21264 including the BWX, MVI, FIX, and CIX to user level code. For historical reasons the processor executes EV5 based PALcode.\nIt can boot unmodified Linux 2.4\u0026frasl;2.6, FreeBSD, or L4Ka::Pistachio as well as applications in syscall emulation mode. Many years ago it was possible to boot HP/Compaq\u0026rsquo;s Tru64 5.1 operating system. We no longer actively maintain that capability, however, and it does not currently work.\nARM The ARM Architecture models within gem5 support an ARMv8-A profile of the ARM® architecture with multi-processor extensions. This includes both AArch32 and AArch64 state. In AArch32, this include support for Thumb®, Thumb-2, VFPv3 (32 double register variant) and NEON™, and Large Physical Address Extensions (LPAE). Optional features of the architecture that are not currently supported are TrustZone®, ThumbEE, Jazelle®, and Virtualization.\nIn full system mode gem5 is able to boot uni- or multi-processor Linux and bare metal applications built with ARM\u0026rsquo;s compilers. Newer Linux versions work out of the box (if used with gem5\u0026rsquo;s DTBs) we also provide gem5-specific Linux kernels with custom configurations and custom drivers Additionally, statically linked Linux binaries can be run in ARM\u0026rsquo;s syscall emulation mode.\nPOWER Support for the POWER ISA within gem5 is currently limited to syscall emulation only and is based on the POWER ISA v2.06 B Book. A big-endian, 32-bit processor is modeled. Most common instructions are available (enough to run all the SPEC CPU2000 integer benchmarks). Floating point instructions are available but support may be patchy. In particular, the Floating-Point Status and Control Register (FPSCR) is generally not updated at all. There is no support for vector instructions.\nFull system support for POWER would require a significant amount of effort and is not currently being developed. However, if there is interest in pursuing this, a set of patches-in-progress that make a start towards this can be obtained from Tim.\nSPARC The gem5 simulator models a single core of a UltraSPARC T1 processor (UltraSPARC Architecture 2005).\nIt can boot Solaris like the Sun T1 Architecture simulator tools do (building the hypervisor with specific defines and using the HSMID virtual disk driver). Multiprocessor support was never completed for full-system SPARC. With syscall emulation gem5 supports running Linux or Solaris binaries. New versions of Solaris no longer support generating statically compiled binaries which gem5 requires.\nx86 X86 support within the gem5 simulator includes a generic x86 CPU with 64 bit extensions, more similar to AMD\u0026rsquo;s version of the architecture than Intel\u0026rsquo;s but not strictly like either. Unmodified versions of the Linux kernel can be booted in UP and SMP configurations, and patches are available for speeding up boot. SSE and 3dnow are implemented, but the majority of x87 floating point is not. Most effort has been focused on 64 bit mode, but compatibility mode and legacy modes have some support as well. Real mode works enough to bootstrap an AP, but hasn\u0026rsquo;t been extensively tested. The features of the architecture that are exercised by Linux and standard Linux binaries are implemented and should work, but other areas may not. 64 and 32 bit Linux binaries are supported in syscall emulation mode.\nMIPS "
},
{
	"uri": "http://localhost/gem5-101/branch_prediction/",
	"title": "Branch prediction",
	"tags": [],
	"description": "",
	"content": "From the ISA, we now move on to the processor micro-architecture. Part III introduces the various different cpu models implemented in gem5, and analyzes the performance of a pipelined implementation. Specifically, you will learn how the latency and bandwidth of different pipeline stages affect overall performance. Also, a sample usage of gem5 pseudo-instructions is also included at no additional cost.\n"
},
{
	"uri": "http://localhost/getting-started/adding-cache/",
	"title": "Adding cache",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost/contributing/reviewing-patches/",
	"title": "Reviewing patches",
	"tags": [],
	"description": "",
	"content": "Reviewing patches is done on our gerrit instance at https://gem5-review.googlesource.com/.\nAfter logging in with your Google account, you will be able to comment, review, and push your own patches as well as review others\u0026rsquo; patches. All gem5 users are encouraged to review patches. The only requirement to review patches is to be polite and respectful of others.\nThere are multiple labels in Gerrit that can be applied to each review detailed below.\n Code-review: This is used by any gem5 user to review patches. When reviewing a patch you can give it a score of -2 to +2 with the following semantics.  -2: This blocks the patch. You believe that this patch should never be committed. This label should be very rarely used. -1: You would prefer this is not merged as is 0: No score +1: This patch seems good, but you aren\u0026rsquo;t 100% confident that it should be pushed. +2: This is a good patch and should be pushed as is.  Maintainer: Currently only PMC members are maintainers. At least one maintainer must review your patch and give it a +1 before it can be merged. Verified: This is automatically generated from the continuous integrated (CI) tests. Each patch must receive at least a +1 from the CI tests before the patch can be merged. The patch will receive a +1 if gem5 builds and runs, and it will receive a +2 if the stats match. Style-Check: This is automatically generated and tests the patch against the gem5 code style (http://www.gem5.org/Coding_Style). The patch must receive a +1 from the style checker to be pushed.  Note: Whenever the patch creator updates the patch all reviewers must re-review the patch. There is no longer a \u0026ldquo;Fix it, then Ship It\u0026rdquo; option.\nOnce you have received reviews for your patch, you will likely need to make changes. To do this, you should update the original git changeset. Then, you can simply push the changeset again to the same Gerrit branch to update the review request.\nPlease see governance and reviewing patches.\n git push origin HEAD:refs/for/master  Note: If you have posted a patch and don\u0026rsquo;t receive any reviews, you may need to prod the reviewers. You can do this by adding a reply to your changeset review on gerrit. It is expected that at least the maintainer will supply a review for your patch.\n"
},
{
	"uri": "http://localhost/getting-started/stats/",
	"title": "Stats",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost/cpu-models/",
	"title": "CPU Models",
	"tags": [],
	"description": "",
	"content": " CPU Models "
},
{
	"uri": "http://localhost/arch/x86/micro-op-isa/",
	"title": "Micro-op ISA",
	"tags": [],
	"description": "",
	"content": " Register Ops These microops typically take two sources and produce one result. Most have a version that operates on only registers and a version which operates on registers and an immediate value. Some optionally set flags according to their operation. Some of them can be predicated.\nAdd Addition.\nadd Dest, Src1, Src2 Dest = Dest \u0026lt;- Src1 + Src2\nAdds the contents of the Src1 and Src2 registers and puts the result in the Dest register.\naddi Dest, Src1, Imm Dest = Dest \u0026lt;- Src1 + Imm\nAdds the contents of the Src1 register and the immediate Imm and puts the result in the Dest register.\nFlags This microop optionally sets the CF, ECF, ZF, EZF, PF, AF, SF, and OF flags.\n   CF and ECF   The carry out of the most significant bit.     ZF and EZF   Whether the result was zero.     PF   The parity of the result.     AF   The carry from the 4th to 5th bit positions.     SF   The sign of the result.     OF   Whether there was an overflow.    Adc Add with carry.\nadc Dest, Src1, Src2 Dest = Dest \u0026lt;- Src1 + Src2 + CF\nAdds the contents of the Src1 and Src2 registers and the carry flag and puts the result in the Dest register.\nadci Dest, Src1, Imm Dest = Dest \u0026lt;- Src1 + Imm + CF\nAdds the contents of the Src1 register, the immediate Imm, and the carry flag and puts the result in the Dest register.\nFlags This microop optionally sets the CF, ECF, ZF, EZF, PF, AF, SF, and OF flags.\n   CF and ECF   The carry out of the most significant bit.     ZF and EZF   Whether the result was zero.     PF   The parity of the result.     AF   The carry from the 4th to 5th bit positions.     SF   The sign of the result.     OF   Whether there was an overflow.    Sub Subtraction.\nsub Dest, Src1, Src2 Dest = Dest \u0026lt;- Src1 - Src2\nSubtracts the contents of the Src2 register from the Src1 register and puts the result in the Dest register.\nsubi Dest, Src1, Imm Dest = Dest \u0026lt;- Src1 - Imm\nSubtracts the contents of the immediate Imm from the Src1 register and puts the result in the Dest register.\nFlags This microop optionally sets the CF, ECF, ZF, EZF, PF, AF, SF, and OF flags.\n   CF and ECF   The barrow into of the most significant bit.     ZF and EZF   Whether the result was zero.     PF   The parity of the result.     AF   The barrow from the 5th to 4th bit positions.     SF   The sign of the result.     OF   Whether there was an overflow.    Sbb Subtract with barrow.\nsbb Dest, Src1, Src2 Dest = Dest \u0026lt;- Src1 - Src2 - CF\nSubtracts the contents of the Src2 register and the carry flag from the Src1 register and puts the result in the Dest register.\nsbbi Dest, Src1, Imm Dest = Dest \u0026lt;- Src1 - Imm - CF\nSubtracts the immediate Imm and the carry flag from the Src1 register and puts the result in the Dest register.\nFlags This microop optionally sets the CF, ECF, ZF, EZF, PF, AF, SF, and OF flags.\n   CF and ECF   The barrow into of the most significant bit.     ZF and EZF   Whether the result was zero.     PF   The parity of the result.     AF   The barrow from the 5th to 4th bit positions.     SF   The sign of the result.     OF   Whether there was an overflow.    Mul1s Signed multiply.\nmul1s Src1, Src2 ProdHi:ProdLo = Src1 * Src2\nMultiplies the unsigned contents of the Src1 and Src2 registers and puts the high and low portions of the product into the internal registers ProdHi and ProdLo, respectively.\nmul1si Src1, Imm ProdHi:ProdLo = Src1 * Imm\nMultiplies the unsigned contents of the Src1 register and the immediate Imm and puts the high and low portions of the product into the internal registers ProdHi and ProdLo, respectively.\nFlags This microop does not set any flags.\nMul1u Unsigned multiply.\nmul1u Src1, Src2 ProdHi:ProdLo = Src1 * Src2\nMultiplies the unsigned contents of the Src1 and Src2 registers and puts the high and low portions of the product into the internal registers ProdHi and ProdLo, respectively.\nmul1ui Src1, Imm ProdHi:ProdLo = Src1 * Imm\nMultiplies the unsigned contents of the Src1 register and the immediate Imm and puts the high and low portions of the product into the internal registers ProdHi and ProdLo, respectively.\nFlags This microop does not set any flags.\nMulel Unload multiply result low.\nmulel Dest Dest = Dest \u0026lt;- ProdLo\nMoves the value of the internal ProdLo register into the Dest register.\nFlags This microop does not set any flags.\nMuleh Unload multiply result high.\nmuleh Dest Dest = Dest \u0026lt;- ProdHi\nMoves the value of the internal ProdHi register into the Dest register.\nFlags This microop optionally sets the CF, ECF, and OF flags.\n   CF and ECF   Whether ProdHi is non-zero     OF   Whether ProdHi is non-zero.    Div1 First stage of division.\ndiv1 Src1, Src2 Quotient * Src2 + Remainder = Src1 Divisor = Src2\nBegins a division operation where the contents of SrcReg1 is the high part of the dividend and the contents of SrcReg2 is the divisor. The remainder from this partial division is put in the internal register Remainder. The quotient is put in the internal register Quotient. The divisor is put in the internal register Divisor.\ndiv1i Src1, Imm: Quotient * Imm + Remainder = Src1 Divisor = Imm\nBegins a division operation where the contents of SrcReg1 is the high part of the dividend and the immediate Imm is the divisor. The remainder from this partial division is put in the internal register Remainder. The quotient is put in the internal register Quotient. The divisor is put in the internal register Divisor.\nFlags This microop does not set any flags.\nDiv2 Second and later stages of division.\ndiv2 Dest, Src1, Src2 Quotient * Divisor + Remainder = original Remainder with bits shifted in from Src1\nDest = Dest \u0026lt;- Src2 - number of bits shifted in above\nPerforms subsequent steps of division following a div1 instruction. The contents of the register Src1 is the low portion of the dividend. The contents of the register Src2 denote the number of bits in Src1 that have not yet been used before this step in the division. Dest is set to the number of bits in Src1 that have not been used after this step. The internal registers Quotient, Divisor, and Remainder are updated by this instruction.\nIf there are no remaining bits in Src1, this instruction does nothing except optionally compute flags.\ndiv2i Dest, Src1, Imm Quotient * Divisor + Remainder = original Remainder with bits shifted in from Src1\nDest = Dest \u0026lt;- Imm - number of bits shifted in above\nPerforms subsequent steps of division following a div1 instruction. The contents of the register Src1 is the low portion of the dividend. The immediate Imm denotes the number of bits in Src1 that have not yet been used before this step in the division. Dest is set to the number of bits in Src1 that have not been used after this step. The internal registers Quotient, Divisor, and Remainder are updated by this instruction.\nIf there are no remaining bits in Src1, this instruction does nothing except optionally compute flags.\nFlags This microop optionally sets the EZF flag.\n   EZF   Whether there are any remaining bits in Src1 after this step.    Divq Unload division quotient.\ndivq Dest Dest = Dest \u0026lt;- Quotient\nMoves the value of the internal Quotient register into the Dest register.\nFlags This microop does not set any flags.\nDivr Unload division remainder.\ndivr Dest Dest = Dest \u0026lt;- Remainder\nMoves the value of the internal Remainder register into the Dest register.\nFlags This microop does not set any flags.\nOr Logical or.\nor Dest, Src1, Src2 Dest = Dest \u0026lt;- Src1 | Src2\nComputes the bitwise or of the contents of the Src1 and Src2 registers and puts the result in the Dest register.\nori Dest, Src1, Imm Dest = Dest \u0026lt;- Src1 | Imm\nComputes the bitwise or of the contents of the Src1 register and the immediate Imm and puts the result in the Dest register.\nFlags This microop optionally sets the CF, ECF, ZF, EZF, PF, AF, SF, and OF flags. There is nothing that prevents computing a value for the AF flag, but it\u0026rsquo;s value will be meaningless.\n   CF and ECF   Cleared     ZF and EZF   Whether the result was zero.     PF   The parity of the result.     AF   Undefined     SF   The sign of the result.     OF   Cleared    And Logical And\nand Dest, Src1, Src2 Dest = Dest \u0026lt;- Src1 \u0026amp; Src2\nComputes the bitwise and of the contents of the Src1 and Src2 registers and puts the result in the Dest register.\nandi Dest, Src1, Imm Dest = Dest \u0026lt;- Src1 \u0026amp; Imm\nComputes the bitwise and of the contents of the Src1 register and the immediate Imm and puts the result in the Dest register.\nFlags This microop optionally sets the CF, ECF, ZF, EZF, PF, AF, SF, and OF flags. There is nothing that prevents computing a value for the AF flag, but it\u0026rsquo;s value will be meaningless.\n   CF and ECF   Cleared     ZF and EZF   Whether the result was zero.     PF   The parity of the result.     AF   Undefined     SF   The sign of the result.     OF   Cleared    Xor Logical exclusive or.\nxor Dest, Src1, Src2 Dest = Dest \u0026lt;- Src1 | Src2\nComputes the bitwise xor of the contents of the Src1 and Src2 registers and puts the result in the Dest register.\nxori Dest, Src1, Imm Dest = Dest \u0026lt;- Src1 | Imm\nComputes the bitwise xor of the contents of the Src1 register and the immediate Imm and puts the result in the Dest register.\nFlags This microop optionally sets the CF, ECF, ZF, EZF, PF, AF, SF, and OF flags. There is nothing that prevents computing a value for the AF flag, but it\u0026rsquo;s value will be meaningless.\n   CF and ECF   Cleared     ZF and EZF   Whether the result was zero.     PF   The parity of the result.     AF   Undefined     SF   The sign of the result.     OF   Cleared    Sll Logical left shift.\nsll Dest, Src1, Src2 Dest = Dest \u0026lt;- Src1 \u0026lt;\u0026lt; Src2\nShifts the contents of the Src1 register to the left by the value in the Src2 register and writes the result into the Dest register. The shift amount is truncated to either 5 or 6 bits, depending on the operand size.\nslli Dest, Src1, Imm Dest = Dest \u0026lt;- Src1 \u0026lt;\u0026lt; Imm\nShifts the contents of the Src1 register to the left by the value in the immediate Imm and writes the result into the Dest register. The shift amount is truncated to either 5 or 6 bits, depending on the operand size.\nFlags This microop optionally sets the CF, ECF, and OF flags. If the shift amount is zero, no flags are modified.\n   CF and ECF   The last bit shifted out of the result.     OF   The exclusive or of the what this instruction would set the CF flag to (if requested) and the most significant bit of the result    Srl Logical right shift.\nsrl Dest, Src1, Src2 Dest = Dest \u0026lt;- Src1 \u0026gt;\u0026gt;(logical) Src2\nShifts the contents of the Src1 register to the right by the value in the Src2 register and writes the result into the Dest register. Bits which are shifted in sign extend the result. The shift amount is truncated to either 5 or 6 bits, depending on the operand size.\nsrli Dest, Src1, Imm Dest = Dest \u0026lt;- Src1 \u0026gt;\u0026gt;(logical) Imm\nShifts the contents of the Src1 register to the right by the value in the immediate Imm and writes the result into the Dest register. Bits which are shifted in sign extend the result. The shift amount is truncated to either 5 or 6 bits, depending on the operand size.\nFlags This microop optionally sets the CF, ECF, and OF flags. If the shift amount is zero, no flags are modified.\n   CF and ECF   The last bit shifted out of the result.     OF   The most significant bit of the original value to shift    Sra Arithmetic right shift.\nsra Dest, Src1, Src2 Dest = Dest \u0026lt;- Src1 \u0026gt;\u0026gt;(arithmetic) Src2\nShifts the contents of the Src1 register to the right by the value in the Src2 register and writes the result into the Dest register. Bits which are shifted in zero extend the result. The shift amount is truncated to either 5 or 6 bits, depending on the operand size.\nsrai Dest, Src1, Imm Dest = Dest \u0026lt;- Src1 \u0026gt;\u0026gt;(arithmetic) Imm\nShifts the contents of the Src1 register to the right by the value in the immediate Imm and writes the result into the Dest register. Bits which are shifted in zero extend the result. The shift amount is truncated to either 5 or 6 bits, depending on the operand size.\nFlags This microop optionally sets the CF, ECF, and OF flags. If the shift amount is zero, no flags are modified.\n   CF and ECF   The last bit shifted out of the result.     OF   Cleared    Ror Rotate right.\nror Dest, Src1, Src2 Rotates the contents of the Src1 register to the right by the value in the Src2 register and writes the result into the Dest register. The rotate amount is truncated to either 5 or 6 bits, depending on the operand size.\nrori Dest, Src1, Imm Rotates the contents of the Src1 register to the right by the value in the immediate Imm and writes the result into the Dest register. The rotate amount is truncated to either 5 or 6 bits, depending on the operand size.\nFlags This microop optionally sets the CF, ECF, and OF flags. If the rotate amount is zero, no flags are modified.\n   CF and ECF   The most significant bit of the result.     OF   The exclusive or of the two most significant bits of the original value.    Rcr Rotate right through carry.\nrcr Dest, Src1, Src2 Rotates the contents of the Src1 register through the carry flag and to the right by the value in the Src2 register and writes the result into the Dest register. The rotate amount is truncated to either 5 or 6 bits, depending on the operand size.\nrcri Dest, Src1, Imm Rotates the contents of the Src1 register through the carry flag and to the right by the value in the immediate Imm and writes the result into the Dest register. The rotate amount is truncated to either 5 or 6 bits, depending on the operand size.\nFlags This microop optionally sets the CF, ECF, and OF flags. If the rotate amount is zero, no flags are modified.\n   CF and ECF   The last bit shifted out of the result.     OF   The exclusive or of the CF flag before the rotate and the most significant bit of the original value.    Rol Rotate left.\nrol Dest, Src1, Src2 Rotates the contents of the Src1 register to the left by the value in the Src2 register and writes the result into the Dest register. The rotate amount is truncated to either 5 or 6 bits, depending on the operand size.\nroli Dest, Src1, Imm Rotates the contents of the Src1 register to the left by the value in the immediate Imm and writes the result into the Dest register. The rotate amount is truncated to either 5 or 6 bits, depending on the operand size.\nFlags This microop optionally sets the CF, ECF, and OF flags. If the rotate amount is zero, no flags are modified.\n   CF and ECF   The least significant bit of the result.     OF   The exclusive or of the most and least significant bits of the result.    Rcl Rotate left through carry.\nrcl Dest, Src1, Src2 Rotates the contents of the Src1 register through the carry flag and to the left by the value in the Src2 register and writes the result into the Dest register. The rotate amount is truncated to either 5 or 6 bits, depending on the operand size.\nrcli Dest, Src1, Imm Rotates the contents of the Src1 register through the carry flag and to the left by the value in the immediate Imm and writes the result into the Dest register. The rotate amount is truncated to either 5 or 6 bits, depending on the operand size.\nFlags This microop optionally sets the CF, ECF, and OF flags. If the rotate amount is zero, no flags are modified.\n   CF and ECF   The last bit rotated out of the result.     OF   The exclusive or of CF before the rotate and most significant bit of the result.    Mov Move.\nmov Dest, Src1, Src2 Dest = Src1 \u0026lt;- Src2\nMerge the contents of the Src2 register into the contents of Src1 and put the result into the Dest register.\nmovi Dest, Src1, Imm Dest = Src1 \u0026lt;- Imm\nMerge the contents of the immediate Imm into the contents of Src1 and put the results into the Dest register.\nFlags This microop does not set any flags. It is optionally predicated.\nSext Sign extend.\nsext Dest, Src1, Imm Dest = Dest \u0026lt;- sign_extend(Src1, Imm)\nSign extend the value in the Src1 register starting at the bit position in the immediate Imm, and put the result in the Dest register.\nFlags This microop does not set any flags.\nZext Zero extend.\nzext Dest, Src1, Imm Dest = Dest \u0026lt;- zero_extend(Src1, Imm)\nZero extend the value in the Src1 register starting at the bit position in the immediate Imm, and put the result in the Dest register.\nFlags This microop does not set any flags.\nRuflag Read user flag.\nruflag Dest, Imm Reads the user level flag stored in the bit position specified by the immediate Imm and stores it in the register Dest.\nThe mapping between values of Imm and user level flags is show in the following table.\n   0   CF (carry flag)     2   PF (parity flag)     3   ECF (emulation carry flag)     4   AF (auxiliary carry flag)     5   EZF (emulation zero flag)     6   ZF (zero flag)     7   SF (sign flag)     10   DF (direction flag)     11   OF (overflow flag)    Flags The EZF flag is always set. In the future this may become optional.\n   EZF   Set if the value of the flag read was zero.    Ruflags Read all user flags.\nruflags Dest Dest = user flags\nStore the user level flags into the Dest register.\nFlags This microop does not set any flags.\nWruflags Write all user flags.\nwruflags Src1, Src2 user flags = Src1 ^ Src2\nSet the user level flags to the exclusive or of the Src1 and Src2 registers.\nwruflagsi Src1, Imm user flags = Src1 ^ Imm\nSet the user level flags to the exclusive or of the Src1 register and the immediate Imm.\nFlags See above.\nRdip Read the instruction pointer.\nrdip Dest Dest = rIP\nSet the Dest register to the current value of rIP.\nFlags This microop does not set any flags.\nWrip Write the instruction pointer.\nwrip Src1, Src2 rIP = Src1 + Src2\nSet the rIP to the sum of the Src1 and Src2 registers. This causes a macroop branch at the end of the current macroop.\nwripi Src1, Imm micropc = Src1 + Imm\nSet the rIP to the sum of the Src1 register and immediate Imm. This causes a macroop branch at the end of the current macroop.\nFlags This microop does not set any flags. It is optionally predicated.\nChks Check selector.\nNot yet implemented.\nLoad/Store Ops Ld Load.\nld Data, Seg, Sib, Disp Loads the integer register Data from memory.\nLdf Load floating point.\nldf Data, Seg, Sib, Disp Loads the floating point register Data from memory.\nLdm Load multimedia.\nldm Data, Seg, Sib, Disp Load the multimedia register Data from memory. This is not implemented and may never be.\nLdst Load with store check.\nLdst Data, Seg, Sib, Disp Load the integer register Data from memory while also checking if a store to that location would succeed. This is not implemented currently.\nLdstl Load with store check, locked.\nLdst Data, Seg, Sib, Disp Load the integer register Data from memory while also checking if a store to that location would succeed, and also provide the semantics of the \u0026ldquo;LOCK\u0026rdquo; instruction prefix. This is not implemented currently.\nSt Store.\nst Data, Seg, Sib, Disp Stores the integer register Data to memory.\nStf Store floating point.\nstf Data, Seg, Sib, Disp Stores the floating point register Data to memory.\nStm Store multimedia.\nstm Data, Seg, Sib, Disp Store the multimedia register Data to memory. This is not implemented and may never be.\nStupd Store with base update.\nStupd Data, Seg, Sib, Disp Store the integer register Data to memory and update the base register.\nLea Load effective address.\nlea Data, Seg, Sib, Disp Calculates the address for this combination of parameters and stores it in Data.\nCda Check data address.\ncda Seg, Sib, Disp Check whether the data address is valid. This is not implemented currently.\nCdaf CDA with cache line flush.\ncdaf Seg, Sib, Disp Check whether the data address is valid, and flush cache lines This is not implemented currently.\nCia Check instruction address.\ncia Seg, Sib, Disp Check whether the instruction address is valid. This is not implemented currently.\nTia TLB invalidate address\ntia Seg, Sib, Disp Invalidate the tlb entry which corresponds to this address. This is not implemented currently.\nLoad immediate Op Limm limm Dest, Imm Stores the 64 bit immediate Imm into the integer register Dest.\nFloating Point Ops Movfp movfp Dest, Src Dest = Src\nMove the contents of the floating point register Src into the floating point register Dest.\nThis instruction is predicated.\nXorfp xorfp Dest, Src1, Src2 Dest = Src1 ^ Src2\nCompute the bitwise exclusive or of the floating point registers Src1 and Src2 and put the result in the floating point register Dest.\nSqrtfp sqrtfp Dest, Src Dest = sqrt(Src)\nCompute the square root of the floating point register Src and put the result in floating point register Dest.\nAddfp addfp Dest, Src1, Src2 Dest = Src1 + Src2\nCompute the sum of the floating point registers Src1 and Src2 and put the result in the floating point register Dest.\nSubfp subfp Dest, Src1, Src2 Dest = Src1 - Src2\nCompute the difference of the floating point registers Src1 and Src2 and put the result in the floating point register Dest.\nMulfp mulfp Dest, Src1, Src2 Dest = Src1 * Src2\nCompute the product of the floating point registers Src1 and Src2 and put the result in the floating point register Dest.\nDivfp divfp Dest, Src1, Src2 Dest = Src1 / Src2\nDivide Src1 by Src2 and put the result in the floating point register Dest.\nCompfp compfp Src1, Src2 Compare floating point registers Src1 and Src2.\nCvtf_i2d cvtf_i2d Dest, Src Convert integer register Src into a double floating point value and store the result in the lower part of Dest.\nCvtf_i2d_hi cvtf_i2d_hi Dest, Src Convert integer register Src into a double floating point value and store the result in the upper part of Dest.\nCvtf_d2i cvtf_d2i Dest, Src Convert floating point register Src into an integer value and store the result in the integer register Dest.\nSpecial Ops Fault Generate a fault.\nfault fault_code Uses the C++ code fault_code to allocate a Fault object to return.\nLddha Set the default handler for a fault. This is not implemented currently.\nLdaha Set the alternate handler for a fault This is not implemented currently.\nSequencing Ops These microops are used for control flow withing microcode\nBr Microcode branch. This is never considered the last microop of a sequence. If it appears at the end of a macroop, it is assumed that it branches to microcode in the ROM.\nbr target micropc = target\nSet the micropc to the 16 bit immediate target.\nFlags This microop does not set any flags. It is optionally predicated.\nEret Return from emulation. This instruction is always considered the last microop in a sequence. When executing from the ROM, it is the only way to return to normal instruction decoding.\neret Return from emulation.\nFlags This microop does not set any flags. It is optionally predicated.\n"
},
{
	"uri": "http://localhost/arch/x86/",
	"title": "X86",
	"tags": [],
	"description": "",
	"content": " X86 This section contains some of the implementation specifics of the x86 support in gem5\n"
},
{
	"uri": "http://localhost/gem5-101/caches/",
	"title": "Caches",
	"tags": [],
	"description": "",
	"content": "After looking at the processor core, we now turn our attention to the cache hierarchy. We continue our focus on experimentation, and consider tradeoffs in cache design such as replacement policies and set-associativity. Furthermore, we also learn more about the gem5 simulator, and create our first simObject!\n"
},
{
	"uri": "http://localhost/getting-started/default-configs/",
	"title": "Default Configuration Files",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost/docs/",
	"title": "Documentation",
	"tags": [],
	"description": "",
	"content": "foo ntahoeus nat ohusnt aoeusnth aoesunth aoseunth\naoeu aoue aoeu ao eu\n"
},
{
	"uri": "http://localhost/ruby/",
	"title": "Ruby Memory System",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": " How do I get started?  Take a look at the documentation, specifically the video on the Introduction page. Then the video on Running gem5 is helpful. If you have any questions, check this page and search this wiki before posting to the Mailing Lists. Additionally, we encourage you to take the gem5 101 course, which goes through a lot of gem5 basics.  Device Related How can I see the packets on the ethernet link?  By creating a Etherdump object, setting it\u0026rsquo;s file parameter, and setting the dump parameter on the EtherLink to it. This is easily accomplished with our fs.py example configuration by adding the command line option \u0026ndash;etherdump=. The resulting file will be named  and be in a standard pcap format.  Compiling Related M5 build fails during linking, but SCons passes check for libpython2.X  You probably have libpython in a non-standard path and need to set LD_LIBRARY_PATH  How do I compile a version of M5 that can use EIO traces?  This is done by passing the EXTRAS argument to scons when you build M5. See Extras for more information.  How do I compile M5 as a shared library?  This is done by specifying a different target in scons  scons -j 4 build/X86/libm5_debug.so CPU_MODELS=TimingSimpleCPU\nOS/Linux Related What is a disk image?  A disk image is just a raw data file\u0026hellip;we use disk images to fake a real hard disk. You can use the util/mkblankimage.sh script in the m5 repository to create a blank image of arbitrary size.  How do I add files to a disk image?  Using either sudo or the root account run /bin/mount -o loop,offset=32256 /z/foo.img /mount/point. You can then copy the desired files to the image. (If you don\u0026rsquo;t have root access on a real machine, you can always create a virtual machine using a tool like VirtualBox or QEMU and mount the image inside the VM.) Remember to unmount it before running the simulator with /bin/umount /mount/point or you may get unexpected results. This is a hack-y method, what you should do to add new binaries to M5 is modify linux-dist and place it in that build structure. The offset parameter is required because our disk images contain both a partition table and the partition. The partition table occupies the first 32256 bytes of the image.  {{#ev:youtube|OXH1oxQbuHA|400|center|A youtube video of adding a file to an .img file on Ubuntu 12.04 64bit. Video resolution can be set to 1080}}\n{{#ev:youtube|Oh3NK12fnbg|400|center|A youtube video of installing programs within an .img file using qemu on Ubuntu 12.04 64bit. Video resolution can be set to 1080}}\nNote: The 2nd video uses an Ubuntu Natty image. Since Natty is now quite old, you will need to update your /etc/apt/sources.list file, on the mounted image. Use, e.g., sudo vi /etc/apt/sources.list, and replace http://old-releases.ubuntu.com/ubuntu/ in place of http://ports.ubuntu.com/. Also, for things to work as show in the video, you will need to have have sshd running on the host machine.\nWhat if I need more space than is available on the disk image?  There are several ways to do this:   You can get the blank image script from the m5/util directory, create a new blank image that is the desired size, and then mount both images and copy the contents of the 50MB one to your new one. You can use the 50MB image to boot the system but then mount a second disk image that has your benchmark on it by adding the image to FSConfig.py and then mounting it with an rcS script You can build an entirely new image of the desired size with PTX dist (available from the Download page).  Can I use MIPS SDE Lite to cross-compile for M5?  The MIPS SDE Lite package is a very good tool but unfortunately it will not work if you would like to cross-compile a MIPS/Linux binary and run it on M5. The MIPS SDE package contains some SDE-specific startup routines and glibc calls that M5 will not be able to support. The cross-compiler solution we found to work is crosstool. For MIPS, gcc 3.4.4 and glibc 2.3.5 seems to work.  Running Related What does \u0026ldquo;Global frequency set at 1000000000000 ticks per second\u0026rdquo; refer to?  The smallest unit of time gem5 understands is 1 picosecond (since 1 picosecond = (1 second)/1000000000000).  I compiled an executable to run in syscall emulation mode but it doesn\u0026rsquo;t work.  If M5 seems to initialize OK, but the CPU never fetches any instructions, it may be because your executable is dynamically linked. M5 does not support dynamic linking (shared libraries) in syscall emulation mode. You must recompile the executable and have it statically linked. With gcc, just add the \u0026ldquo;-static\u0026rdquo; flag to the command line.  How many CPUs can M5 run?  There is no inherent limit in M5 (other than simulation speed). In SE mode there are no obstacles to simulating as many CPUs as you want. However, in FS mode, the real-world Alpha platform we model (Tsunami) only supports up to 4 processors. To get around this limit, we defined and implemented a variant of the Tsunami platform (which we call BigTsunami) that can take up to 64 processors. Note that BigTsunami does not correspond to any real system. BigTsunami support is included in the standard M5 Alpha build, but booting with more than 4 CPUs requires modifications to the PAL code and kernel as well. Take a look at the Download page for our Linux patches and modified PAL code. Note that even with the BigTsunami changes, simulating 64 processors will be quite slow, and the Linux scheduler doesn\u0026rsquo;t seem particularly good at scheduling a large number of processors.  Prebuilt kernel and PAL binaries can be found at: http://www.cs.utexas.edu/~cart/parsec_m5/\nHow do I see what options I can set?  Using the \u0026lsquo;-h\u0026rsquo; flag will show what options M5 can take in general. Running m5.debug foo.py -h (or any m5 binary variant) will list all options that are available based on M5\u0026rsquo;s internal options and the options defined in the .py file.  How do I run SPEC cpu2000 benchmarks on M5?  See the SPEC2000 benchmarks page.  How do I run SPEC cpu2006 benchmarks on M5?  See the SPEC2006 benchmarks page.  How do I run SPLASH benchmarks on M5?  See the Splash benchmarks page.  When I try to run the simulator and I get an error: ImportError: Can\u0026rsquo;t find a path to system files.  You have not installed the full-system files (disk images, kernels, and other binaries) or have not setup the path to them correctly. See Getting Additional Tools and Files.  How do I run multiprogram workloads on M5?  In SE mode, simply create a system with multiple CPUs and assign a different workload object to each CPU\u0026rsquo;s workload parameter. If you\u0026rsquo;re using the O3 model, you can also assign a vector of workload objects to one CPU, in which case the CPU will run all of the workloads concurrently in SMT mode. Note that SE mode has no thread scheduling; if you need a scheduler, run in FS mode and use the fine scheduler built into the Linux kernel.  How do I terminate multiprogram workloads?  There are some very fundamental issues with whatever approach you choose. Here are your options:   Terminate as soon as any thread reaches a particular maximum number of instructions. This option is equivalent to max_insts_any_thread. The potential problem here is that because of the inherent non-determinism of multithreaded programs, there is no way to ensure that all experiments do the same work. You might also not get the same amount of work done. For example, if you have two threads, one of them must reach the maximum. The other could either execute no instructions, or could execute max-1 instructions. The benefit of this approach is that all threads are running fully until the simulation terminates (provided that none of the threads terminate early due to some other condition.) Terminate once all threads have reached a maximum number of instructions. This option is equivalent to max_insts_all_threads. In this mode, we make sure all threads do at least a certain amount of work, but threads that reach the maximum continue executing. This has the same benefit as the previous example, but also suffers from the problem that non-determinism will cause you to potentially not do the same amount of total work. In this unimplemented mode, all threads would run for exactly a specified number of instructions with some threads terminating early. All threads will do the same amount of work thus avoiding the problem of the previous options. The downside of this option is that the threads may not all be running for the entire simulation. For example, one thread might finish its instructions almost right away, while the other thread has quite a bit left to do. When this happens, you\u0026rsquo;re only running a multiprogram workload for a fraction of the total time. Another unimplemented option could be to specify how many instructions each thread has to complete before exiting. This is not implemented, but would allow a balance to be struck between options 1 and 2 if the user experimented to figure out what a good mix was.  If you want to implement either of the unimplemented options, or if you have other ideas, please let us know!\nHow do I use the sampler? The sampler from the previous version of M5 has been replaced with functionality via Python. See the Sampling documentation for details.\nDebugging and Error Related When running my program I get an error \u0026ldquo;Error: syscall unimplemented.\u0026rdquo;  That means that one of the libraries that you are linking with uses a syscall that is not emulated. You can do a man on the syscall name to see what the syscall is supposed to do and then try to implement at least whatever functionality your application needs. Or you can try the quick \u0026amp; dirty approach, which is to change the function pointer for syscall in arch///.cc from unimplementedFunc to ignoreFunc, which will make it print a warning and keep going rather than terminate with an error. No guarantees that your program won\u0026rsquo;t crash because of this though.  How do I access reference counted pointers in GDB?  Objects such as DynInsts are reference counted, making it slightly harder to obtain the data inside. In gdb you must access them through the pointer that is stored in the ref counted pointer, which is called data. Thus given a ref counted pointer ptr, in gdb you would say (gdb) ptr-\u0026gt;data to get the pointer to the actual object.  I get an error \u0026ldquo;undefined reference to `void foo()\u0026lsquo;\u0026rdquo; when the compiler is doing the final linking.  This is due to having a function that is declared but never defined. Either you forgot to define it, or are not compiling in the file that defines it. In the case of templated code, you may be including the wrong file or you may not have manually instantiated a templated class that needs to be manually instantiated.  When I\u0026rsquo;m running in SE mode I get warnings about unimplemented or ignored system calls or trapping modes.  M5 does not support IEEE FP floating point traps (underflow, overflow, etc.) and as a result doesn\u0026rsquo;t bother supporting the system calls that enable/disable these traps or set the corresponding trap handlers. It\u0026rsquo;s pretty unlikely your application relies on them (we haven\u0026rsquo;t seen one yet that does). As long as everything else seems to work you can ignore the warning.  Miscellaneous Where are the classes for each ISA instruction? Where are the execute() methods for the StaticInst class?  Both the classes and the execute() methods are generated through Python upon building any version of M5. For example, After building ALPHA, they will be located in the build/ALPHA/arch/alpha/ folder. The key files are decoder.hh, decoder.cc (which describe the ISA instructions), and *_exec.cc (which describe the execute() methods). The definitions for both exist in the .isa files found in src/arch/*/isa/, which are processed by src/arch/isa_parser.py to generate the previously mentioned .hh/.cc files.  Is fast-forwarding supported in SMT mode?  It is not currently supported. The SimpleCPU doesn\u0026rsquo;t support SMT, so it doesn\u0026rsquo;t support fast-forwarding in SMT mode. However it should be feasible given some hacking on the SimpleCPU or the Sampler.  I\u0026rsquo;ve created a new file, how do I get SCons to compile it.  Add the file to the source list in the SConscript in the current directory or (if there is none) the closest parent directory.  I\u0026rsquo;ve got a new SimObject compiled but I can\u0026rsquo;t use it.  You need to both have a Python version of your object defined (see the various py files in src/  /* and C++ file need to have the Params::create() function difned for your particular object.  How do I use a normal variable in a statistic formula?  This is not supported. Just create a Scalar\u0026lt;\u0026gt; statistic that does the same thing as your normal variable and use that in the formula instead.  What are all these *_impl.hh files?  There is a lot of templated code used within M5, and these *_impl.hh are used to make it a little easier to organize things. Normally template functions must be entirely included in the header file in order to not require the programmer to manually instantiate the copies of the template functions. However, when you have an entire class that\u0026rsquo;s templated, the header files quickly become bloated and too big, except for small helper classes. Thus we put the declarations in the header file as normal, the definitions in the *_impl.hh file, and the manual instantiations in the *.cc file. This makes it easier to sort out the instantiations from the definitions. Also if there are only a few templated functions inside a non-templated class, it may be possible to include the functions in the *_impl.hh file and not have to manually instantiate the functions. You just need to include the *_impl.hh file in any .cc files that use the templated function. See mem/packet{.hh|_impl.hh|.cc} for an example.  What if SCons complains that it can\u0026rsquo;t find a file I just deleted?  Delete the scons.dblite file in the m5 directory.  Which config files are used for syscall emulation? Full system mode? configs/example/se.py is a sample configuration file for syscall emulation. Similarly, configs/example/fs.py is a sample configuration for full system simulations. Both these files include files in the configs/common/ directory.\nWhere does the stack and program arguments get setup for a process in Syscall Emulation mode? A good point of reference for this is the \u0026ldquo;argsInit\u0026rdquo; function in the src/sim/process.cc file. For Syscall Emulation, each process is given a \u0026ldquo;LiveProcess\u0026rdquo; object and that function initializes the arguments to that process and also sets up the initial stack for that process. You\u0026rsquo;ll also notice that the architecture specific Process objects (e.g. AlphaLinuxLiveProcess found in arch/alpha/linux/process.hh) derive from the LiveProcess object too.\nWhat should I do when I get a loader error in a HelloDeviceParams function? When creating a new HelloDevice class extending SimObject class (following the example in the ASPLOS tutorial slide 106-115), one might encounter loader errors like the following:\nbuild/X86/params/params_wrap.do: In function `_wrap_DeviceParams_create': m5/build/X86/params/params_wrap.cc:19999: undefined reference to `DeviceParams::create()' collect2: ld returned 1 exit status scons: *** [build/X86/m5.debug] Error 1 scons: building terminated because of errors.  This error is caused by NOT defining a HelloDeviceParams::create() function in the device.cc. One should add the following in the device.cc\nHelloDevice * HelloDeviceParams::create() { return new HelloDevice(this); }  "
},
{
	"uri": "http://localhost/arch/x86/segmentation/",
	"title": "Segmentation",
	"tags": [],
	"description": "",
	"content": " Segment bases When computing an address for a load or store, the segment base is added in before the address is sent to the CPU to actually perform the access. This has several advantages. First, because there are no alignment restrictions on segment bases, the virtual (pre-segmentation) address for an access could be aligned but produce an unaligned linear (post-segmentation) address. The opposite could also happen where an unaligned access becomes aligned. Once outside of the instruction, the majority of M5 doesn\u0026rsquo;t know about segmentation and wouldn\u0026rsquo;t be able to handle those sorts of situations. In CPUs like the O3 model which can do store to load forwarding, accesses to the same virtual address are expected to refer to the same piece of memory. By applying segment bases before the CPU gets the address, that remains true for x86. Because the base address for a particular segment isn\u0026rsquo;t always used, both the intended base and the effective base are stored. The effective base is the value that\u0026rsquo;s actually added into a virtual address.\nLimit and attribute checks Limit and attribute checks are performed by the TLB. The TLB is a convenient place to perform any type of address validation which could result in a fault, including these sorts of checks.\nImplemented segments Architected user segments These segments are for code or data and are specified by the ISA.\n CS - Code segment. DS - Default data segment. ES - Data segment. Implicitly used by string instructions. SS - Stack segment. FS, GS - Extra data segments.  M5 internal data segments These segments are specific to M5 and are used internally.\n HS - A temporary segment register. LS - A segment register which always has a flat segment with base address 0.  Architected system segments These segments are for system data structures and are specified by the ISA..\n TSL (LDT) - Local descriptor table. TSG (GDT) - Global descriptor table. TR (TSS) - Task state segment. IDTR (IDT) - Interrupt descriptor table.  Emulation memory The MS segment is not actually a segment. Instead, it\u0026rsquo;s a flag to the TLB that a particular address should be decoded as an access to an alternative address space. All alternative address spaces are 32 bits wide, so the upper 32 bits are used to specify which one to use. The following prefixes have been defined:\n 0x100000000 - Originally planned for calls to CPUID. CPUID will likely be implemented as a special function unit/function, so this may never be used. 0x200000000 - Access to an MSR. The MSR number needs to be scaled by the size of a MiscReg so that M5 doesn\u0026rsquo;t get confused and think adjacent MSRs overlap. 0x300000000 - IO port access.  "
},
{
	"uri": "http://localhost/benchmarks/",
	"title": "Benchmarks w/gem5",
	"tags": [],
	"description": "",
	"content": "This section contains instructions about running various benchmarks with gem5\n"
},
{
	"uri": "http://localhost/gem5-101/multi-core/",
	"title": "Multi Core",
	"tags": [],
	"description": "",
	"content": "For this last part, we go both multi-core and full system at the same time! We analyze the performance of a simple application on giving it more computational resources (cores). We also boot a full-fledged unmodified operating system (Linux) on the target system simulated by gem5. Most importantly, we teach you how to create your own, simpler version of the dreaded fs.py configuration script, one that you can feel comfortable modifying.\n"
},
{
	"uri": "http://localhost/contributing/committing/",
	"title": "Committing",
	"tags": [],
	"description": "",
	"content": "Each patch must meet the following criteria to be merged:\n At least one review with +2 At least one maintainer with +1 At least +1 from the CI tests (gem5 must build and run) At least +1 from the style checker  Once a patch meets the above criteria, the submitter of the patch will be able to merge the patch by pressing the \u0026ldquo;Submit\u0026rdquo; button on Gerrit. When the patch is submitted, it is merged into the public gem5 branch.\n"
},
{
	"uri": "http://localhost/getting-started/asking-for-help/",
	"title": "Asking for help",
	"tags": [],
	"description": "",
	"content": "Many of the people on the gem5-users mailing list are happy to help when someone has a problem or something doesn\u0026rsquo;t work. However, please keep in mind that it\u0026rsquo;s not our job to help. We all have other commitments, so before we spend time helping someone we like to see that they have put some effort into solving the problem themselves.\n Before posting a question to the list, please check if the question is already answered. The wiki provides answers to the most common questions (check both Documentation and the Frequently Asked Questions), and the mailing list archive at gmane.org is easily searchable. If your problem is with your configuration script, look at the config.ini output. This is the final configuration that\u0026rsquo;s getting built in textual form. Make sure this reflects the configuration you think you\u0026rsquo;re building. Make sure you\u0026rsquo;re running with gem5.opt or gem5.debug and not gem5.fast. The gem5.fast binary compiles out assertion checking for speed, so a problem that causes a crash or mysterious error on m5.fast may result in a more informative assertion failure with gem5.opt or gem5.debug. Try running your code on the latest version from the gem5 repository, if you\u0026rsquo;re not doing that already. Your problem may have been fixed since you last updated your local version. Take a few minutes to look at the source code and see if you can identify your problem. If you\u0026rsquo;re running into a specific error message, the file and line number where the error message is printed should be displayed along with the error message, so start there. Even if you don\u0026rsquo;t get a specific error message, look at the module that\u0026rsquo;s giving you trouble and see if you can figure it out. Use cscope or another tool to find where relevant functions/variables are used. gem5 is not like other open-source software packages where end users would never be expected to look at the source code when problems occur. gem5 is as much of a framework as an application. We don\u0026rsquo;t (and you shouldn\u0026rsquo;t) expect to have it do everything you want without touching the source code. If it seems appropriate, enable some debug flags (\u0026ndash;debug-flags=Foo) and see if the resulting information helps. Your examination of the source in step 3 should show you which trace flags might be relevant (they\u0026rsquo;re the first argument to the DPRINTF calls). If your problem is occurring on the C++ side, don\u0026rsquo;t be afraid to run under gdb to see what\u0026rsquo;s really happening. See Debugging for more details. If you still need help, use the information you\u0026rsquo;ve gathered in steps 1-4 to ask the most specific and informative question possible on the gem5-users list. Include the command line you used, specific error messages, program outputs, stack traces, relevant trace snippets, etc. (Don\u0026rsquo;t post huge traces in their entirety though\u0026hellip; just the relevant bits.) If you\u0026rsquo;ve written your own scripts, try and find the shortest script (or the minimum modification to one of the supplied example scripts) that exhibits the same problem and post that. If you found something on the wiki but it didn\u0026rsquo;t quite apply or didn\u0026rsquo;t work, mention that so we can update the wiki appropriately. If you have a theory about what the problem might be, please let us know, but include enough basic information so others can decide whether your theory is correct. If you have solved a problem that you reported on the list and the answer may be of general interest, post it to the list as a follow-up to the original thread so others can benefit from the solution.  Finally, please don\u0026rsquo;t e-mail or call any list member directly unless explicitly invited to do so. If we haven\u0026rsquo;t responded we are either busy, don\u0026rsquo;t know the answer or some combination of the two. Pestering will not get your question answered faster, and it may get it never answered at all.\n"
},
{
	"uri": "http://localhost/arch/x86/todo/",
	"title": "Todo list",
	"tags": [],
	"description": "",
	"content": " This todo list is likely out-of-date\n Highest priority Flesh out and debug 64-bit modern ISA (what\u0026rsquo;s needed by users) FS-mode core timing issues Debug TimingSimpleCPU issues? In-order pipeline core model support Out-of-order core model (O3) support  Multiprocessor timing support: need to enforce atomicity of locked load/op/store sequences in timing cache models Ruby and M5 classic?  Performance correlation With real hardware and/or existing correlated simulator Micro-op counts for functional implementation Timing for out-of-order core (requires O3 support)  Complete x87 support AVX support  Useful but not strictly necessary Split up ISA output for faster compiling Improve ISA description language support for x86  To do eventually but not right away ACPI support KVM-based fast functional CPU model Virtualization extensions support (AMD SVM, etc.)  Could be done but might never happen other OS support (OSX, Windows, ??) complete real mode support  "
},
{
	"uri": "http://localhost/getting-started/mailing-lists/",
	"title": "Mailing Lists",
	"tags": [],
	"description": "",
	"content": " gem5 has two main mailing lists where you can ask for help or advice. Before posting a problem to the mailing list please read Reporting Problems and follow the guidelines listed. Doing so will get your question answered far faster and more precisely.\nThere are two mailing lists for gem5 hosted on gem5.org:\n gem5-dev (subscribe) - gem5-dev is for developers who are working on the main version of gem5. This is the version that\u0026rsquo;s distributed from the website and most likely what you\u0026rsquo;ll base your own work off of. Bug reports should be sent here. This list receives emails from Gerrit reviews and cron test results. gem5-users (subscribe) - General discussion about gem5 and its use. gem5-users is a larger mailing list and is for people working on their own projects which are not, at least initially, going to be distributed as part of the official version of gem5. Most of the time gem5-users is the right mailing list to use. gem5-announce (subscribe) - Announcements related to gem5.  To subscribe or view the archives click on the links above.\nAdditional (and much more easily searchable) archives are hosted by:\n   List Gmane The Mail Archive     gem5-announce here here   gem5-users here here   gem5-dev here here   m5-dev here here         Alternative communication channels The following alternative support communication channels also exist, although they are less canonical not as actively monitored by the developers currently.\nIf you don\u0026rsquo;t get an answer in those channels quickly, make sure to also send the full text of your post to the correct mailing list with an additional link to the original post.\n https://stackoverflow.com/questions/tagged/gem5 Stack Overflow gem5 tag. Make sure that your question complies with the site guidelines before posting: https://stackoverflow.com/help/asking Interested users can opt to receive email notifications for such questions as explained at: https://meta.stackexchange.com/questions/25224/email-notifications-for-new-questions-matching-specific-tags https://www.quora.com/topic/Gem5-Simulator Stack Overflow is generally preferred, but you can use Quora instead if your question is not on topic for Stack Overflow, e.g. for more subjective questions  "
},
{
	"uri": "http://localhost/tutorials/isca-2015/",
	"title": "ISCA 2015 - 2nd User Workshop",
	"tags": [],
	"description": "",
	"content": "     Second gem5 User Workshop\n June 14th, 2015; Portland, OR\n       Following up from a successful 2012 workshop, it is time for the 2015 edition of the gem5 user workshop. The primary objective of this workshop is to bring together groups across the community who are actively using gem5. Discussion topics will include the activity of the gem5 community, how we can best leverage each others contributions, and how we continue to make gem5 a successful, community-supported simulation framework. Those who will get the most out of the workshop are current users of gem5, although anyone is welcome to attend.\nThe key part of the workshop is a set of presentations from the community about how individuals or groups are using the simulator, any features you have added that might be useful to others, and any major pain points, and what can be done to make gem5 better and more broadly adopted. The hope is that this will provide a forum for people with similar uses or needs to connect with each other.\nFinal Program    Topic Time Presenter Affiliation     Introduction \u0026amp; Overview of Changes 9:00 AM Steve Reinhardt AMD   Classic Memory System Re-visited 9:30 AM Andreas Hansson ARM   User Perspectives      AMD\u0026rsquo;s gem5 APU Simulator 10:00 AM Brad Beckmann AMD   NoMali: Understanding the Impact of Software Rendering Using a Stub GPU 10:15 AM Andreas Sandberg ARM   Cycle-Accurate STT-MRAM model in gem5 10:30 AM Cong Ma University of Minnesota   An Accurate and Detailed Prefetching Simulation Framework for gem5 10:45 AM Martí Torrents Lapuerta Polytechnic University of Catalonia   Break 11:00 AM     Supporting Native PThreads in SE Mode 11:30 AM Brandon Potter AMD   Dynamically Linked Executables in SE Mode 11:45 AM Brandon Potter AMD   Coupling gem5 with SystemC TLM 2.0 Virtual Platforms 12:00 PM Matthias Jung University of Kaiserslautern   SST/gem5 Integration 12:15 PM Simon D. Hammond Sandia   Lunch 12:30 PM     Full-System Simulation at Near Native Speed 1:30 PM Trevor Carlson Uppsala University   Enabling x86 KVM-Based CPU Model in Syscall Emulation Mode 1:45 PM Alexandru Dutu AMD   Parallel gem5 Simulation of Many-Core Systems with Software-Progammable Memories 2:00 PM Bryan Donyanavard UC Irvine   Infrastructure for AVF Modeling 2:15 PM Mark Wilkening AMD   gem5-Aladdin Integration for Heterogeneous SoC Modeling 2:30 PM Y. Sophia Shao Harvard University   Experiences Implementing Tinuso in gem5 2:45 PM Maxwell Walter Technical University of Denmark   Experiences with gem5 3:00 PM Miquel Moretó Planas BSC/UPC   Little Shop of gem5 Horrors (see also Jason\u0026rsquo;s blog post) 3:15pm Jason Power University of Wisconsin   Break 3:30 PM     Breakout Sessions      Breakout Sessions 4:00 PM Breakout Groups    Wrap-Up 5:00 PM Everyone          Conclusions 5:30 PM Ali Saidi ARM    NOTOC\n"
},
{
	"uri": "http://localhost/getting-started/adding-functionality/",
	"title": "Adding Functionality",
	"tags": [],
	"description": "",
	"content": "This section needs to be updated; no mercurial; should reference Contribuing\n If you find the need to modify or extend gem5, you may be tempted to just start editing files in your local gem5 repository. While this approach will work initially, it will cause problems if/when you decide to update your copy of gem5 with changes that have been made since you originally cloned the repository. It\u0026rsquo;s very likely that you will want to update your code to get bug fixes and new features. Thus we strongly advise you to follow one of the following methods. It will save you a great deal of time in the future and allow you to take advantage of new gem5 versions without the error prone process of manually diffing and patching versions.\nThere are two recommend ways to add your own functionality to gem5 and retain the ability to revision control your own code: Mercurial Queues (MQ) and the EXTRAS feature of our build system.\nMercurial Queues is the most powerful option, as it tracks changes you make to the existing gem5 code as well as files you may add to the source tree. It\u0026rsquo;s also the recommended path for developing changes that you contribute back to the public code base (see Submitting Contributions). For more information, see Managing Local Changes with Mercurial Queues.\nThe EXTRAS option is more limited, in that it only allows you to compile additional files in to the gem5 code base, rather than changing or overriding existing files. However, the code compiled with EXTRAS is completely decoupled from the gem5 repository, and thus can be managed separately (e.g., in a different Mercurial repository, or using a completely different revision control system). EXTRAS can also be used to incorporate code that can\u0026rsquo;t be distributed with gem5 due to licensing issues (e.g., the \u0026ldquo;encumbered\u0026rdquo; repository). Often users end up using EXTRAS to incorporate new SimObject models while they concurrently manage a (ideally much smaller) set of changes using MQ. See the EXTRAS page for more details.\n"
},
{
	"uri": "http://localhost/docs/adding-cpu-model/",
	"title": "Howto add a CPU model",
	"tags": [],
	"description": "",
	"content": " should be updated for gem5\n Overview First, make sure you have basic understanding of how the CPU models function within the M5 framework. A good start is the CPU Models page.\nThis brief tutorial will show you how to create a custom CPU model called \u0026lsquo;MyCPU\u0026rsquo;, which will just be a renamed version of the AtomicSimpleCPU. After you learn how to compile and build \u0026lsquo;MyCPU\u0026rsquo;, then you have the liberty to edit the \u0026lsquo;MyCPU\u0026rsquo; code at your heart\u0026rsquo;s content without worrying about breaking any existing M5 CPU Models.\nPort C++ Code for MyCPU The easiest way is to derive a new C++ class of your CPU Model from M5 CPU Models that are already defined and the easiest model to start with is the \u0026lsquo;AtomicSimpleCPU\u0026rsquo; located in the \u0026lsquo;m5/src/cpu/simple\u0026rsquo; directory.\nFor this example, we\u0026rsquo;ll just copy the files from the \u0026lsquo;m5/src/cpu/simple\u0026rsquo; and place them in our own CPU directory: m5/src/cpu/mycpu.\nme@mymachine:~/m5$ cd src/cpu me@mymachine:~/m5/src/cpu$ mkdir mycpu me@mymachine:~/m5/src/cpu$ cp -r simple/* mycpu  Now check the mycpu directory to make sure you\u0026rsquo;ve copied the files successfully:\nme@mymachine:~/m5/src/cpu$ cd mycpu me@mymachine:~/m5/src/cpu/mycpu$ ls AtomicSimpleCPU.py BaseSimpleCPU.py SConscript SConsopts TimingSimpleCPU.py atomic.cc atomic.hh base.cc base.hh timing.cc timing.hh  Let\u0026rsquo;s move the AtomicSimpleCPU.py object file to MyCPU.py and remove the TimingSimpleCPU.py file. We\u0026rsquo;ll edit MyCPU.py a little later:\nme@mymachine:~/m5/src/cpu/mycpu$ mv AtomicSimpleCPU.py MyCPU.py me@mymachine:~/m5/src/cpu/mycpu$ rm TimingSimpleCPU.py me@mymachine:~/m5/src/cpu/mycpu$ ls BaseSimpleCPU.py MyCPU.py SConscript SConsopts atomic.cc atomic.hh base.cc base.hh timing.cc timing.hh  Since we want to change \u0026lsquo;AtomicSimpleCPU\u0026rsquo; to \u0026lsquo;MyCPU\u0026rsquo; we will just replace all the names in the atomic.* files and name them mycpu.* files:\nme@mymachine:~/m5/src/cpu/mycpu$ perl -pe s/AtomicSimpleCPU/MyCPU/g atomic.hh \u0026gt; mycpu.hh me@mymachine:~/m5/src/cpu/mycpu$ perl -pe s/AtomicSimpleCPU/MyCPU/g atomic.cc \u0026gt; mycpu.cc me@mymachine:~/m5/src/cpu/mycpu$ ls BaseSimpleCPU.py MyCPU.py SConscript SConsopts atomic.cc atomic.hh base.cc base.hh mycpu.hh mycpu.cc timing.cc timing.hh  The last thing you need to do is edit your mycpu.cc file to contain a reference to the \u0026lsquo;cpu/mycpu/mycpu.hh\u0026rdquo; header file instead of the \u0026lsquo;cpu/simple/atomic.hh\u0026rsquo; file:\n#include \u0026quot;arch/locked_mem.hh\u0026quot; ... #include \u0026quot;cpu/mycpu/mycpu.hh\u0026quot; ...  NOTE: The AtomicSimpleCPU is really just based off the BaseSimpleCPU (src/cpu/simple/base.hh) so your new CPU Model MyCPU is really a derivation off of this CPU model. Additionally, the BaseSimpleCPU model is derived from the BaseCPU (src/cpu/base.hh). As you can see, M5 is heavily object oriented.\nMaking M5 Recognize MyCPU Now that you\u0026rsquo;ve created a separate directory and files for your MyCPU code (i.e. m5/src/cpu/mycpu), there are a couple files that need to be updated so that M5 can recognize M5 as a build option:\n m5/src/cpu/mycpu/MyCPU.py: Edit your MyCPU python script file (e.g. MyCPU.py) so that your CPU can be recognized as a simulation object. For this example, we will just use the same code tha was previously in AtomicSimpleCPU.py file but replace the name \u0026lsquo;AtomicSimpleCPU\u0026rsquo; with \u0026lsquo;MyCPU\u0026rsquo;.  from m5.params import * from m5 import build_env from BaseSimpleCPU import BaseSimpleCPU class MyCPU(BaseSimpleCPU): type = 'MyCPU' width = Param.Int(1, \u0026quot;CPU width\u0026quot;) simulate_data_stalls = Param.Bool(False, \u0026quot;Simulate dcache stall cycles\u0026quot;) simulate_inst_stalls = Param.Bool(False, \u0026quot;Simulate icache stall cycles\u0026quot;) icache_port = Port(\u0026quot;Instruction Port\u0026quot;) dcache_port = Port(\u0026quot;Data Port\u0026quot;) physmem_port = Port(\u0026quot;Physical Memory Port\u0026quot;) _mem_ports = BaseSimpleCPU._mem_ports + \\ ['icache_port', 'dcache_port', 'physmem_port']   m5/src/cpu/mycpu/SConscript: Edit the SConscript for your CPU model and add the relevant files that need to be built in here. Your file should only contain this code:  Import('*') if 'MyCPU' in env['CPU_MODELS']: Source('mycpu.cc') SimObject('MyCPU.py') TraceFlag('MyCPU') # make sure that SimpleCPU is part of the TraceFlag # Otherwise, DPRINTF may not work properly TraceFlag('SimpleCPU') Source('base.cc') SimObject('BaseSimpleCPU.py')   m5/src/cpu/mycpu/SConsopts: Edit the SConscript for your CPU model and add the relevant files that need to be built in here. Your file should only contain this code:  Import('*') all_cpu_list.append('MyCPU') default_cpus.append('MyCPU')   m5/src/cpu/static_inst.hh: Put a forward class declaration of your model in here  ... class CheckerCPU; class FastCPU; class AtomicSimpleCPU; class TimingSimpleCPU; class InorderCPU; class MyCPU; ...   m5/src/cpu/cpu_models.py: Add in CPU Model-specific information for the ISA Parser. The ISA Parser will use this when referring to the \u0026ldquo;Execution Context\u0026rdquo; for executing instructions. For instance, the AtomicSimpleCPU\u0026rsquo;s instructions get all of their information from the actual CPU (since it\u0026rsquo;s a 1 CPI machine). Thus, instructions only need to know the current state or \u0026ldquo;Execution Context\u0026rdquo; of the \u0026lsquo;AtomicSimpleCPU\u0026rsquo; object. However, the instructions in a O3CPU needed to know the register values (\u0026amp; other state) only known to that current instruction so it\u0026rsquo;s \u0026ldquo;Execution Context\u0026rdquo; is the O3DynInst object. (check out the ISA Description Language documentation page for more details)  ... CpuModel('AtomicSimpleCPU', 'atomic_simple_cpu_exec.cc', '#include \u0026quot;cpu/simple/atomic.hh\u0026quot;', { 'CPU_exec_context': 'AtomicSimpleCPU' }) CpuModel('MyCPU', 'mycpu_exec.cc', '#include \u0026quot;cpu/mycpu/mycpu.hh\u0026quot;', { 'CPU_exec_context': 'MyCPU' }) ...  Building MyCPU Navigate to the M5 top-level directory and build your model:\nme@mymachine:~/m5/src/python/objects$cd ~/m5 me@mymachine:~/m5$scons build/MIPS_SE/m5.debug CPU_MODELS=MyCPU scons: Reading SConscript files ... Checking for C header file Python.h... (cached) yes ... Building in /y/ksewell/research/m5-sim/newmem-clean/build/MIPS_SE Options file /y/ksewell/research/m5-sim/newmem-clean/build/options/MIPS_SE not found, using defaults in build_opts/MIPS_SE Compiling in MIPS_SE with MySQL support. scons: done reading SConscript files. scons: Building targets ... make_hh([\u0026quot;build/MIPS_SE/base/traceflags.hh\u0026quot;], [\u0026quot;build/MIPS_SE/base/traceflags.py\u0026quot;]) Generating switch header build/MIPS_SE/arch/interrupts.hh Generating switch header build/MIPS_SE/arch/isa_traits.hh Defining FULL_SYSTEM as 0 in build/MIPS_SE/config/full_system.hh. Generating switch header build/MIPS_SE/arch/regfile.hh Generating switch header build/MIPS_SE/arch/types.hh Defining NO_FAST_ALLOC as 0 in build/MIPS_SE/config/no_fast_alloc.hh. ... g++ -o build/MIPS_SE/arch/mips/faults.do -c -pipe -fno-strict-aliasing -Wall -Wno-sign-compare -Werror -Wundef -ggdb3 -DTHE_ISA=MIPS_ISA -DDEBUG -DTRACING_ON=1 -Iext/dnet -I/usr/include/python2.4 -Ibuild/libelf/include -I/usr/include/mysql -Ibuild/MIPS_SE build/MIPS_SE/arch/mips/faults.cc g++ -o build/MIPS_SE/arch/mips/isa_traits.do -c -pipe -fno-strict-aliasing -Wall -Wno-sign-compare -Werror -Wundef -ggdb3 -DTHE_ISA=MIPS_ISA -DDEBUG -DTRACING_ON=1 -Iext/dnet -I/usr/include/python2.4 -Ibuild/libelf/include -I/usr/include/mysql -Ibuild/MIPS_SE build/MIPS_SE/arch/mips/isa_traits.cc g++ -o build/MIPS_SE/arch/mips/utility.do -c -pipe -fno-strict-aliasing -Wall -Wno-sign-compare -Werror -Wundef -ggdb3 -DTHE_ISA=MIPS_ISA -DDEBUG -DTRACING_ON=1 -Iext/dnet -I/usr/include/python2.4 -Ibuild/libelf/include -I/usr/include/mysql -Ibuild/MIPS_SE build/MIPS_SE/arch/mips/utility.cc ... cat build/MIPS_SE/m5.debug.bin build/MIPS_SE/m5py.zip \u0026gt; build/MIPS_SE/m5.debug chmod +x build/MIPS_SE/m5.debug scons: done building targets.  If you are compiling on a dual-core CPU, use this command-line to speed-up the compilation:\nscons -j2 build/MIPS_SE/m5.debug CPU_MODELS=MyCPU  Creating Configuration Scripts for MyCPU Now that you have a M5 binary built for use with the MIPS Architecture in a M5, MyCPU Model, you are almost ready to simulate. Note that the standard M5 command line requires taht your provide at least a configuration script for the M5 binary to use.\nThe easiest way to get up and running is to use the sample Syscall-Emulation script: configs/example/se.py.\nYou\u0026rsquo;ll note that line 9 of the se.py Python script imports the details of what type of Simulation will be run (e.g. what CPU Model?) from the Simulation.py file found here: m5/configs/common/Simulation.py. And then, the Simulation.py file imports it\u0026rsquo;s CPU Model options from the Options.py file in the same directory. Edit those two files and your M5 binary will be ready to simulate.\n m5/configs/common/Options.py: Add a option for your model in this file\u0026hellip;  ... parser.add_option(\u0026quot;--my_cpu\u0026quot;, action=\u0026quot;store_true\u0026quot;, help=\u0026quot;Use MyCPU Model\u0026quot;) ...   m5/configs/common/Simulation.py: Edit the Simulation script to recognize your model as a CPU class. After your edits, the setCPUClass function should look like this:  ... def setCPUClass(options): atomic = False if options.timing: class TmpClass(TimingSimpleCPU): pass elif options.my_cpu: class TmpClass(MyCPU): pass atomic = True elif options.detailed: if not options.caches: print \u0026quot;O3 CPU must be used with caches\u0026quot; sys.exit(1) class TmpClass(DerivO3CPU): pass elif options.inorder: if not options.caches: print \u0026quot;InOrder CPU must be used with caches\u0026quot; sys.exit(1) class TmpClass(InOrderCPU): pass else: class TmpClass(AtomicSimpleCPU): pass atomic = True CPUClass = None test_mem_mode = 'atomic' if not atomic: if options.checkpoint_restore != None or options.fast_forward: CPUClass = TmpClass class TmpClass(AtomicSimpleCPU): pass else: test_mem_mode = 'timing' return (TmpClass, test_mem_mode, CPUClass) ...  Testing MyCPU Once you\u0026rsquo;ve edited the configuration scripts, you can run a M5 simulation from the top level directory with this command\nline: me@mymachine:~/m5$build/MIPS_SE/m5.debug configs/example/se.py --my_cpu --cmd=tests/test-progs/hello/bin/mips/linux/hello  NOTE: This binary path refers to the m5/tests directory.\nSummary So the above \u0026ldquo;tutorial\u0026rdquo; showed you how to build your own CPU model in M5. Now, it\u0026rsquo;s up to you to customize the CPU Model as you like and do your experiments! Good luck!\n"
},
{
	"uri": "http://localhost/tutorials/micro-2012/",
	"title": "MISCO 2012 - 1st user workshop",
	"tags": [],
	"description": "",
	"content": "     First gem5 User Workshop\n December 2012; Vancouver, BC\n       The primary objective of this workshop is to bring together groups across the community who are actively using gem5, discuss what is going on in the gem5 community, how we can best leverage each others contributions, and how we continue to make gem5 a successful community-supported simulation framework. Those who will get the most out of the conference are current users of gem5, although anyone is welcome to attend.\nProgram    Topic Time Presenter Affiliation     Introduction 8:30 AM Ali Saidi ARM   Recent Contributions      Memory System Enhancements 8:45 AM Andreas Hannson ARM   Visualizing stats via Streamline 9:05 AM Dam Sunwoo ARM   User Perspectives      HAsim: FPGA-Based Micro-Architecture Simulator 9:20 AM Michael Adler Intel   VLIW DSPs/MIPS FS mode 9:35 AM Deyuan Guo and Hu He Tsinghua Univ.   Eclipse Integration 9:50 AM Deyuan Guo and Hu He Tsinghua Univ.   Break 10:05 AM     Full-System Workloads and Asymmetric Multi-Core Simulation 10:30 AM Anthony Gutierrez Univ. of Michigan   ARM SoC exploration 10:45 AM Alexandre Romana and Abhilash Nair Texas Instruments   SystemC integration 11:00 AM Alexandre Romana Texas Instruments   Composite Cores 11:15 AM Shruti Padmanabha and Andrew Lukefahr Univ. of Michigan   Customized InOrder CPU Modeling 11:30 AM Korey Sewell Univ. of Michigan (now at Qualcomm)   Cross-Cutting Infrastructure for Evaluating Managed Languages and Future Architectures 11:45 AM Paul Gratz Texas A\u0026amp;M Univ.   Lunch 12:00 PM     Simplifying SLICC via Atomic Messages 1:00 PM Brad Beckmann AMD   Accelerating Simulation with Virtual Machines 1:15 PM Ali Saidi ARM   gem5-gpu: A Simulator for Heterogeneous Processors 1:30 PM Jason Power and Marc Orr Univ. of Wisconsin-Madison   Breakout Sessions      Breakout Sessions 1:45 PM Breakout Groups    Break 3:00 PM     Wrap-Up/Next Steps 3:30 PM Everyone          Conclusions 4:00 PM Steve Reinhardt AMD    Location The workshop is co-located with MICRO-45 in Vancouver, BC.\nDate Sunday December 2nd from 8:30 - 16:30.\nNOTOC\n"
},
{
	"uri": "http://localhost/docs/workload-automation/",
	"title": "Workload Automation",
	"tags": [],
	"description": "",
	"content": " It\u0026rsquo;s not clear if this is still up-to-date\n To run workloads in gem5, we advise you to use Workload Automation (WA). This framework allows you to run workloads automatically on Android and Linux platforms. More information can be found here: https://github.com/ARM-software/workload-automation\nWhat do I need? To use WA together with gem5 you will need to make changes to the host system and the guest system. These changes will enable 9P over virtio, which will allow for files to be transported into the simulation.\nHost system requirements On the host system side this means that diod needs to be present. To install diod on Ubuntu you can use the following command:\nsudo apt-get install diod\nGuest system requirements To enable this in the guest system we need to ensure the support is built into the kernel. To do this the following configuration options need to be set:\nCONFIG_NETWORK_FILESYSTEMS=y CONFIG_NET_9P=y CONFIG_NET_9P_VIRTIO=y CONFIG_9P_FS=y CONFIG_9P_FS_POSIX_ACL=y CONFIG_9P_FS_SECURITY=y CONFIG_VIRTIO_BLK=y\nThe guest system also needs to have an m5 binary, which can be found in the gem5 repository under util/m5. The m5 binary is used for extracting files out of the simulation, checkpointing and simulation control (e.g. stats dumps, and exiting gem5). It is important that this file can be found on the path. For checkpointing to work correctly two things need to be ensured when taking the checkpoint:\n The virtio device is included in the system No part of the host file system is mounted by the virtio device  During boot the operating system will initialize all the drivers corresponding with which devices are present in the system. If the virtio device is not found, it cannot be used later, thus the deivce needs to be present during boot (1). Unfortunately, when checkpointing the system it is impossible (well, very hard) to give guarantees about the preservation of the state. You may add or remove files from the host system in between taking and resuming from the checkpoint. For this reason we advise to only mount the host system after restoring from the checkpoint (2).\nUsing gem5 with Workload Automation WA uses agendas to specify the experiments that need to be run. An agenda is a YAML file that describes the configuration of the device, the workloads to be run, and which results to extract. It can be seen as a recipe of how to recreate an experiment. Here is an example of what an agenda looks like for gem5:\nconfig: device: gem5_linux device_config: gem5_args: \u0026quot;configs/example/fs.py\u0026quot; gem5_vio_args: \u0026quot;--workload-automation-vio={}\u0026quot; username: root temp_dir: \u0026quot;/tmp\u0026quot; checkpoint: True run_delay: 10 reboot_policy: never result_processors: [~sqlite] instrumentation: [~cpufreq] workloads: - id: dhrystone workload_name: dhrystone iterations: 1  Required YAML entries for gem5  gem5_args Specify the simulation to be run. This should be the same as you would specify for a stand-alone gem5 run. gem5_vio_args This parameter enables the virtio device in the simulated system. As a minimal requirement you need to ensure that the root parameter of the virtio device is exposed to the command line. Please set this to ‘{}’ in the agenda as it will later be set to the used directory by WA.  Optional YAML entries  gem5_binary Specify which gem5 to execute. This option is useful for executing gem5 in a non-standard location, or in debug mode. temp_dir Temporary directory used for file transferring into gem5. This directory will be created and removed by WA. checkpoint When this is set to ‘True’, WA creates a checkpoint once the system is booted. This checkpoint can then later be used by WA to avoid boot time. run_delay This parameter sets the time that the system should sleep prior to running workloads or taking checkpoints. username This is a Linux-specific parameter, as most Linux systems require a login. If a login prompt is found, this field will be used as username. password As above, but for passwords  Patch for the gem5 simulator The patch below is required to complete the integration with gem5. It adds a VirtIO device to the default full-system simulation script, and exposes it via an argument to the script.\ndiff --git a/configs/common/Options.py b/configs/common/Options.py --- a/configs/common/Options.py +++ b/configs/common/Options.py @@ -351,3 +351,8 @@ parser.add_option(\u0026quot;--command-line-file\u0026quot;, action=\u0026quot;store\u0026quot;, default=None, type=\u0026quot;string\u0026quot;, help=\u0026quot;File with a template for the kernel command line\u0026quot;) + + # Workload Automation options + parser.add_option(\u0026quot;--workload-automation-vio\u0026quot;, action=\u0026quot;store\u0026quot;, type=\u0026quot;string\u0026quot;, + default=None, help=\u0026quot;Enable the Virtio 9P device and set \u0026quot; + \u0026quot;the path to use. Required to use Workload Automation\u0026quot;) diff --git a/configs/example/fs.py b/configs/example/fs.py --- a/configs/example/fs.py +++ b/configs/example/fs.py @@ -225,6 +225,25 @@ not options.fast_forward: CpuConfig.config_etrace(TestCPUClass, test_sys.cpu, options) + if buildEnv['TARGET_ISA'] != \u0026quot;arm\u0026quot; and options.workload_automation_vio: + warn(\u0026quot;Ignoring --workload-automation-vio. It is unsupported on \u0026quot; + \u0026quot;non-ARM systems.\u0026quot;) + else: + from m5.objects import PciVirtIO, VirtIO9PDiod + viopci = PciVirtIO(pci_bus=0, pci_dev=test_sys.realview._num_pci_dev, + pci_func=0, InterruptPin=1, + InterruptLine=test_sys.realview._num_pci_int_line) + + test_sys.realview._num_pci_dev = test_sys.realview._num_pci_dev + 1 + test_sys.realview._num_pci_int_line = test_sys.realview._num_pci_int_line + 1 + + viopci.vio = VirtIO9PDiod() + viopci.vio.root = options.workload_automation_vio + + test_sys.realview.viopci = viopci + test_sys.realview.viopci.dma = test_sys.iobus.slave + test_sys.realview.viopci.pio = test_sys.iobus.master + CacheConfig.config_cache(options, test_sys) MemConfig.config_mem(options, test_sys) diff --git a/src/dev/arm/RealView.py b/src/dev/arm/RealView.py --- a/src/dev/arm/RealView.py +++ b/src/dev/arm/RealView.py @@ -270,6 +270,8 @@ cxx_header = \u0026quot;dev/arm/realview.hh\u0026quot; system = Param.System(Parent.any, \u0026quot;system\u0026quot;) _mem_regions = [(Addr(0), Addr('256MB'))] + _num_pci_dev = 0 + _num_pci_int_line = 0 def _on_chip_devices(self): return [] @@ -615,10 +617,18 @@ # Attach any PCI devices that are supported def attachPciDevices(self): - self.ethernet = IGbE_e1000(pci_bus=0, pci_dev=0, pci_func=0, - InterruptLine=1, InterruptPin=1) - self.ide = IdeController(disks = [], pci_bus=0, pci_dev=1, pci_func=0, - InterruptLine=2, InterruptPin=2) + self.ethernet = IGbE_e1000(pci_bus=0, pci_dev=self._num_pci_dev, + pci_func=0, + InterruptLine=self._num_pci_int_line, + InterruptPin=1) + self._num_pci_dev = self._num_pci_dev + 1 + self._num_pci_int_line = self._num_pci_int_line + 1 + self.ide = IdeController(disks = [], pci_bus=0, + pci_dev=self._num_pci_dev, pci_func=0, + InterruptLine=self._num_pci_int_line, + InterruptPin=2) + self._num_pci_dev = self._num_pci_dev + 1 + self._num_pci_int_line = self._num_pci_int_line + 1 def enableMSIX(self): self.gic = Pl390(dist_addr=0x2C001000, cpu_addr=0x2C002000, it_lines=512)  "
},
{
	"uri": "http://localhost/tutorials/",
	"title": "Previous Tutorials",
	"tags": [],
	"description": "",
	"content": " We have held a handful of tutorials on M5 at various conferences. Though the material in these tutorials can be out of date, the tutorial materials present a more organized (and in some cases more in-depth) overview than the wiki documentation. We highly recommend taking a look at the most recent tutorial as a complement to the documentation on the wiki.\nThe slides and handouts are the same material except that the handouts are formatted with two slides per page.\nICS 2018: Vector Architecture Exploration with gem5 Vector Architecture Exploration with gem5\nInternational Conference on Supercomputing, Beijing (China), June 2018\nThis tutorial covers the Arm Scalable Vector Extension (SVE) and how to use gem5 to explore system architecture designs of microarchitectures implementing SVE.\nArm Research Starter Kit on System Modeling using gem5 https://github.com/arm-university/arm-gem5-rsk\nGetting started instructions and an overview of the HPI model.\nISCA 45: AMD gem5 APU Model AMD gem5 APU Simulator: Modeling GPUs Using the Machine ISA\nThis tutorial covers the gem5 APU model in detail. In particular, we discuss the model\u0026rsquo;s support for executing GPU machine ISA instructions and the full user space ROCm stack.\nArm Research Summit 2017: gem5 workshop ARM Research Summit 2017 Workshop covers many advanced topics in gem5 such as Ruby, Garnet, and SystemC.\ndist-gem5 at ISCA-44 (Toronto, 2017) dist-gem5 is a gem5-based simulation infrastructure which enables full-system simulation of a parallel/distributed computer system using multiple simulation hosts.\n Tutorial web site  ASPLOS 22 Full day tutorial on gem5 at ASPLOS 2017\nHiPEAC Computer Systems Week This tutorial was held in Gothenburg, Sweden in April 2012. It covers gem5 although for information about Ruby you should look at the ISCA 38 tutorial. We recorded video of the tutorial which is available\nbelow.   Slides Overview Introduction Basics Running Experiments Debugging Memory CPU Models Common Tasks Configuration Conclusion  ISCA 38 This tutorial, held in June 2011 at ISCA-38, it covered gem5 (the merger between M5 and GEMS). It was extremely well attended with 65 people participating.\n Slides Podcasts/video coming soon provided there are no technical difficulties  ASPLOS-13 This tutorial, held in March 2008 at ASPLOS XIII in Seattle, covered M5 2.0 and included several small examples on creating SimObjects and adding\n parameters.   Slides Handouts Video  Introduction \u0026ndash; A brief overview of M5, its capabilities and concepts Running \u0026ndash; How to compile and run M5 Full System \u0026ndash; Full system benchmarks, disk images, and scripts Objects \u0026ndash; An overview of the various object models that are available out of the box Extending \u0026ndash; M5 internals, defining new objects \u0026amp; parameters, statistics, ISA descriptions, ARM \u0026amp; X86 support, future development Debugging \u0026ndash; Facilities in M5 to aid debugging  Description  ISCA-33 This tutorial, held in June 2006 at ISCA 33 in Boston, was the first one to cover M5 2.0.\n Slides Handouts Description  ISCA-32 Our first tutorial, held in June 2005 at ISCA 32 in Madison, is rather dated as it covered M5 1.X and not 2.0.\n Slides Handouts  "
},
{
	"uri": "http://localhost/tutorials/isca-2011/",
	"title": "ISCA 2011",
	"tags": [],
	"description": "",
	"content": "Call for Participation: ISCA 2011 Tutorial gem5: A Multiple-ISA Full System Simulator with Detailed Memory Modeling Sunday, June 5, 2011 8:30 am http://www.gem5.org The gem5 simulator is a merger of two of the computer architecture community’s most popular, open source simulators: M5 and GEMS. The best features of each simulator have been combined to provide an infrastructure capable of simulating multiple ISAs, CPU models, memory system components, cache coherence protocols and interconnection networks. The gem5 simulation team invites users, developers, and all other interested parties to participate in a tutorial that will highlight the key aspects of the gem5 simulator .\nThe first half of this full-day tutorial will be an organized presentation focusing on gem5 usage and capabilities. The second half is intended to be more free form where we will answer audience questions on specific usage, including modification of the simulator to enable new features.\nTopics to be discussed include:\n Multiple ISA support (e.g. ARM and x86) Detailed and simple CPU models including “execute-in-execute” in-order and out-of-order pipeline models. Cache coherence protocols using SLICC Interconnection network modeling (Crossbar, Mesh, etc.) Checkpointing and fast-forwarding  We look forward to your participation in the gem5 tutorial and hope that by the end of the tutorial you’ll be able to utilize the gem5 infrastructure in your future research.\nThanks, The gem5 Simulation Team\n"
},
{
	"uri": "http://localhost/tutorials/isca-2006/",
	"title": "ISCA 2006",
	"tags": [],
	"description": "",
	"content": " Using the M5 Simulator ISCA 2006 Tutorial Sunday June 18th, 2006\nIntroduction This half-day tutorial will introduce participants to the M5 simulator system. M5 is a modular platform for computer system architecture research, encompassing system-level architecture as well as processor microarchitecture.\nWe will be releasing version 2.0 of M5 in conjunction with this tutorial. Features new in 2.0 include:\n Multiple ISA support (Alpha, MIPS, and SPARC) An all-new, execute-in-execute out-of-order SMT CPU timing model, with no SimpleScalar license encumbrance All-new, message-oriented interface for memory system objects, designed to simplify the development of non-bus interconnects More extensive Python integration and scripting support  Because the primary focus of the M5 development team has been simulation of network-oriented server workloads, M5 incorporates several features not commonly found in other simulators.\n Full-system simulation using unmodified Linux 2.4\u0026frasl;2.6, HP Tru64 5.1, or L4Ka::Pistachio) (Alpha only at this time\u0026hellip; coming in the future for MIPS and SPARC) Detailed timing of I/O device accesses and DMA operations Accurate, deterministic simulation of multiple networked systems Flexible, script-driven configuration to simplify specification of complex multi-system configurations Included network workloads such as Apache, NAT, and NFS Support for storing results from multiple simulations in a unified database (e.g. MySQL) for automated reporting and graph generation  M5 also integrates a number of other desirable features, including pervasive object orientation, multiple interchangeable CPU models, an event-driven memory system model, and multiprocessor capability. Additionally, M5 is also capable of application-only simulation using syscall emulation.\nM5 is freely distributable under a BSD-style license, and does not depend on any commercial or restricted-license software.\nIntended Audience Researchers in academia or industry looking for a free, open-source, full-system simulation environment for processor, system, or platform architecture studies. Please register via the ISCA 2006 web page.\nTentative Outline  M5 structure  Object structure  Intro to SimObjects Object builder Configuration language Specialization using C++ templates Object serialization (checkpointing)  Events  CPU models  Simple functional model Detailed out-of-order model Sampling and warm-up support  Memory \u0026amp; I/O system overview  Cache models Interconnect models (busses, point-to-point networks) Coherence support I/O modeling  Programmed I/O (uncached accesses) DMA I/O  Ethernet model  NIC device models Linux driver Link layer model   Full-system modeling  Building disk images Console and PAL code Running benchmarks via system init scripts Target kernel introspection support  Statistics  Built-in statistics types Adding new statistics Using the database back end  Setting up a results database Using scripts to generate reports and graphs from the database   Debugging techniques  Built-in debugging support  Tracing Runtime checking Gdb hooks  Debugging target code (including kernels) using remote gdb  ISA description language  Adding your own instructions to the ISA Adding support for new ISAs   Speakers  Steven K. Reinhardt is an associate professor in the EECS Department at the University of Michigan, and a principal developer of M5. He received a BS from Case Western Reserve University and an MS from Stanford University, both in electrical engineering, and a PhD in computer science from the University of Wisconsin-Madison. While at Wisconsin, he was the principal developer of the Wisconsin Wind Tunnel parallel architecture simulator.   Nathan L. Binkert received his Ph.D. candidate from the EECS Department at the University of Michigan, and a principal developer of M5. He received a BSE in electrical engineering and MS in computer science both from the University of Michigan. As an intern at Compaq VSSAD, he was a principal developer of the ASIM simulator, currently in use at Intel and is currently with Arbor Networks.   Ronald G. Dreslinski is a Ph.D. student in the EECS Department at the University of Michigan, and a developer of M5\u0026rsquo;s memory system. He received a BSE in electrical engineering, a BSE in computer engineering, and a MSE in computer science and engineering all from the University of Michigan.   Kevin T. Lim is a Ph.D. student in the EECS Department at the University of Michigan, and the developer of M5\u0026rsquo;s detailed CPU model. He received a BSE in computer engineering and an MSE in computer science and engineering from the University of Michigan.   Ali G. Saidi is a Ph.D. candidate in the EECS Department at the University of Michigan, and wrote much of the platform code for Linux full-system simulation. He received a BS in electrical engineering from the University of Texas at Austin and an MSE in computer science and engineering from the University of Michigan.  NOTOC\n"
},
{
	"uri": "http://localhost/developer/",
	"title": "Developer Documentation",
	"tags": [],
	"description": "",
	"content": "This section contains documentation of the internals of gem5 that is useful for people developing the simulator.\n"
},
{
	"uri": "http://localhost/tutorials/arm-summit-2017/",
	"title": "ARM research summit 2017",
	"tags": [],
	"description": "",
	"content": " The ARM Research Summit is an academic summit to discuss future trends and disruptive technologies across all sectors of computing. On the first day of the Summit, ARM Research will host a gem5 workshop to give a brief overview of gem5 for computer engineers who are new to gem5 and dive deeper into some of gem5\u0026rsquo;s more advanced capabilities. The attendees will learn what gem5 can and cannot do, how to use and extend gem5, as well as how to contribute back to gem5.\nThe ARM Research Summit will take place in Cambridge (UK) over the days of 11-13 September 2017. The gem5 workshop will be a full day event on the 11th September.\nStreaming \u0026amp; Offline viewing The workshop is being streamed live and all talks will be available on YouTube after the workshop. See the main summit page for details.\nTarget Audience The primary audience is researchers who are using, or planning to use, gem5 for architecture research.\nPrerequisites: Attendees are expected to have a working knowledge of C++, Python, and computer systems.\nRegistration See the main ARM Research Summit website for details about registration.\nSchedule The workshop will take place on Monday the 11th September 2017 at Robinson College in Cambridge (UK). The workshop starts at 9.00 and runs in parallel with the main Summit program until 16.30 when it joins the main program.\n   Time Topic     09.00-09.30 Welcome and introduction to gem5 — slides   09.30-09.45 Interacting with gem5 using workload-automation \u0026amp; devlib — slides   09.45-10.00 ARM Research Starter Kit: System Modeling using gem5 — slides   10.00-10.15 Break   10.15-10.30 Debugging a target-agnostic JIT compiler with GEM5   10.30-11.00 Learning gem5: Modeling Cache Coherence with gem5 — slides   11.00-11.15 Break (overlaps with main program break)   11.15-11.45 A Detailed On-Chip Network Model inside a Full-System Simulator — slides   11.45-12.00 Integrating and quantifying the impact of low power modes in the DRAM controller in gem5 — slides   12.00-12.15 Break   12.15-12.45 CPU power estimation using PMCs and its application in gem5 — slides   12.45-13.00 gem5: empowering the masses — slides   13.00-14.15 Lunch   14.15-14.45 Trace-driven simulation of multithreaded applications in gem5 — slides   14.45-15.00 Generating Synthetic Traffic for Heterogeneous Architectures — slides   15:00-15:15 Break   15:15-16:45 System Simulation with gem5, SystemC and other Tools — slides   15:45-16:00 COSSIM: An Integrated Solution to Address the Simulator Gap for Parallel Heterogeneous Systems — slides   16:00-16:15 Simulation of Complex Systems Incorporating Hardware Accelerators — slides   16:15-16:30 Break   16:30-18:15 Introduction to ARM Research   18:20-20.00 Poster Session \u0026amp; Pre-Dinner Drinks   20.00-21.30 Buffet Dinner    Talks Trace-driven simulation of multithreaded applications in gem5 The gem5 modular simulator provides a rich set of CPU models which permits balancing simulation speed and accuracy. The growing interest in using gem5 for design-space exploration however requires higher simulation speeds so as to enable scalability analysis with systems comprising tens to hundreds of cores. One relevant approach for enabling significant speedups lies in using trace-driven simulation, in which CPU cores are abstracted away thereby enabling to refocus simulation effort on memory/interconnect subsystems which play a key role on performance. This talk describes some of the work carried out on the Mont-Blanc european projects on trace-driven simulation and discusses the related challenges for multicore architectures in which trace injection requires to account for the API synchronization of the underlying running application. The ElasticSimMATE tool is presented as an initiative towards combining Elastic Traces and SimMATE so as to enable fast and accurate simulation of multithreaded applications on ARM multicore systems.\n Dr Gilles Sassatelli is a CNRS senior scientist at LIRMM, a CNRS-University of Montpellier academic research unit with a staff of over 400. He is vice-head of the microelectronics department and leads a group of 20 researchers working in the area of smart embedded digital systems. He has authored over 200 peer-reviewed papers and has occupied key roles in a number of international conferences. Most of his research is conducted in the frame of international EU-funded projects such as the DreamCloud and Mont-Blanc projects.\nAlejandro Nocua received the Ph.D. degree in Microelectronics from the University of Montpellier, France, in 2016. Currently, he is a postdoctoral researcher at the French National Center for Scientific Research (CNRS). His research interests include the analysis of high-performance and energy-efficiency design methodologies. He received his Master degree in Science from the National Institute of Astrophysics, Optics and Electronics (INAOE), Mexico, in 2013. Alejandro was awarded his BS degree in Electronics Engineering from Industrial University of Santander (UIS), Colombia in 2011.\nFlorent Bruguier received the M.S. and Ph.D. degrees in microelectronics from the University of Montpellier, France, in 2009 and 2012, respectively. From 2012 to 2015, he was a Scientific Assistant with the Montpellier Laboratory of Informatics, Robotics, and Microelectronics, University of Montpellier. Since 2015, he is a Permanent Associate Professor. He has co-authored over 30 publications. His research interests are focused on self-adaptive and secure approaches for embedded systems.\nAnastasiia Butko, Ph.D. is a Postdoctoral Fellow in the Computational Research Division at Lawrence Berkeley National Laboratory (LBNL), CA. Her research interests lie in the general area of computer architecture, with particular emphasis on high-performance computing, emerging and heterogeneous technologies, associated parallel programming and architectural simulation techniques. Broadly, her reasearch addresses the question of how alternative technologies can provide continuing performance scaling in the approaching Post-Moore’s Law era. Her primary research projects include development of the EDA tools for fast superconducting logic design, development of the classical ISA for quantum processor control, development of the fast and flexible System-on-Chip generators using Chisel DSL. Dr. Butko co-leads Open Source Supercomputing project and is a technical committee member of the RISC-V foundation.\nDr. Butko received her Ph.D. in Microelectronics from the University of Montpellier, France (2015). Her doctoral thesis developed fast and accurate simulation techniques for many-core architectures exploration. Her graduate work has been conducted within the European project MontBlanc, which aims to design a new supercomputer architecture using low-power embedded technologies.\nDr. Butko received her MSc. Degree in Microelectronics from UM2, France and MSc and BSc Degrees in Digital Electronics from NTUU \u0026ldquo;KPI\u0026rdquo;, Ukraine. During her Master she participated on the international program of double diploma between Montpellier and Kiev universities.\n \nModeling Cache Coherence with gem5 Correctly implementing cache coherence protocols is hard and these implementation details can affect the system\u0026rsquo;s performance. Therefore, it is important to robustly model the detailed cache coherence implementation. The popular computer architecture simulator gem5 uses Ruby as its cache coherence model providing higher fidelity cache coherence modeling than many other simulators.\nIn this talk, I will give a brief overview of Ruby, including SLICC: the domain-specific language Ruby uses to specify cache protocols. I will show the extreme flexibility of this model and details of a simple cache coherence protocol. After this talk, you will be able to dive in and begin writing your own coherence protocols!\n Jason Lowe-Power is an Assistant Professor at University of California, Davis in the Computer Science department. Jason\u0026rsquo;s research focuses on increasing the energy efficiency and performance of end-to-end applications like analytic database operations used by Amazon, Google, Target, etc. One important aspect of this research is adding hardware mechanisms to systems that enable all programmers to use emerging hardware accelerators like GPUs. Additionally, Jason is a leader of the open-source architectural simulator, gem5, used by over 1500 academic papers. Jason received his PhD from University of Wisconsin-Madison in Summer 2017. He was awarded the Wisconsin Distinguished Graduate Fellowship Cisco Computer Sciences Award in 2014 and 2015.\n \nA Detailed On-Chip Network Model inside a Full-System Simulator Compute systems are ubiquitous, with form factors ranging from smartphones at the edge to datacenters in the cloud. Chips in all these systems today comprise 10s to 100s of homogeneous/heterogeneous cores or processing elements. The growing emphasis on parallelism, distributed computing, heterogeneity, and energy-efficiency across all these systems makes the design of the Network-on-Chip (NoC) fabric connecting the cores critical to both high-performance and low power consumption.\nIt is imperative to model the details of the NoC when architecting and exploring the design-space of a complex many-core system. If ignored, an inaccurate NoC model could lead to over-design or under-design due to incorrect trade-off choices, causing performance losses at runtime. To this end, we have designed and integrated a detailed on-chip network model called Garnet inside the gem5 (www.gem5.org) full-system architectural simulator which is being used extensively by both industry and academia. Together with Garnet, gem5 provides plug-and-play models of cores, caches, cache coherence protocols, NoC, memory controller, and DRAM, with varying levels of details, enabling computer architects and designers to trade-off simulation speed and accuracy.\nIn this talk, we will first introduce the basic building blocks of NoCs and present the state-of-the-art used in chips today. We will then present Garnet, and demonstrate how it faithfully models the state-of-the-art, while also offering immense flexibility in modifying various parts of the microarchitecture to serve the needs of both homogeneous many-cores and heterogeneous accelerator-based systems of the future via case studies and code-snippets. Finally, we will demonstrate how Garnet works within the entire gem5 ecosystem.\n Tushar Krishna is an Assistant Professor in the Schools of ECE and CS at Georgia Tech. He received a Ph.D. in Electrical Engineering and Computer Science from the Massachusetts Institute of Technology in\n Prior to that he received a M.S.E from Princeton University in 2009, and a B.Tech from the Indian Institute of Technology (IIT) Delhi in 2007, both in Electrical Engineering.  Before joining Georgia Tech in 2015, Dr. Krishna was a post-doctoral researcher in the VSSAD Group at Intel, Massachusetts, and then at the Singapore-MIT Alliance for Research and Technology at MIT.\nDr. Krishna\u0026rsquo;s research interests are in computer architecture, interconnection networks, networks-on-chip, deep learning accelerators, and FPGAs.\n \nSystem Simulation with gem5, SystemC and other Tools SystemC TLM based virtual prototypes have become the main tool in industry and research for concurrent hardware and software development, as well as hardware design space exploration. However, there exists a lack of accurate, free, changeable and realistic SystemC models of modern CPUs. Therefore, many researchers use the cycle accurate open source system simulator gem5, which has been developed in parallel to the SystemC standard. In this tutorial we present the coupling of gem5 with SystemC that offers full interoperability between both simulation frameworks, and therefore enables a huge set of possibilities for system level design space exploration. Furthermore, we show several examples for coupling gem5 with SystemC and other tools.\n Matthias Jung received his PhD degree in Electrical Engineering from the University of Kaiserslautern Germany in 2017. His research interest are SystemC based virtual prototypes, especially with the focus on the modeling of memory systems and memory controller design. Since may 2017 he is a researcher at Fraunhofer IESE, Kaiserslautern, Germany.\nChristian Menard received a Diploma degree in Information Systems Technology from TU Dresden in Germany in 2016 and joined the chair for compiler construction as a Ph.D. student within the excellence cluster cfaed in TU Dresden. His current research includes system-level modeling of widely heterogeneous hardware as well dataflow compilers for heterogeneous MPSoC platforms.\n \nCPU power estimation using PMCs and its application in gem5 Fast and accurate estimation of CPU power consumption is necessary to inform run-time power management approaches and allow effective design space exploration. Power simulators, combined with a full-system architectural simulator such as gem5, enable power-performance trade-offs to be investigated early in the design of a system. However, the accuracy of existing power simulators is known to be low, and this can lead to incorrect conclusions being made. In this talk, I will present our statistically rigorous methodology for building accurate run-time power models using Performance Monitoring Counters (PMCs) for mobile and embedded devices, and demonstrate how our models make more efficient use of limited training data and better adapt to unseen scenarios by uniquely considering stability. Models built using the methodology for both ARM Cortex-A7 and Cortex-A15 CPUs exhibit a 3.8% and 2.8% average error respectively. I will also present online resources that we have made available from the work, including software tools, documentation, raw data and further results. I will also present results from an investigation into the correlation between gem5 activity statistics and hardware PMCs. Based on this, a gem5 power model for a simulated quadcore ARM Cortex-A15 has been created, built using the above methodology, and its accuracy compared against experimental results obtained from hardware.\n Geoff Merrett is an Associate Professor in the Department of Electronics and Computer Science at the University of Southampton. He received the BEng (1st, Hons) and PhD degrees in Electronic Engineering from Southampton in 2004 and 2009 respectively. His research interests are in energy-aware and self-powered computing systems, with application across the spectrum from highly constrained IoT devices to many-core mobile and embedded systems. He has published over 100 peer-reviewed articles in these areas, and given invited talks at a number of international events. Dr Merrett is a Co-Investigator on the EPSRC-funded £5.6M PRiME Programme Grant (where he leads the applications and cross-layer interaction theme), \u0026ldquo;Continuous on-line adaptation in many-core systems: From graceful degradation to graceful amelioration\u0026rdquo;, and deputy-lead on the \u0026ldquo;Wearable and Autonomous Computing for Future Smart Cities\u0026rdquo; Platform Grant. He is technical manager of Southampton’s ARM-ECS Research Centre, an award-winning industry-academia collaboration between the University of Southampton and ARM. He coordinates IoT research at the University, and leads the wireless sensing theme of its Pervasive Systems Centre. He is an Associate Editor for the IET CDS journal, serves as a reviewer for a number of leading journals, and on TPCs for a range of conferences. He co-manages the UK’s Energy Harvesting Network, was General Chair of the ACM Workshop on Energy-Harvesting and Energy-Neutral Sensing Systems in 2013, 2014, and 2015, and was the General Chair of the European Workshop on Microelectronics Education 2016. He is a member of the IEEE, IET and Fellow of the HEA.\n \nShort Talks Debugging a target-agnostic JIT compiler with GEM5 Author: Boris Shingarov - LabWare\nWe explain how GEM5 enabled us to develop a target-agnostic JIT compiler, in which no knowledge about the target ISA is coded by the human programmer; instead, the backend is inferred, using logic programming, from a formal machine description written in a Processor Description Language. Debugging such a JIT presents some challenges which can not be addressed using traditional approaches. One such challenge is the impedance mismatch between the high-level abstractions in the PDL and the low-level inferred implementation. In this talk, we present a new debugger based on simulating the execution of the target runtime VM in GEM5; the debugger frontend connects to this simulation using the RSP wire protocol. \nCOSSIM: An Integrated Solution to Address the Simulator Gap for Parallel Heterogeneous Systems In an era of complex networked heterogeneous systems, simulating independently only parts, components or attributes of a system-under-design is not a viable, accurate or efficient option. The interactions are too many and too complicated to produce meaningful results and the optimization opportunities are severely limited when considering each part of a system in an isolated manner. COSSIM offers a framework that can handle the simulation of a complete system-of-systems including processors, peripherals and networks that can appeal to Parallel (Heterogeneous) Systems designers and application developers in an integrated way.\nThe framework is based on gem5 as the main simulation engine for processor-based systems and extends its capabilities by integrating it with the OMNET++ network simulator. This integration allows independent gem5 instances to be networked with all network protocols and hierarchies that can be supported by OMNET++, thus creating a very flexible solution. The integration of the two main simulation tools is realized through the IEEE 1516 High-Level Architecture standard (HLA), through which all communication tasks are performed. Through HLA and custom libraries, a two-level (per node and global) synchronization scheme is also implemented to ensure a coherent notion of time between all nodes.\nSince HLA is IP-based all gem5 instances and OMNET++ can be executed on the same physical machine or on any distributed system (or any combination in between). The overall framework – the set of gem5 nodes, the OMNET++ simulator and the CERTI HLA – are integrated in a unified Eclipse-based GUI that has been developed to provide easy simulation set-up, execution and visualization of results. McPAT is also integrated in a semi-automated way through the GUI in order to provide power and energy estimations for each node, while OMNET++ provides power estimations for networking-related components (NICs and network devices).\n Andreas Brokalakis is a senior hardware engineer at Synelixis Solutions Ltd. At the same time he is pursuing a PhD degree at the Technical University of Crete, Greece. He holds a Bachelor degree in Computer Engineering from University of Patras, Greece and a Master’s Degree on Hardware/Software Co-design from the same university. Current work and research interests involve computer architecture and arithmetic, as well as design of ASIC and FPGA systems and accelerators.\nNikolaos Tampouratzis is a PhD student at Technical University of Crete, working on simulation tools for computing systems. He has joined Telecommunication Systems Institute, Technical University of Crete since October 2012 as a research associate, providing research and development services to several EU-funded research projects. He received his Computer Science diploma from the University of Crete (UOC, Greece), with specialization in Hardware Design and FPGAs. He continued his studies in the Technical University of Crete (TUC Greece) where he received his Master Diploma in Electronic and Computer Engineering in which he specialized in Computer Architecture and Hardware Design.\n \nSimulation of Complex Systems Incorporating Hardware Accelerators The breakdown of Dennard scaling coupled with the persistently growing transistor counts increased the importance of application-specific hardware acceleration; such an approach offers significant performance and energy benefits compared to general-purpose solutions. In order to thoroughly evaluate such architectures, the designer should perform a quite extensive design space exploration so as to evaluate the trade-offs across the entire system. The design, until recently, has been predominantly done using Register Transfer Level languages such as Verilog and VHDL, which, however, lead to a prohibitively long and costly design effort. In order to reduce the design time a wide range of both commercial and academic High-Level Synthesis (HLS) tools have emerged; most of these tools, handle hardware accelerators that are described in synthesizable SystemC. The problem today, however, is that most simulators used for evaluating the complete user applications (i.e. full-system CPU/Mem/Peripheral simulators) lack any type of SystemC accelerator support.\nWithin this context, we extend gem5 to support the simulation of generic SystemC accelerators. We introduce a novel flow that enables us to rapidly prototype synthesisable SystemC hardware accelerators in conjunction with gem5. The proposed solution handles automatically all communication and synchronisation issues.\nCompared to a standard gem5 system, several changes at different levels are required, from the OS and device drivers level down to the implementation of a device model in the gem5 simulator. Instead of using files to write data for an external accelerator, perform the simulation and then read back the results, our approach communicates with the SystemC simulator through programmed I/Os and DMA engines, supporting full global synchronisation. Apart from the apparent benefits concerning the implementation and simulation accuracy, the proposed solution is also orders of magnitude faster.\n Nikolaos Tampouratzis is a PhD student at Technical University of Crete, working on simulation tools for computing systems. He has joined Telecommunication Systems Institute, Technical University of Crete since October 2012 as a research associate, providing research and development services to several EU-funded research projects. He received his Computer Science diploma from the University of Crete (UOC, Greece), with specialization in Hardware Design and FPGAs. He continued his studies in the Technical University of Crete (TUC Greece) where he received his Master Diploma in Electronic and Computer Engineering in which he specialized in Computer Architecture and Hardware Design.\n \nGenerating Synthetic Traffic for Heterogeneous Architectures Modern system-on-chip architectures consist of many heterogeneous processing elements. The communication fabric and memory hierarchy supporting these processing elements heavily influence the system’s overall performance. Exploring the design space of these heterogeneous architectures with detailed models of each processing element can be time-consuming. Statistical simulation has been shown to be an effective tool for quickly evaluating architectures by abstracting away complexity.\nThis talk describes work done on modelling the spatial and temporal behaviour of a processing element’s address stream. We present a methodology that can automatically characterize a processing element by observing its reads and writes. Using these characteristics we can stimulate a communication fabric connecting many different processing elements by synthetically recreating their addresses. These addresses arrive at their destination in the memory hierarchy, spawning new messages and responses to read and write requests. Architects can now combine ynthetic processing elements that represent various different components on current and future systems-on-chip to evaluate the impact of changes at the interconnection network and memory hierarchy.\n Mario Badr is a PhD Candidate at the University of Toronto working under the supervision of Dr. Natalie Enright Jerger. He received his B.A.Sc. and M.A.Sc from the University of Toronto in Electrical Engineering and Computer Engineering, respectively. He has interned with Qualcomm Research Silicon Valley and received the Roberto Padovani Scholarship for his outstanding technical contributions. In addition, he has been recognized at the university and departmental levels for excellence as a teaching assistant. His research interests include performance evaluation in computer architecture, heterogeneous architectures, and multi-threaded workloads.\n \nARM Research Starter Kit: System Modeling using gem5 ARM Research Enablement aims to enhance computing research by enabling researchers worldwide to easily access ARM-based IP and technologies, and helping them to increase their research impact. As a part of our research enablement activities, we provide a System Modeling Research Starter Kit using gem5. We have released a High Performance In-order (HPI) CPU timing model based on ARMv8-A in gem5. I will present a high-level overview of the released system, its documentation and benchmark scripts. This talk will target those who are new to gem5 as well as those who would like to promote gem5 in research.\n Ashkan Tousi is a Senior Research Engineer at ARM Cambridge and an Honorary Lecturer at the University of Glasgow. He received his PhD in computing science (parallel computing) in 2015. He currently leads research enablement activities at ARM, which cover a range of different research areas from SoC design to IoT and data science.\n \nInteracting with gem5 using workload-automation \u0026amp; devlib Running workloads on gem5 is often not straightforward. This talk will discuss workload-automation and devlib, 2 new open-source tools to interact with gem5. These frameworks, written to interact with various hardware platforms, have recently been extended to include gem5 as a platform. We will discuss use cases and advantages/disadvantages of each tool and show how they can make your gem5 work easier.\n Anouk Van Laer is a Modelling Engineer in Architecture: Systems \u0026amp; Technology group at ARM. She obtained her PhD at University College London, where she investigated the effects of optical interconnects on the performance of chip multiprocessors, using gem5.\n \ngem5: empowering the masses This talk will give an overview of the state of power modelling in gem5. After discussing the basic power modelling infrastructure, it will cover the state of CPU DVFS as well as recent improvements in how CPU power states are controlled for the ARM architecture in gem5. The talk will cover these improvements in power modelling, highlighting the way in which the accuracy and versatility of the simulator have been improved.\n Sascha Bischoff is a Senior Software Engineer in the Architecture: Systems \u0026amp; Technology group at ARM in Cambridge. Whilst completing his PhD with the University of Southampton, he spent 3.5 years based in ARM Research in Cambridge. He has spent a large part of the last 6 years working with gem5, typically with a focus on power management, ideally without impacting the delivered performance.\n \nIntegrating and quantifying the impact of low power modes in the DRAM controller in gem5 Across applications, DRAM is a significant contributor to the overall system power, with the DRAM access energy per bit up to three orders of magnitude higher compared to on-chip memory accesses. To improve the power efficiency, DRAM technology incorporates multiple low power modes, each with different trade-offs between achievable power savings and performance impact due to entry and exit delay requirements. Accurate modeling of these low power modes and entry and exit control is crucial to analyze the trade-offs across controller configurations and workloads with varied memory access characteristics.\nIn this talk, we will give an overview of the decision making logic we added to the DRAM controller in gem5 that triggers transitions to/from the power-down modes. Integrating this functionality makes gem5 the first publicly available DRAM low power full-system simulator, providing the research community a tool for DRAM power analysis for a breadth of use cases. We will conclude with simulation data that characterises the low power behaviour and shows energy and performance trade-offs for realistic workloads.\nNote: This talk is based on a paper accepted at MEMSYS 17. Authors from ARM: Radhika Jagtap, Wendy Elsasser and Andreas Hansson. Authors from University of Kaiserslautern: Matthias Jung and Norbert Wehn.\n Radhika Jagtap is a Senior Research Engineer working in the Memory \u0026amp; Systems research group. She has plenty of experience with gem5 (elastic traces, interconnect, memory controller) and is involved in several collaborative research projects, especially with academics. Currently she is exploring the problem of energy efficient data movement for sparse data workloads.\n \n"
},
{
	"uri": "http://localhost/tutorials/asplos-2008/",
	"title": "ASPLOS 2008",
	"tags": [],
	"description": "",
	"content": " Using the M5 Simulator ASPLOS 2008 Tutorial Sunday March 2nd, 2008\nIntroduction This half-day tutorial will introduce participants to the M5 simulator system. M5 is a modular platform for computer system architecture research, encompassing system-level architecture as well as processor microarchitecture.\nWe will be discussing version 2.0 of the M5 simulator and specifically its new features including:\n Multiple ISA support (Alpha, ARM, MIPS, and SPARC) An execute-in-execute out-of-order SMT CPU timing model, with no SimpleScalar license encumbrance Message-oriented interface for memory system objects, designed to simplify the development of non-bus interconnects New caches models that are easier to modify New multi-level bus-based coherence protocol More extensive Python integration and scripting support Performance improvements Generating checkpoints for simpoints  Because the primary focus of the M5 development team has been simulation of network-oriented server workloads, M5 incorporates several features not commonly found in other simulators.\n Full-system simulation using unmodified Linux 2.4\u0026frasl;2.6, FreeBSD, or Solaris (More are on the way) Detailed timing of I/O device accesses and DMA operations Accurate, deterministic simulation of multiple networked systems Flexible, script-driven configuration to simplify specification of complex multi-system configurations Included network workloads such as Apache, NAT, and NFS Support for storing results from multiple simulations in a unified database (e.g. MySQL) for automated reporting and graph generation  M5 also integrates a number of other desirable features, including pervasive object orientation, multiple interchangeable CPU models, an event-driven memory system model, and multiprocessor capability. Additionally, M5 is also capable of application-only simulation using syscall emulation.\nM5 is freely distributable under a BSD-style license, and does not depend on any commercial or restricted-license software.\nIntended Audience Researchers in academia or industry looking for a free, open-source, full-system simulation environment for processor, system, or platform architecture studies. Please register via the ASPLOS 2008 web page.\nTentative Topics The following topics will be discussed in detail during the tutorial:\n M5 structure Object structures Specifying configurations Object serialization (checkpoints) Events CPU models Memory/Cache models I/O devices Full-system modeling Statistics Debugging techniques ISA description language Future directions  Speakers  Ali G. Saidi is a Ph.D. candidate in the EECS Department at the University of Michigan, and wrote much of the platform code for Linux full-system simulation. He received a BS in electrical engineering from the University of Texas at Austin and an MSE in computer science and engineering from the University of Michigan.   Steven K. Reinhardt is an associate professor in the EECS Department at the University of Michigan, and a principal developer of M5. He received a BS from Case Western Reserve University and an MS from Stanford University, both in electrical engineering, and a PhD in computer science from the University of Wisconsin-Madison. While at Wisconsin, he was the principal developer of the Wisconsin Wind Tunnel parallel architecture simulator.   Nathan L. Binkert is currently a Senior Research Scientist with HP Labs and a principal developer of M5. He received a BSE in electrical engineering and an MS and a PhD in computer science both from the University of Michigan. As an intern at Compaq VSSAD, he was a principal developer of the ASIM simulator, currently in widespread use at Intel.   Steve Hines is a Ph.D. candidate in the CS Department at Florida State University, and created the ARM port of M5. He received a BS from Illinois Institute of Technology and an MS from Florida State University.  NOTOC\n"
},
{
	"uri": "http://localhost/docs/extras/",
	"title": "Adding Files to Build",
	"tags": [],
	"description": "",
	"content": "The EXTRAS SCons option is a way to add functionality in gem5 without adding your files to the gem5 source tree. Specifically, it allows you to identify one or more directories that will get compiled in with gem5 as if they appeared under the \u0026lsquo;src\u0026rsquo; part of the gem5 tree, without requiring the code to be actually located under \u0026lsquo;src\u0026rsquo;. It\u0026rsquo;s present to allow user to compile in additional functionality (typically additional SimObject classes) that isn\u0026rsquo;t or can\u0026rsquo;t be distributed with gem5. This is useful for maintaining local code that isn\u0026rsquo;t suitable for incorporating into the gem5 source tree, or third-party code that can\u0026rsquo;t be incorporated due to an incompatible license. Because the EXTRAS location is completely independent of the gem5 repository, you can keep the code under a different revision control system as well.\nThe main drawback of the EXTRAS feature is that, by itself, it only supports adding code to gem5, not modifying any of the base gem5 code. If you are changing the base gem5 code (instead of or in addition to adding code with EXTRAS), see Managing Local Changes with Mercurial Queues.\nOne use of the EXTRAS feature is to support EIO traces. The trace reader for EIO is licensed under the SimpleScalar license, and due to the incompatibility of that license with gem5\u0026rsquo;s BSD license, the code to read these traces is not included in the gem5 distribution. Instead, the EIO code is distributed via a separate \u0026ldquo;encumbered\u0026rdquo; repository.\nThe following examples show how to compile the EIO code. By adding to or modifying the extras path, any other suitable extra could be compiled in. To compile in code using EXTRAS simply execute the following\nscons EXTRAS=/path/to/encumbered build/ALPHA/gem5.opt\nIn the root of this directory you should have a SConscript that uses the Source() and SimObject() scons functions that are used in the rest of M5 to compile the appropriate sources and add any SimObjects of interest. If you want to add more than one directory, you can set EXTRAS to a colon-separated list of paths.\nNote that EXTRAS is a \u0026ldquo;sticky\u0026rdquo; parameter, so after a value is provided to scons once, the value will be reused for future scons invocations targeting the same build directory (build/ALPHA_SE in this case) as long as it is not overridden. Thus you only need to specify EXTRAS the first time you build a particular configuration or if you want to override a previously specified value. For more information on sticky scons options, see the SCons build system page.\nTo run a regression with EXTRAS use a command line similar to the following:\n./util/regress --scons-opts=\u0026quot;EXTRAS=/path/to/encumbered\u0026quot; -j 2 quick\n"
},
{
	"uri": "http://localhost/deprecated/alpha-dependencies/",
	"title": "Alpha Dependencies",
	"tags": [],
	"description": "",
	"content": " Purpose The purpose of this page is to list the areas where M5 is dependent on the alpha architecture, and discuss ways to remove or quarantine the dependencies.\nDependencies VPtr class The VPtr class uses the page size for Alpha in an assert which determines whether or not to an access spans page boundaries. Other than this, this file is not alpha specific, and can be moved somewhere else. My suggestion is that it moves to sim.\nGabe\n-\nIt looks like the only place VPtr is currently used is in kern/linux_threadinfo.hh. Is VPtr compatible with the new memory system? --[Stever](User:Stever \u0026quot;wikilink\u0026quot;) 22:22, 12 February 2006 (EST)  Pseudo Instructions The implementation of the pseudo instructions doesn\u0026rsquo;t seem to be very alpha dependent, other than knowing how to get function parameters. This might be pulled into an interface called abi.hh, for instance, which knows how to speak in the appropriate architecture\u0026rsquo;s ABI. The psuedo_inst.hh and psuedo_inst.cc can use this interface, and live outside of arch, probably in sim.\nGabe\nIt seems like this interface already exists, for the most part, in arguments.hh.\n--Gblack 13:25, 10 February 2006 (EST)\nAfter thinking about it more, there seems to be inherent problems in implementing something like arguments.hh in a completely architecture independent way. One problem is that you can\u0026rsquo;t always return the same thing for a particular argument without knowing certain things about it, like if it\u0026rsquo;s a floating point value, and the particulars that are important are probably not completely consistent across architectures. Also, accessing values beyond the ones held in registers means accessing memory, and that introduces more complications. Instead, what I think should be done is to move the parameter preperation into the decoder. In other words, rather than just passing the execution context to the pseudo instruction function, the execution context and whatever paramers the function needs are passed in after being pulled from wherever, and the function can just operate as standard C++ code with normal arguments. The complication in the decoder would be minimal, and that would allow the pseudo instructions to be reused for all of the architectures.\n--Gblack 18:06, 10 February 2006 (EST)\n-\nI see you appear to have implemented this already... it looks good to me. --[Stever](User:Stever \u0026quot;wikilink\u0026quot;) 22:09, 12 February 2006 (EST)  Misc Regs in CPU Model There are some files in the model that access some of the miscellaneous registers by name. The problem is that misc. regs differ per architecture.\nFor instance, the MIPS has coprocessor0 registers which can be looked at as a version of misc. regs that need to be accessed.\nAlso, when we deallocate or allocate context (or even copy and restore data for syscalls) we copy some of the misc. regs by name.\nTo make this work for different archs. we either need to have all of these cpu files for misc. regs in ISA-specific folders or access the misc. regs by index (bounded by a size variable). Since there are only maybe 3 or 4 files I think the former (ISA-specific folders) is a better idea\nKorey\nCould you list which files specifically your talking about, preferably with function names or even line numbers?\nGabe\n-\nThe original idea was that every ISA would define its own MiscRegFile struct. Any piece of code that accesses individual regs in the struct would have to be ISA-specific. --[Stever](User:Stever \u0026quot;wikilink\u0026quot;) 22:13, 12 February 2006 (EST)  I\u0026rsquo;m part way through defining the MiscRegs class that we decided to use, which would have all of the misc regs in it and be ISA specific. Right now it has the FPCR, Uniq, load locked flag/addr, and the IPRs in it. It has methods readMiscReg and writeMiscReg defined on it. There are two main problems I have encountered so far.\nThe first is deciding where to define the enums used to index into the misc reg file. While the FPCR and Uniq registers already have a dependence tag entry in the DependenceTag enum, the IPRs are in a separate enum, and in a separate file. Because C++ doesn\u0026rsquo;t let you just expand an enum, it\u0026rsquo;s difficult to choose where to define the misc regs. For now I\u0026rsquo;ve left the definitions the same, where the FPCR, uniq regs are defined in the DependenceTags (and I also added the load locked regs), and the IPRs are defined in a separate enum. This requires the two functions to take an input of an int, not a specific enum type, which makes things a little less clear. However, for lack of a better answer, I\u0026rsquo;ve defaulted to leaving the definitions the same. In the future I may opt to put both the misc regs and the IPRs into one enum which is defined for non-full system in the isa_traits.hh file, and defined for full_system in isa_fullsys_traits.hh. It means that certain names will be duplicated across the files, but hopefully it doesn\u0026rsquo;t happen too much and isn\u0026rsquo;t changed often.\nThe second problem is that the current readIpr function has a reference to a Fault passed in. This is only used for two cases in readIpr: when a write-only register is accessed, and when the index doesn\u0026rsquo;t match any IPR. In those cases it returns an UnimplementedOpcodeFault. However, because readIpr calls now go through the readMiscReg function, the readMiscReg function must take in a reference to a Fault. This means that code that previously accessed the FPCR or Uniq register will have to pass in a dummy Fault. Unforutnately because it\u0026rsquo;s a reference, I can\u0026rsquo;t set the default argument for it to NULL. While not a huge problem, it does make the code a bit messier for just two cases that only exist in full system mode. \u0026ndash;Ktlim 16:37, 23 February 2006 (EST)\nev5 IPRs The file arch/alpha/ev5.hh is used directly in a few places in the non-alpha portion of m5 where the functions setIpr and readIpr are defined. These files are cpu/o3/cpu.hh and cpu/o3/regfile.hh. These files shouldn\u0026rsquo;t be tied to a alpha, and even more so to a specific type of alpha. I think the best solution would be to move any Ipr related functionality to an intentionally alpha specific version of the cpu, which I believe cpu/o3/alpha_cpu.[hh|cc] is for. Unfortunately, there doesn\u0026rsquo;t seem to be any place to declare the actual storage location for the IPRs other than in regfile.hh where they are currently.\nTo fix this, I propose that the way the CPU model is specialized is change to be based on inheritance, rather than being based on templates. That would allow an AlphaCPU (or similar) to have an IPR register file declared internally to itself. The functions which manipulate the IPR would be defined at that level as well.\n--Gblack 18:37, 12 February 2006 (EST)\n-\nIs this a general problem or an O3 CPU model design issue? Is the problem with IPR storage or the function call interface to access IPRs?  -\nThe register file is going to be ISA-specific anyway (e.g., the number of regs and MiscRegFile will depend on the ISA), so I don't see how allocating storage for IPRs in regfile.hh is different. I would guess that every CPU has something along the line of IPRs even if they're called something different. Another possibility would be to allocate space for necessary IPRs inside of MiscRegFile.  -\nWe had also decided previously that the IPRs that are really internal control registers (not just scratch space) should be associated with the things they control and not with the register file. For example, accesses to TLB control IPRs should be diverted directly to the TLB, as opposed to storing the values in a central IPR reg file and forcing the TLB to look there. --[Stever](User:Stever \u0026quot;wikilink\u0026quot;) 22:19, 12 February 2006 (EST)  The register file will be ISA specific in certain parameters, like the number and width of registers, but the IPR registers are more fundementally specific to Alpha. By specific, I mean that all the registers are referred to by name in the regfile.hh. This, especially in its current form, shouldn\u0026rsquo;t be in general purpose code. My understanding is that the IPRs are basically control registers. If that\u0026rsquo;s correct (please let me know otherwise), those should be stored in the MiscRegFile with the other control registers. It\u0026rsquo;s not immediately clear without looking at the code more where the readIpr and setIpr functions would go in that situation.\n--Gblack 00:58, 13 February 2006 (EST)\n-\nAs far as the cpu/o3/cpu.hh include of arch/alpha/ev5.hh, that's definitely in the wrong place. That was a relic of some old code I had used as a building block for full system mode. Considering that it's reading from an IPR to get the specific ITB/DTB ASN, it should probably be at the AlphaFullCPU level. I'm not too clear on what you mean by specialization through inheritance. The most derived level of CPU is the AlphaFullCPU which does have methods defined for handling the IPRs in it, although they just defer to the register file. If you wanted a separate misc reg file that's specific to each ISA, you could put it at this level, though as Steve has mentioned, we're still having discussions about where exactly to put all those other registers.  -\nFor the cpu/o3/regfile.hh include of arch/alpha/ev5.hh, I'm not sure how to handle this. The solution will be linked to where exactly we decide to place all of the registers. If we continue to have one centralized location for all IPRs, then it may make sense for the regfile (or maybe a class derived from the regfile) to store those IPRs and provide methods to read and write to them. If it's decentralized, then the control for reading and writing those registers probably needs to go to the AlphaFullCPU. Some storage may still be in the regfile.hh (if we agree that all ISAs have some sort of \u0026quot;MiscRegFile\u0026quot; and \u0026quot;ipr\u0026quot; class that can be obtained through the ISA namespace).  -\nNot all IPRs are control registers. Some are just used to place values when faults happen, such as the virtual address register, which gets the faulting address written to it when the ITB or DTB fault. The problem is that while it's convenient to logically think of the IPRs as one big array of registers, that's not necessarily how they act or are accessed, and it creates some problems when moving over to using ExecContext as an interface and not for storing any state. --[Ktlim](User:Ktlim \u0026quot;wikilink\u0026quot;) 14:31, 13 February 2006 (EST)  Faults arch/alpha/faults.hh and arch/alpha/faults.cc describe some types of faults which can occur, and provide a mapping to the names of the faults. This mechanism needs to be pared back. One the one hand, if it\u0026rsquo;s too specific it doesn\u0026rsquo;t allow for variations in faults in other architectures. On the other hand, if it\u0026rsquo;s general, it can force generality on the faults in other architectures that need to be specific. For instance, there is only one entry in faults.hh for an Unimplemented Opcode fault, but in SPARC, there are 3 exceptions this could map to. These are \u0026ldquo;illegal_instruction\u0026rdquo;, \u0026ldquo;unimplemented_LDD\u0026rdquo;, and \u0026ldquo;unimplemented_STD\u0026rdquo;. In another example, there is only 1 processor reset fault in faults.hh, but SPARC defines 4 matching exceptions, \u0026ldquo;watchdog_reset\u0026rdquo;, \u0026ldquo;externally_initiated_reset\u0026rdquo;, \u0026ldquo;software_initiated_reset\u0026rdquo;, and \u0026ldquo;power_on_reset\u0026rdquo;.\nIn order to address these issues, I propose that faults.hh be reduced in scope to just those exceptions which are -very- common across architectures with almost no semantic variation, namely \u0026ldquo;Machine_Check_Fault\u0026rdquo; and \u0026ldquo;No_Fault\u0026rdquo;. The other faults can be enumerated internally to the architecture, which is where they almost always generated and where they are consumed. The exact mechanism this happens with needs to change as well, since enums can\u0026rsquo;t be extended once they\u0026rsquo;re defined. One way to do this would be to define constants inside a namespace. A problem with this approach would be that it would be difficult to assure that there were never collisions between the different places faults were defined. Another approach would be to create a Fault class which would be inherited by the more specialized Faults. A pointer to this class could be passed around by code which is unaware of the specialization while the Fault is enroute to where ever the architecture deals with it.\n--Gblack 18:56, 12 February 2006 (EST)\n-\nI thought we had decided that we would have an ISA-independent base class that ISAs could derive from to define their own fault types (with associated state), and that functions would return fault object pointers, with a null pointer indicating no fault (thus eliminating the No_Fault value).  -\nI'd still like to see some attempt to organize common fault concepts in an ISA-independent fashion; so for example we could have an ISA-independent IllegalInstruction class, but SPARC could derive from that to create the three specific subclasses we need. --[Stever](User:Stever \u0026quot;wikilink\u0026quot;) 22:25, 12 February 2006 (EST)  That\u0026rsquo;s what I remember deciding. This is here for completeness.\n--Gblack 00:59, 13 February 2006 (EST)\nThere were complications implementing the Fault classes as described above for various reasons including ones having to do with fault statistics and fault vectors. The currently implemented system has a set of system wide Faults which are generated by non-isa code, and a second set which are maintained by the ISA. Eventually, the Faults should be pulled out of everything but the CPU and the ISA, and be defined and maintained by the ISA. The CPU will have a set of interface functions which will return a small subset of faults for certain situations, which will allow the CPU to pass some faults directly to the ISA.\n--Gblack 01:53, 16 February 2006 (EST)\nExecContext and CPU models In various parts of the XC and the CPUs there are very Alpha-specific code segments. For example, in the ExecContext, the read and write functions will set/check the lock addr and lock flag when doing accesses in full system mode. The SimpleCPU has interrupt code for Alpha (and probably more specifically the ev5) in its tick() function. What was our plan for handling these?\nIn the CPU model I had planned to abstract away the functions that were specific to Alpha into the AlphaISA itself. In FastCPU (mostly a clone of SimpleCPU), the interrupt code that existed in SimpleCPU is replaced by a call to TheISA::checkInterrupts(this). The other two places that were mostly ISA specific were clearing the zero registers, and calling the proper trap function, both of which are abstracted away to the ISA.\nShould we try something similar for ExecContext? It might be somewhat more difficult because the code in ExecContext has control statements in it, such as \u0026ldquo;return NoFault\u0026rdquo;. We\u0026rsquo;d have to check for specific return values from the call to the ISA, and handle them appropriately, which somewhat defeats the idea of removing the ISA dependences. Perhaps we\u0026rsquo;ll just punt for now and leave it as it is, where the code is guarded by a #if defined(TARGET_ALPHA).\nAlso, there are ISA specific structures within both the XC and the CPUs. There are AlphaITB and DTB pointers in both. I think we agreed upon (or maybe just Nate suggested) having some sort of semi-opaque ISA object that holds onto ISA specific objects such as the Alpha ITB and DTB. It could live in the CPU and anything that needs it could have a pointer to it. We\u0026rsquo;d have to define a specific interface for it so that the ITB and DTB can be used.\nObject Loader flexibility Right now, the object loader expects that all the binaries it reads will be for Alpha, and determines what architecture to use based on the type of file, ie. ecoff is Tru64, elf is Linux, and aout is PAL code. Solaris supports all three formats, and Linux supports at least elf and aout. There is also obviously a need to load binaries for other architectures as well. What I think would be a good solution would be for the loader to determine from the binary what sort of file it is, ie what architecture it wants to run on and what os it\u0026rsquo;s for. If those things exist and work in that combination, m5 reports what it will use and starts the program appropriately. If that combination isn\u0026rsquo;t supported, then m5 can panic. I\u0026rsquo;m most familiar with elf, and for elf it would be relatively easy to determine what the requirements of the binary are. Is it harder for the other formats? Also, how could we determine if a particular combination of OS and architecture will work? We\u0026rsquo;ll need to do that regardless if we want to load object files safely.\n--Gblack 04:02, 1 March 2006 (EST)\n-\nThe framework is already there for deriving both the architecture and the OS from the binary: there are enums in ObjectFile to report both of these. It doesn't (quite) assume that all the binaries it sees are Alpha; that's just the only one it recognizes right now :-). The purpose of the tryFile() method is to see if the object is of the given type, and if so, create an ObjectFile object that knows the appropriate ISA and OS it's for.  -\nNote that the ecoff tryFile() only reports Tru64 if it's an Alpha binary, otherwise it gives up. So a SPARC Solaris ecoff file would not be a problem, particularly if that's the only SPARC ecoff platform we know of. Similarly for elf; there's a check in there for the Alpha but Ali commented it out because it's \u0026quot;not official\u0026quot;. I don't know if he found something that uses a different machine code or not. Ali?  -\nRight now m5 does auto-detect the OS and generate the correct syscall emulation object based on the executable (see LiveProcess::create() in sim/process.cc). This is not hard since the executable is a parameter to the Process object which is what determines the syscall emulation. This would also be the right place to detect an unsupported ISA/OS combination (assuming there are combinations that the loader recognizes but we don't support).  -\nI had thought about automatically generating the correct CPU ISA based on the binary as well; I agree that would be cool. It would be significantly harder though since we would have to defer creating the CPU object until it knows what binary it will be running. This dependence doesn't fit at all in our current initialization scheme, which assumes that we can create all the objects in one pass and then make another pass if necessary to resolve init-time dependences among objects. That is, having the \\*creation\\* of one object (i.e., the CPU) depend on some post-creation initialization of another object (the Process object) just doesn't fit the mold. So even though it would be cool, I think in the short term anyway we should rely on users to specify AlphaCPU or SparcCPU or MipsCPU in their config file, and then just generate an error if they mess up. The number of people that will be doing cross-ISA experiments is probably vanishingly small anyway. --[Stever](User:Stever \u0026quot;wikilink\u0026quot;) 08:22, 1 March 2006 (EST)  -\n - From what I've seen so far, the changes to the ELF loader code have been very small. The only things necessary were adding a Mips 'enum value' in object_file.hh, removing the panic that m5 throws when it sees a 32-bit ELF in the elf_object.cc file, and finally calling the ElfObject constructor with the enum value I specified earlier. (This is also assuming one has already made edits so that you load a \u0026lt;your arch here\u0026gt;LinuxProcess too). The GELF library looks to be a real lifesaver since it handles both 32 \u0026amp; 64-bit formats. The ELF file seems to load correctly, so I'm wondering am I overlooking something I need to change?  -\n - You need to do something in sim/process.cc to create a MipsLinuxProcess once you've detected that it's a MIPS binary (along with whatever it takes to set up the MipsLinuxProcess object).  -\nI've created a new bridging header called \u0026quot;process.hh\u0026quot; which brings in all the different types of processes for a particular architecture. This is because not every OS is available for every ISA, so they can't invidividually be bridging, and including each individaully all the time brings in alot of other ISA specific files. In each process.hh, there is also a \u0026quot;createProcess\u0026quot; function, which is used to generate the appropriate process object from an object file or to panic if the object file doesn't fit with the ISA. \\--[Gblack](User:Gblack \u0026quot;wikilink\u0026quot;) 22:08, 2 March 2006 (EST)  -\nA problem which has come up with the libelf as distributed with m5 is that it is has different constants defined than the one installed locally on my computers. The local version is used first, which causes compilation errors when certain constants are used, specifically the ones I need for 64 bit/v9 SPARC. Several possible solutions are to update the version used in m5 so that it is in line with the local version, remove the version in m5 and use only the local version, or force m5 to use only it's own internal version. Some problems with using the internal version only have been that gcc, despite what seems to be in the documentation, is not allowing m5 to direct it to find it's internal version first. It is also not possible to force the correct version by changing the include directives or their paths because there are includes inside the file which use \\\u0026lt;\\\u0026gt;s and the normal paths. Preprocessing the files so that they don't have includes also doesn't work, because the macros which define the constants it uses go away, and the issue of getting m5 to find the correct header files during this step is the same as in the original. \\--[Gblack](User:Gblack \u0026quot;wikilink\u0026quot;) 22:08, 2 March 2006 (EST)  -\ngcc needs to have both the path to the libelf directory, and that path including the libelf directory, indicated with the -I option to find the m5 version of libelf always. Also, in libelf, the line \u0026quot;\\#undef __LIBELF_HEADER_ELF_H\u0026quot; in lib/sys_elf.h.in needs to be commented out to prevent libelf from including the system header elf.h, which is provided with glibc, and to instead use its internal elf_repl.h. Making these changes, as well as updating libelf to have a complete list of constants, has cleared up the current problems with that library and its headers. \\--[Gblack](User:Gblack \u0026quot;wikilink\u0026quot;) 02:04, 4 March 2006 (EST)  "
},
{
	"uri": "http://localhost/benchmarks/android-mashmellow/",
	"title": "Android Marshmallow",
	"tags": [],
	"description": "",
	"content": " These instructions expect that the user will be using ARM support of gem5\n Overview To successfully run Android in gem5, an image, a compatible kernel and a device tree blob .dtb) file configured for the simulator are necessary. This guide shows how to build Android Marshmallow 32bit version using a 3.14 kernel with Mali support. An extra section will be added in the future on how to build the 4.4 kernel with Mali.\nPre-requisites This guide assumes a 64-bit system running 14.04 LTS Ubuntu. Before starting it is important first to set up our system correctly. To do this the following packages need to be installed through shell.\n Tip: Always check for the up-to-date prerequisites at the Android build page.  Update and install all the dependencies. This can be done with the following commands:\nsudo apt-get update\nsudo apt-get install openjdk-7-jdk git-core gnupg flex bison gperf build-essential zip curl zlib1g-dev gcc-multilib g++-multilib libc6-dev-i386 lib32ncurses5-dev x11proto-core-dev libx11-dev lib32z-dev ccache libgl1-mesa-dev libxml2-utils xsltproc unzip\nAlso, make sure to have repo correctly installed (instructions here).\nEnsure that the default is OpenJDK 1.7\njavac -version\nTo cross-compile the kernel (32bit) and for the device tree we will need the following packages to be installed:\nsudo apt-get install gcc-arm-linux-gnueabihf device-tree-compiler\nBefore getting started, as a final step make sure to have the gem5 binaries and busybox for 32-bit ARM.\n For the gem5 binaries just do the following starting from your gem5 directory:  -\n`.cd util/m5` `make -f Makefile.arm` `cd ../term` `make` `cd ../../system/arm/simple_bootloader/` `make`   For busybox you can find the guide here  Building Android We build Android Marshmallow using an AOSP running build based on the release for the Pixel C. The AOSP provides other builds, which are untested with this guide.\nTip: Synching with repo will take a long time. Use -jN to speed up the make process, where N number of parallel jobs to run.   Make a directory and pull the android repository. Type in the following:  -\n`mkdir android` `cd android` `repo init --depth=1 -u `\u0026lt;https://android.googlesource.com/platform/manifest\u0026gt;` -b android-6.0.1_r63` `repo sync -c -jN`   Before you start the AOSP build, you will need to make one change to the build system to enable building libion.so, which is used by the Mali driver. Edit the file aosp/system/core/libion/Android.mk to change LOCAL_MODULE_TAGS for libion from \u0026lsquo;optional\u0026rsquo; to \u0026lsquo;debug\u0026rsquo;. Here is the output of repo diff:   --- a/system/core/libion/Android.mk  +++ b/system/core/libion/Android.mk  @@ -3,7 +3,7 @@ LOCAL_PATH := $(call my-dir)  include $(CLEAR_VARS)  LOCAL_SRC_FILES := ion.c  LOCAL_MODULE := libion  -LOCAL_MODULE_TAGS := optional  +LOCAL_MODULE_TAGS := debug  LOCAL_SHARED_LIBRARIES := liblog  LOCAL_C_INCLUDES := $(LOCAL_PATH)/include $(LOCAL_PATH)/kernel-headers  LOCAL_EXPORT_C_INCLUDE_DIRS := $(LOCAL_PATH)/include  $(LOCAL_PATH)/kernel-headers\n Source the environment setup and build Android. Make sure to do this in a bash shell.    Tip: For root access and \u0026ldquo;debuggability\u0026rdquo; [sic] we choose userdebug. Build can be done in different modes as seen here\n Tip: Making Android will take a long time. Use -jN to speed up the make process, where N number of parallel jobs to run.  -\n`source build/envsetup.sh`  -\n`lunch aosp_arm-userdebug`  -\n`make -jN`  Creating an Android image After a successful build, we create an image of Android and add the init files and binaries that configure the system for gem5.\n Create an empty image to flash the Android build and attach the image to a loopback device. The commands below create a 3GB image.  Tip: If you want to add applications or data, make the image large enough to fit the build and anything else that is meant to be written into it.  -\n`dd if=/dev/zero of=myimage.img bs=1M count=2560`  -\n`sudo losetup /dev/loop0 myimage.img`   Make three partitions AndroidRoot (1.5GB), AndroidData (1GB), AndroidCache (512MB):  -\nPartition the device:  -\n`sudo fdisk /dev/loop0`  -\nUpdate the partition table:  -\n`sudo partprobe /dev/loop0`  -\nName the partitions / Define filesystem as ext4:  -\n`sudo mkfs.ext4 -L AndroidRoot /dev/loop0p1`  -\n`sudo mkfs.ext4 -L AndroidData /dev/loop0p2`  -\n`sudo mkfs.ext4 -L AndroidCache /dev/loop0p3`   Mount the Root partition to a directory:  -\n`sudo mkdir -p /mnt/androidRoot`  -\n`sudo mount /dev/loop0p1 /mnt/androidRoot`   Load the build to the partition:  -\n`cd /mnt/androidRoot`  -\n`sudo zcat `\u0026lt;path/to/build/android\u0026gt;`/out/target/product/generic/ramdisk.img | sudo cpio -i`  -\n`sudo mkdir cache`  -\n`sudo mkdir /mnt/tmp`  -\n`sudo mount -oro,loop `\u0026lt;path/to/build/android\u0026gt;`/out/target/product/generic/system.img /mnt/tmp`  -\n`sudo cp -a /mnt/tmp/* system/`  -\n`sudo umount /mnt/tmp`   Download and unpack the overlays that are necessary from the gem5 Android KitKat page and make the following changes to the init.gem5.rc file. Here is the output of repo diff:   --- /kitkat_overlay/init.gem5.rc  +++ /m_overlay/init.gem5.rc  @@ -1,21 +1,13 @@  +  on early-init  mount debugfs debugfs /sys/kernel/debug    on init  - export LD_LIBRARY_PATH ${LD_LIBRARY_PATH}:/vendor/lib/egl  -  - # See storage config details at http://source.android.com/tech/storage/  - mkdir /mnt/media_rw/sdcard 0700 media_rw media_rw  - mkdir /storage/sdcard 0700 root root  + # Support legacy paths  + symlink /sdcard /mnt/sdcard  chmod 0666 /dev/mali0  chmod 0666 /dev/ion  -  - export EXTERNAL_STORAGE /storage/sdcard  -  - # Support legacy paths  - symlink /storage/sdcard /sdcard  - symlink /storage/sdcard /mnt/sdcard    on fs  mount_all /fstab.gem5  @@ -60,7 +52,6 @@  group root  oneshot    -# fusewrapped external sdcard daemon running as media_rw (1023)  -service fuse_sdcard /system/bin/sdcard -u 1023 -g 1023 -d  /mnt/media_rw/sdcard /storage/sdcard  +service fingerprintd /system/bin/fingerprintd  class late_start  - disabled  + user system\n Add the Android overlays and configure their permissions:  -\n`sudo cp -r `\u0026lt;path/to/android/overlays\u0026gt;`/* /mnt/androidRoot/`  -\n`sudo chmod ug+x /mnt/androidRoot/init.gem5.rc`  -\n`/mnt/androidRoot/gem5/postboot.sh`   Add the m5 and busybox binaries under the sbin directory and make them executable:  -\n`sudo cp `\u0026lt;path/to/gem5\u0026gt;`/util/m5/m5 /mnt/androidRoot/sbin`  -\n`sudo cp `\u0026lt;path/to/busybox\u0026gt;`/busybox /mnt/androidRoot/sbin`  -\n`sudo chmod a+x /mnt/androidRoot/sbin/busybox /mnt/androidRoot/sbin/m5`   Make the directories readable and searchable:  -\n`sudo chmod a+rx /mnt/androidRoot/sbin/ /mnt/androidRoot/gem5/`   Remove the boot animation:  -\n`sudo rm /mnt/androidRoot/system/bin/bootanimation`   Download and unpack the Mali drivers (for gem5 Android 4.4) from here. Add the drivers and adjust their permissions:  -\nMake the directories for the drivers and copy them:  -\n`sudo mkdir -p /mnt/androidRoot/system/vendor/lib/egl`  -\n`sudo mkdir -p /mnt/androidRoot/system/vendor/lib/hw`  -\n`sudo cp `\u0026lt;path/to/userspace/Mali/drivers\u0026gt;`/lib/egl/libGLES_mali.so /mnt/androidRoot/system/vendor/lib/egl`  -\n`sudo cp `\u0026lt;path/to/userspace/Mali/drivers\u0026gt;`/lib/hw/gralloc.default.so /mnt/androidRoot/system/vendor/lib/hw`  -\nChange the permissions  -\n`sudo chmod 0755 /mnt/androidRoot/system/vendor/lib/hw`  -\n`sudo chmod 0755 /mnt/androidRoot/system/vendor/lib/egl`  -\n`sudo chmod 0644 /mnt/androidRoot/system/vendor/lib/egl/libGLES_mali.so`  -\n`sudo chmod 0644 /mnt/androidRoot/system/vendor/lib/hw/gralloc.default.so`   Unmount and remove loopback device:  -\n`cd /..`  -\n`sudo umount /mnt/androidRoot`  -\n`sudo losetup -d /dev/loop0`  Building the Kernel (3.14) After successfully setting up the image, a compatible kernel needs to be built and a .dtb file generated.\n Clone the repository containing the gem5 specific kernel:  -\n`git clone -b ll_20140416.0-gem5 `\u0026lt;https://github.com/gem5/linux-arm-gem5.git\u0026gt;   Make the following changes to the kernel gem5 config file at /arch/arm/configs/vexpress_gem5_defconfig. Here is the output of repo diff:   --- a/arch/arm/configs/vexpress_gem5_defconfig  +++ b/arch/arm/configs/vexpress_gem5_defconfig  @@ -200,4 +200,15 @@ CONFIG_EARLY_PRINTK=y  CONFIG_DEBUG_PREEMPT=n  # CONFIG_CRYPTO_ANSI_CPRNG is not set  # CONFIG_CRYPTO_HW is not set  +CONFIG_MALI_MIDGARD=y  +CONFIG_MALI_MIDGARD_DEBUG_SYS=y  +CONFIG_ION=y  +CONFIG_ION_DUMMY=y  CONFIG_BINARY_PRINTF=y  +CONFIG_NET_9P=y  +CONFIG_NET_9P_VIRTIO=y  +CONFIG_9P_FS=y  +CONFIG_9P_FS_POSIX_ACL=y  +CONFIG_9P_FS_SECURITY=y  +CONFIG_VIRTIO_BLK=y  +CONFIG_VMSPLIT_3G=y  +CONFIG_DNOTIFY=y  +CONFIG_FUSE_FS=y\n For the device tree, add the Mali GPU device and increase the memory to 1.8GB. Do this with the following changes at /arch/arm/boot/dts/vexpress-v2p-ca15-tc1-gem5.dts. Here is the output of repo diff:   --- a/arch/arm/boot/dts/vexpress-v2p-ca15-tc1-gem5.dts  +++ b/arch/arm/boot/dts/vexpress-v2p-ca15-tc1-gem5.dts  @@ -45,7 +45,7 @@    memory@80000000 {  device_type = \u0026quot;memory\u0026quot;;  - reg = \u0026lt;0 0x80000000 0 0x40000000\u0026gt;;  + reg = \u0026lt;0 0x80000000 0 0x74000000\u0026gt;;  };    hdlcd@2b000000 {  @@ -59,6 +59,14 @@  // mode = \u0026quot;3840x2160MR-16@60\u0026quot;; // UHD4K mode string  framebuffer = \u0026lt;0 0x8f000000 0 0x01000000\u0026gt;;  };  +  + gpu@0x2d000000 {  + compatible = \u0026quot;arm,mali-midgard\u0026quot;;  + reg = \u0026lt;0 0x2b400000 0 0x4000\u0026gt;;  + interrupts = \u0026lt;0 86 4\u0026gt;, \u0026lt;0 87 4\u0026gt;, \u0026lt;0 88 4\u0026gt;;  + interrupt-names = \u0026quot;JOB\u0026quot;, \u0026quot;MMU\u0026quot;, \u0026quot;GPU\u0026quot;;  + };  +  /*  memory-controller@2b0a0000 {  compatible = \u0026quot;arm,pl341\u0026quot;, \u0026quot;arm,primecell\u0026quot;;\n Download and unpack the userspace matching Mali kernel drivers for gem5 from here. Copy them to the gpu driver directory:  -\n`cp -r `\u0026lt;path/to/kernelspace/Mali/drivers\u0026gt;`/driver/product/kernel/drivers/gpu/arm/ drivers/gpu`   Change the following in /drivers/video/Kconfig and /drivers/gpu/Makefile:  Here is the output of the Kconfig repo diff:\n --- a/drivers/video/Kconfig  +++ b/drivers/video/Kconfig  @@ -23,6 +23,8 @@ source \u0026quot;drivers/gpu/host1x/Kconfig\u0026quot;    source \u0026quot;drivers/gpu/drm/Kconfig\u0026quot;    +source \u0026quot;drivers/gpu/arm/Kconfig\u0026quot;  +  config VGASTATE  tristate  default n\nHere is the output of the drivers/gpu/Makefile repo diff:\n --- a/drivers/gpu/Makefile  +++ b/drivers/gpu/Makefile  @@ -1,2 +1,2 @@  -obj-y += drm/ vga/  +obj-y += drm/ vga/ arm/\n Finally, build the kernel and the .dtb file:  -\nBuild the kernel  Tip: Use -jN to speed up the make process, where N number of parallel jobs to run.  -\n`make CROSS_COMPILE=arm-linux-gnueabihf- ARCH=arm vexpress_gem5_defconfig`  -\n`make CROSS_COMPILE=arm-linux-gnueabihf- ARCH=arm vmlinux -jN`  -\nCreate the .dtb file:  -\n`dtc -I dts -O dtb arch/arm/boot/dts/vexpress-v2p-ca15-tc1-gem5.dts \u0026gt; vexpress-v2p-ca15-tc1-gem5.dtb`  Testing the build Make the following changes to example/fs.py. Here is the output repo diff:\n --- a/configs/example/fs.py Thu Jun 02 20:34:39 2016 +0100  +++ b/configs/example/fs.py Fri Jun 10 15:37:29 2016 -0700  @@ -144,6 +144,13 @@  if is_kvm_cpu(TestCPUClass) or is_kvm_cpu(FutureClass):  test_sys.vm = KvmVM()    + test_sys.gpu = NoMaliGpu(  + gpu_type=\u0026quot;T760\u0026quot;,  + ver_maj=0, ver_min=0, ver_status=1,  + int_job=118, int_mmu=119, int_gpu=120,  + pio_addr=0x2b400000,  + pio=test_sys.membus.master)  +  if options.ruby:  # Check for timing mode because ruby does not support atomic accesses  if not (options.cpu_type == \u0026quot;detailed\u0026quot; or options.cpu_type == \u0026quot;timing\u0026quot;):\nAnd the changes to FS config to either enable or disable software rendering.\n --- a/configs/common/FSConfig.py Thu Jun 02 20:34:39 2016 +0100  +++ b/configs/common/FSConfig.py Thu Jun 16 10:23:44 2016 -0700  @@ -345,7 +345,7 @@    # release-specific tweaks  if 'kitkat' in mdesc.os_type():  - cmdline += \u0026quot; androidboot.hardware=gem5 qemu=1 qemu.gles=0 \u0026quot; + \\  + cmdline += \u0026quot; androidboot.hardware=gem5 qemu=1 qemu.gles=1 \u0026quot; + \\  \u0026quot;android.bootanim=0\u0026quot;    self.boot_osflags = fillInCmdline(mdesc, cmdline\nSet the following M5_PATH:\nM5_PATH=. build/ARM/gem5.opt configs/example/fs.py --cpu-type=atomic --mem-type=SimpleMemory --os-type=android-kitkat --disk-image=myimage.img --machine-type=VExpress_EMM --dtb-filename=vexpress-v2p-ca15-tc1-gem5.dtb -n 1 --mem-size=1800MB\n"
},
{
	"uri": "http://localhost/docs/arch-state/",
	"title": "Architectural State",
	"tags": [],
	"description": "",
	"content": " Registers Register types - float, int, misc Indexing - register spaces stuff See Register Indexing for a more thorough treatment.\nA \u0026ldquo;nickle tour\u0026rdquo; of flattening and register indexing in the CPU models.\nFirst, an instruction has identified that it needs register such and such as determined by its encoding (or the fact that it always uses a certain register, or \u0026hellip;). For the sake of argument, lets say we\u0026rsquo;re talking about SPARC, the register is %g1, and the second bank of globals is active. From the instructions point of view, the unflattened register is %g1, which, likely, is just represented by the index 1.\nNext, we need to map from the instruction\u0026rsquo;s view of the register file(s) down to actual storage locations. Think of this like virtual memory. The instruction is working within an index space which is like a virtual address space, and it needs to be mapped down to the flattened space which is like physical memory. Here, the index 1 is likely mapped to, say, 9, where 0-7 is the first bank of globals and 8-15 is the second.\nThis is the point where the CPU gets involved. The index 9 refers to an actual register the instruction expects to access, and it\u0026rsquo;s the CPU\u0026rsquo;s job to make that happen. Before this point, all the work was done by the ISA with no insight available to the CPU, and beyond this point all the work is done by the CPU with no insight available to the ISA.\nThe CPU is free to provide a register directly like the simple CPU by having an array and just reading and writing the 9th element on behalf of the instruction. The CPU could, alternatively, do something complicated like renaming and mapping the flattened index further into a physical register like O3.\nOne important property of all this, which makes sense if you think about the virtual memory analogy, is that the size of the index space before flattening has nothing to do with the size after. The virtual memory space could be very large (presumably with gaps) and map to a smaller physical space, or it could be small and map to a larger physical space where the extra is for, say, other virtual spaces used at other times. You need to make sure you\u0026rsquo;re using the right size (post flattening) to size your tables because that\u0026rsquo;s the space of possible options.\nOne other tricky part comes from the fact that we add offsets into the indices to distinguish ints from floats from miscs. Those offsets might be one thing in the preflattening world, but then need to be something else in the post flattening world to keep things from landing on top of each other without leaving gaps. It\u0026rsquo;s easy to make a mistake here, and it\u0026rsquo;s one of the reasons I don\u0026rsquo;t like this offset idea as a way to keep the different types separate. I\u0026rsquo;d rather see a two dimensional index where the second coordinate was a register type. But in the world as it exists today, this is something you have to keep track of.\nPCs "
},
{
	"uri": "http://localhost/benchmarks/bbench/",
	"title": "BBench",
	"tags": [],
	"description": "",
	"content": " Note that the ICS images are deprecated. Please see the instructions for running Android on gem5 and how to use workload automation.\nTips for Making Your Disk Image gem5 Friendly Speeding Up the Boot Process When a fresh Android image is booted it generates a lot of files and does a lot of JIT compiling; this can slow down the boot process significantly. gem5 uses a copy-on-write (COW) layer between the simulator and the actual disk image, because of this COW layer none of the changes are stored to the disk image. To make these changes permanent, and avoid having to repeat them in the future, you can remove the COW layer during the first boot of an image. This will significantly speedup future runs. To do so make the following changes to configs/common/FSConfig.py:\n\n class RawIdeDisk(IdeDisk):  image = RawDiskImage(read_only=False)  def childImage(self, ci):  self.image.image_file=ci\n\nThen, inside of makeArmSystem(), change from:\n\n self.c0 = CowIdeDisk(driveID='master')\n\nto\n\n self.c0 = RawIdeDisk(driveID='master')\n\nBe careful when doing this. Any changes made to the disk image will be permanent when using the raw ide disk. To ensure that all the changes are written to the disk image properly you can use the sync and halt Linux commands. These are not available on Android so it is recommended that you use something like BusyBox. Once the image has finished booting and has settled, you can run busybox sync and busybox halt -f from a gem5 terminal to write all changes to the disk and halt it properly. You will likely get a panic when the simulator exits regarding an unrecognized byte, however, this doesn\u0026rsquo;t seem to cause any problems. Remember to re-enable the COW layer once you\u0026rsquo;ve finished setting up your disk image.\nBusyBox BusyBox is a useful tool providing many common Linux utilities for embedded systems. To build busybox, download the source and compile statically using the following commands:\n make CROSS_COMPILE=arm-none-linux-gnueabi- defconfig LDFLAGS=\u0026quot;--static\u0026quot; make CROSS_COMPILE=arm-none-linux-gnueabi- -jn  Place the busybox binary into the /sbin/ directory of your Android file-system. Run busybox --help to see a full list of the available utilities.\nAndroid Init Process To have your benchmarks start automatically you will likely need to modify the init.rc script located in the file-system\u0026rsquo;s root directory. You can see the changes made to the init.rc script for BBench by looking at the init.rc script in the images we distribute with BBench, this may be a useful template. Another useful resource for understanding the init process is located here.\nm5 Utility You will likely need the m5 utility, whose source is located in util/m5/, to do anything useful with your disk image. Build this (statically) and place it in the /sbin/ directory of your Android file-system.\nPublications If you use BBench in your work please cite our IISWC 2011 paper:\nA. Gutierrez, R.G. Dreslinski, T.F. Wenisch, T. Mudge, A. Saidi, C. Emmons, and N. Paver. Full-System Analysis and Characterization of Interactive Smartphone Applications. IEEE International Symposium on Workload Characterization, pages 81-90, Austin, TX, November 2011.\n"
},
{
	"uri": "http://localhost/deprecated/bad-names/",
	"title": "Bad names",
	"tags": [],
	"description": "",
	"content": " We may have a couple of classes, variables, and functions that aren\u0026rsquo;t named too well. Some are too long, aren\u0026rsquo;t descriptive, etc. List the things that you don\u0026rsquo;t like here, and try to give some suggestions for alternates.\nFunction names  setMiscRegWithEffect - setMiscReg (will have to rename current setMiscReg to setMiscRegNoEffect or something similar); setMisc readMiscRegWithEffect - same as above  Class names Variable names Command to replace string find . -name \u0026quot;*.cc\u0026quot; -o -name \u0026quot;*.hh\u0026quot; -o -name \u0026quot;*.isa\u0026quot; | grep -v SCCS | xargs perl -pi -e \u0026quot;s/oldstring/newstring/\u0026quot;\n"
},
{
	"uri": "http://localhost/docs/branch-delay-slots/",
	"title": "Branch-delay slots",
	"tags": [],
	"description": "",
	"content": " Since MIPS and SPARC use branch delay slots, we\u0026rsquo;re faced with an interesting issue on how to implement them correctly. There are two issues: basic support for branch delay slots, and support for conditionally executed delay-slot instructions (SPARC \u0026ldquo;annulled\u0026rdquo; delay slots).\nBasic functionality In the context of M5 instruction execution, PC is the current instruction\u0026rsquo;s PC and NPC is the PC of the next instruction. Conventional non-delayed branches (as in Alpha) write to NPC to change the next instruction executed. Conceptually all non-branch instructions also update NPC to PC+4, though currently this is implemented in the CPU model and not in the ISA. (Obviously this would have to change if we had variable-length instructions.) Thus between each pair of instruction executions we simply set PC to NPC to advance the program counter.\nIn delayed-branch architectures, we need an additional PC, NNPC, which is the PC of the \u0026ldquo;next next\u0026rdquo; instruction. A simple delayed branch can be implemented by writing the target address to NNPC instead of NPC. Non-branch instructions set NNPC to NPC+4. Between each pair of instruction executions we set PC to NPC and NPC to NNPC. Note that this model (I think) automatically supports some of the funky SPARC delayed branch semantics, such as having a branch in the delay slot of another branch. In MIPS, executing a branch in a branch delay slot results in UNDETERMINED behavior.\nConditional delay slot instructions Things get more complicated when the delay-slot instruction is effectively predicated on the branch direction. SPARC supports \u0026ldquo;annulled\u0026rdquo; branches in which the delay-slot instruction is not executed if the branch is not taken. (Actually for unconditional branches the annul bit means never execute the delay slot instruction, which is kind of the opposite of what it means for conditional branches.)\nApparently MIPS is even more complex, with bits that allow the delay-slot instruction to be predicated in either direction (execute only if taken or only if not taken).(Korey, can you fill in some specifics here?)\n-\nI dont think MIPS allows the delay-slot instructions to be predicated. (I may have miscommunicated a bit on my original post) MIPS actually has separate opcodes to distinguish between branches where the delay-slot is always executed and instructions where the delay-slot is conditionally instructed. Additionally, I gather that there is no difference between how MIPS and SPARC treats these delay-slot instructions. The only difference is between the annulled-bit and the opcode to specify which \u0026quot;handling\u0026quot; is required. Luckily, the decoder/ISA scheme makes the annulled-bit/opcode difference a non-issue. --[Ksewell](User:Ksewell \u0026quot;wikilink\u0026quot;) 18:37, 16 February 2006 (EST)  -\n - By predicated I just meant conditionally executed. The only other possibility that MIPS might have that SPARC doesn't is a flavor of branch that only executes the delay slot instruction if the branch is \\*not\\* taken (as opposed to only if the branch \\*is\\* taken). --[Stever](User:Stever \u0026quot;wikilink\u0026quot;) 19:52, 16 February 2006 (EST)  The \u0026ldquo;formal semantics\u0026rdquo; of SPARC delayed branches are shown in Table 13 of Section 6.3.4 of the SPARC v9 reference manual (p. 100 in [http://www.sparc.com/standards/SPARCV9.pdf| this pdf]). Basically there are four possibilities, plus two special cases for returning from traps. Following the SPARC conventions, \u0026ldquo;B\u0026rdquo; is an unconditional branch while \u0026ldquo;Bcc\u0026rdquo; is a conditional branch, and \u0026ldquo;Tcc\u0026rdquo; is a conditional trap. (Yes, SPARC has a branch that\u0026rsquo;s unconditionally not taken\u0026hellip; don\u0026rsquo;t ask why.)\n   SPARC Instructions NPC NNPC     Non-control-transfer instructions, non-taken non-annulled B \u0026amp; Bcc, non-taken Tcc NPC NPC + 4   Taken Bcc, taken non-annulled B, CALL, RETURN, JMP NPC EA   Non-taken annulled B \u0026amp; Bcc NPC + 4 NPC + 8   Taken annulled B, taken Tcc EA EA + 4   DONE TNPC[TL] TNPC[TL] + 4   RETRY TPC[TL] TNPC[TL]    -DONE and RETRY are two flavors of \u0026ldquo;return from exception\u0026rdquo; in SPARC. \u0026ndash;Stever 19:52, 16 February 2006 (EST)\n   MIPS Instructions NPC NNPC     Non-control-transfer instructions, non-taken Branches (B) \u0026amp; Branch-Likely(Bl), non-taken Trap NPC NPC + 4   Taken Bl, taken B, CALL, RETURN, JMP NPC EA   Non-taken B \u0026amp; Bl NPC + 4 NPC + 8   Taken B, Bl, taken Trap EA EA + 4   ERET, DERET TNPC[TL] TNPC[TL] + 4    -ERET and DERET are the MIPS return from exceptions. DERET is return from debug exception.\u0026ndash;Ksewell 16:32, 17 February 2006 (EST)\nIt seems like the easiest way to implement this is to expose NPC and NNPC to the ISA definition and let each instruction set either or both of these explicitly as necessary (with the default behavior being no change to NPC and NNPC = NPC + 4). Note that this model should work just fine for FastCPU and SimpleCPU where each instruction executes its semantic definition atomically (no pipelining), without any additional state beyond NNPC.\nI think the best way to handle this for pipelined (incl. out-of-order) execution is to treat it as an extension of branch prediction. Right now we predict NPC and catch mispredictions by comparing the predicted \u0026amp; actual NPC values. For SPARC and MIPS we\u0026rsquo;ll have to predict both NPC and NNPC and catch mispredictions by comparing the predicted \u0026amp; actual values of both of them as well. (It appears that comparing the predicted \u0026amp; actual NNPC values might be enough, since looking at the table it\u0026rsquo;s hard to see a case where you could get NNPC right but NPC wrong. But I wouldn\u0026rsquo;t count on that.) The prediction mechanism really just needs one additional bit to distinguish among the first four cases above as opposed to the current two cases (NPC = PC+4 vs. NPC = EA).\n-\nI believe it's impossible to get the NNPC right and NPC wrong since when the CPU model sees a branch it makes the prediction for that branch in the fetch stage. If that branch is taken then the other instructions aren't even allowed into the pipeline. --[Ksewell](User:Ksewell \u0026quot;wikilink\u0026quot;) 18:37, 16 February 2006 (EST)  -\n - I'm more worried about a situation where there's a branch in a branch delay slot or a return from exception into a branch delay slot or something like that... things that MIPS generally doesn't have to worry about but SPARC does. Just to be safe we should always compare both. --[Stever](User:Stever \u0026quot;wikilink\u0026quot;) 19:52, 16 February 2006 (EST)  Other ideas 1. Keep some type of \u0026ldquo;condition-code\u0026rdquo;-type register(s) in the miscRegs file.\n2. For branch delay slot instructions, we can add an extra source register to them. The source register would simply be the destination register of the preceding branch.\nI think #2 would work and the extra source register could be added in the rename stage\u0026hellip;\n-\nThe catch here is that just looking at the target may not be sufficient, depending on how you do it. For example if you had this:   0x1000: beq 0x1008  0x1004: add  0x1008: sub\n-\nthen the 'add' could look at the branch target (NPC) but it would be 0x1008 regardless of whether the branch was taken or not taken.  The only drawback is that it wouldnt necessarily allow a branch delay slot instruction to execute the same cycle as it\u0026rsquo;s branch and that kind of defeats the purpose of having delay slot instructions.\n-\nJust to clarify: delay slots really make sense only for single-issue in-order pipelines without BTBs, where the delay slot instruction executes in the cycle \\*after\\* the branch, hiding the branch-taken bubble that comes from not having a BTB. In this situation the squashing delayed branches are not that hard to implement, as the branch always resolves before the delay slot instruction hits writeback, so you can execute it speculatively and just not write back its result. They were never intended to be executed in the same cycle as the branch.  CPU independent implementation Should code for these be implemented in the CPU with #define MIPS or #define SPARC \u0026hellip; Or should there be architecture-specific files and/or functions for this branch-delay code?\n-\nIt seems like the main question is do we need an NNPC or not. We could probably cover most cases by putting in an NNPC but not using it when we don't need it (e.g. in Alpha), though this could be confusing. Unless we do extra work to maintain NNPC in Alpha though there will be a difference when it comes to doing branch predictions in pipelined CPU models (see above). If we can generalize both MIPS and SPARC to a unified model, then we could have a single flag that enables or disables that model and set it in MIPS and SPARC and leave it cleared in Alpha.  -\nWould it be possible for the PC to be considered a control register which lives in the ISA? There could be special getPC/stepPC/whatever functions to hook into for the CPU, and NPC and NNPC could be maintained totally internally to the ISA and wouldn't exist if they aren't needed. I'm not sure how this complicates things for the CPU model, though. --[Gblack](User:Gblack \u0026quot;wikilink\u0026quot;) 13:26, 16 February 2006 (EST)  -\nIt's not entirely clear what would be less complex. Regardless of whether or not the PC is a control register, there will still have to be differences between the branch prediction in a pipelined model. One problem with the PC and NPC and NNPC living totally in the ISA is with dynamic instructions. Currently they hold a variety of information that is necessary to keep track of with the instruction, such as PC and NextPC. If you were to push the PC/NPC/NNPC to the ISA level, then you'd also have to define within the ISA some sort of struct that is all the ISA state information that needs to be carried along with a dynamic instruction. DynInst already has quite a bit of indirection to it, so I'm a bit hesitant to add another level of indirection. It may also be difficult to get the interface correct; for certain registers (such as the PC) you'd have to call xc-\\\u0026gt;getISAState()-\\\u0026gt;setPC(pc), while other registers would just be xc-\\\u0026gt;setIntReg(5, 1000). It'd be nice to keep them uniform so that the programmer doesn't have to keep checking if the register is in the ISA or can be accessed normally. On the other hand, this ISA-dependent state struct may be useful to store, for example, updates to IPRs which won't happen until commit time. I'm really not sure at the moment...it could be useful, or it could just be more complication. --[Ktlim](User:Ktlim \u0026quot;wikilink\u0026quot;) 15:44, 16 February 2006 (EST)  -\nI'm fine with RegFile being an ISA-dependent type. As long as it stays that way then each ISA can choose whether to have an NNPC or not. They can all export get/set NNPC functions and the Alpha versions can panic. If we were going to have ten different ISAs with 8 different PC/NPC schemes it might be worth more effort, but if there are just two schemes and no third one on the horizon that I can see it's not worth the effort to get all abstract about it, especially for something so common and potentially performance sensitive. --[Stever](User:Stever \u0026quot;wikilink\u0026quot;) 21:09, 16 February 2006 (EST)  Hazard Clearing Instructions MIPS has hazard clearing instructions which clear instruction and execution hazards (most of the hazards have to do with the special system (or CP0) registers). MIPS execution hazards are defined as \u0026ldquo;those created by the execution of one instruction, and seen by the execution of another instruction\u0026rdquo; and instruction hazards as \u0026ldquo; those created by the execution of one instruction, and seen by the instruction fetch of another instruction\u0026rdquo;. A table of the hazard clearing instructions is presented below and a list of all the hazard cases (e.g. disable-interrupt (producer) to interrupt-instruction(consumer)) can be found in 7.0 of vol.III of the MIPS manual.\nThe reason I mention this on this WIKI page is because the jalr_hb and jr_hb instructions require the hazard clearing to take place after the branch delay-slot gets executed. I was thinking to do one of two things:\n1. Schedule a CPU event for cycle N+2 (where N is the cycle of the jump instruction executing). However, this solution quickly breaks down if the branch delay-slot instruction takes more than one cycle.\n2. Create a flag in the CPU for a pending hazard clearing.\nWith respect to the Simple CPU model, I am not sure if a complicated solution is needed in the first place so I would be up for implementing #2. Currently, I have these hazard clearing instructions call a function called \u0026ldquo;clear_exe_inst_hazards()\u0026ldquo;. This function can be configured later depending on our design decision.\nThoughts???\n"
},
{
	"uri": "http://localhost/ruby/cache-coherence-protocols/",
	"title": "Cache Coherence Protocols",
	"tags": [],
	"description": "",
	"content": " Common Notations and Data Structures Coherence Messages These are described in the \u0026lt;protocol-name\u0026gt;-msg.sm file for each protocol.\n   Message Description     ACK/NACK positive/negative acknowledgement for requests that wait for the direction of resolution before deciding on the next action. Examples are writeback requests, exclusive requests.   GETS request for shared permissions to satisfy a CPU\u0026rsquo;s load or IFetch.   GETX request for exclusive access.   INV invalidation request. This can be triggered by the coherence protocol itself, or by the next cache level/directory to enforce inclusion or to trigger a writeback for a DMA access so that the latest copy of data is obtained.   PUTX request for writeback of cache block. Some protocols (e.g. MOESI_CMP_directory) may use this only for writeback requests of exclusive data.   PUTS request for writeback of cache block in shared state.   PUTO request for writeback of cache block in owned state.   PUTO_Sharers request for writeback of cache block in owned state but other sharers of the block exist.   UNBLOCK message to unblock next cache level/directory for blocking protocols.    AccessPermissions These are associated with each cache block and determine what operations are permitted on that block. It is closely correlated with coherence protocol states.\n   Permissions Description     Invalid The cache block is invalid. The block must first be obtained (from elsewhere in the memory hierarchy) before loads/stores can be performed. No action on invalidates (except maybe sending an ACK). No action on replacements. The associated coherence protocol states are I or NP and are stable states in every protocol.   Busy TODO   Read_Only Only operations permitted are loads, writebacks, invalidates. Stores cannot be performed before transitioning to some other state.   Read_Write Loads, stores, writebacks, invalidations are allowed. Usually indicates that the block is dirty.    Data Structures  Message Buffers:TODO TBE Table: TODO Timer Table: This maintains a map of address-based timers. For each target address, a timeout value can be associated and added to the Timer table. This data structure is used, for example, by the L1 cache controller implementation of the MOESI_CMP_directory protocol to trigger separate timeouts for cache blocks. Internally, the Timer Table uses the event queue to schedule the timeouts. The TimerTable supports a polling-based interface, isReady() to check if a timeout has occurred. Timeouts on addresses can be set using the set() method and removed using the unset() method.  -\n - **Related Files**: - - src/mem/ruby/system/TimerTable.hh: Declares the TimerTable class src/mem/ruby/system/TimerTable.cc: Implementation of the methods of the TimerTable class, that deals with setting addresses \u0026amp; timeouts, scheduling events using the event queue.  Coherence controller FSM Diagrams  The Finite State Machines show only the stable states Transitions are annotated using the notation \u0026ldquo;Event list\u0026rdquo; or \u0026ldquo;Event list : Action list\u0026rdquo; or \u0026ldquo;Event list : Action list : Event list\u0026rdquo;. For example, Store : GETX indicates that on a Store event, a GETX message was sent whereas GETX : Mem Read indicates that on receiving a GETX message, a memory read request was sent. Only the main triggers and actions are listed. Optional actions (e.g. writebacks depending on whether or not the block is dirty) are enclosed within [ ] In the diagrams, the transition labels are associated with the arc that cuts across the transition label or the closest arc.  "
},
{
	"uri": "http://localhost/docs/cache-replacement-policies/",
	"title": "Cache Replacement Policies",
	"tags": [],
	"description": "",
	"content": " gem5 has multiple implemented replacement policies. Each one uses its specific replacement data to determine a replacement victim on evictions.\nAll of the replacement policies prioritize victimizing invalid blocks.\nAltough most of the replacement policies don\u0026rsquo;t need a counter of the number of references to an entry, or the insertion timestamp, they are by default updated by every replacement policy, due to their usage in the statistics.\nRandom The simplest replacement policy; it does not need replacement data, as it randomly selects a victim among the candidates.\nLeast Recently Used (LRU) Its replacement data consists of a last touch timestamp, and the victim is chosen based on it: the oldest it is, the more likely its respective entry is to be victimized.\nBimodal Insertion Policy (BIP) The Bimodal Insertion Policy is similar to the LRU, however, blocks have a probability of being inserted as the MRU, according to a bimodal throttle parameter (btp). The highest btp is, the highest is the likelihood of a new block being inserted as MRU.\nLRU Insertion Policy (LIP) The LRU Insertion Policy consists of a LRU replacement policy that instead of inserting blocks with the most recent last touch timestamp, it inserts them as the LRU entry. On subsequent touches to the block, its timestamp is updated to be the MRU, as in LRU. It can also be seen as a BIP where the likelihood of inserting a new block as the most recently used is 0%.\nMost Recently Used (MRU) The Most Recently Used policy chooses replacement victims by their recency, however, as opposed to LRU, the newest the entry is, the more likely it is to be victimized.\nLeast Frequently Used (LFU) The victim is chosen using the reference frequency. The least referenced entry is chosen to be evicted, regardless of the amount of times it has been touched, or how long has passed since its last touch.\nFirst-In, First-Out (FIFO) The victim is chosen using the insertion timestamp. If no invalid entries exist, the oldest one is victimized, regardless of the amount of times it has been touched.\nSecond-Chance The Second-Chance replacement policy is similar to FIFO, however entries are given a second chance before being victimized, being re-inserted at the end of the FIFO if its second chance bit is set. When an entry is re-inserted, its second chance bit is cleared, and the entry is threated as in the FIFO policy (unless it is touched again, in which case its second chance bit is set again).\nNot Recently Used (NRU) Not Recently Used (NRU) is an approximation of LRU that uses a single bit to determine if a block is going to be re-referenced in the near or distant future. If the bit is 1, it is likely to not be referenced soon, so it is chosen as the replacement victim. When a block is victimized, all its co-replacement candidates have their re-reference bit incremented.\nRe-Reference Interval Prediction (RRIP) Re-Reference Interval Prediction (RRIP) is an extension of NRU that uses a re-reference prediction value to determine if blocks are going to be re-used in the near future or not. The higher the value of the RRPV, the more distant the block is from its next access. From the original paper, this implementation of RRIP is also called Static RRIP (SRRIP), as it always inserts blocks with the same RRPV.\nBimodal Re-Reference Interval Prediction (BRRIP) Bimodal Re-Reference Interval Prediction (BRRIP) is an extension of RRIP that has a probability of not inserting blocks as the LRU, as in the Bimodal Insertion Policy. This probability is controlled by the bimodal throtle parameter (btp).\n"
},
{
	"uri": "http://localhost/developer/checker-cpu/",
	"title": "Checker CPU",
	"tags": [],
	"description": "",
	"content": " The Checker allows for dynamic verification of any CPUs that use DynInst. Currently it does not support SMT or MP systems. Extending the Checker for those systems should not be difficult, but makes it impossible to verify memory value of loads.\nWhat it verifies The Checker can verify:\n Correct instruction path Correct fetched instruction Correct execution results Correct PC redirection due to branches, faults, or PC events  What it can\u0026rsquo;t verify The Checker cannot verify:\n Correct interrupt handling Store values being correct in memory Certain instructions, such as RPCC (reads cycle counter), RC/RS (reads interrupt flags)  The Checker does not have access to the main CPU\u0026rsquo;s interrupt signals, so it can\u0026rsquo;t ensure that the interrupts are detected and handled correctly.\nStores, although they commit and are issued to memory in program order, may actually complete out of order. This is especially true with store conditionals, which don\u0026rsquo;t actually complete until the store conditional result is written back. Because instructions are verified in program order and only after they complete, the value in memory may not reflect the store being verified but may instead be due to a younger (but also committed) store.\nInstructions such as RPCC will have different results by the time the instruction has completed versus when it executed. Thus these instructions must be marked as IsUnverifiable in the decoder.isa file.\nHow it works The Checker works by keeping a separate SimpleThread, which it updates with its own results of instruction fetching and execution. Once the main CPU completes an instruction, it sends that instruction to the Checker. The Checker verifies these instructions in program order. It ensures that each instruction matches the instruction that the Checker fetches. It does the same with the execution of the instruction, as well as any faults that the instruction generates. In the cases of loads it checks memory to verify the load\u0026rsquo;s value matches what is in memory. This verification can not be done on anything other than UP systems because memory\u0026rsquo;s value may have changed between the time the load executes and the time the load completes.\nThe Checker defines a ThreadContext, called CheckerThreadContext, which serves as a wrapper for the main CPU\u0026rsquo;s ThreadContext and its own SimpleThread. It takes any calls made to CheckerThreadContext and forwards them to either: both the Checker and the main CPU (in the case of setting registers); or just the main CPU alone (in the case of reading registers). This allows the Checker to be able to keep its state updated properly even when the main CPU\u0026rsquo;s state is updated externally (e.g. through a fault). It also allows CheckerThreadContext to be used transparently in place of the main CPU\u0026rsquo;s ThreadContext because it will return the main CPU\u0026rsquo;s state properly.\nHow to use For a CPU that uses DynInst, the following steps must be taken to be able to use the Checker:\n Add the Checker to its SimObject params. Setup the Icache and Dcache ports of the Checker when they are created at Fetch and in the LSQ. Store the result of any destination register write in the result variable inside DynInst. Use the Checker\u0026rsquo;s ThreadContext in place of its normal ThreadContext when the checker is enabled. Add calls to tell the Checker when instructions have completed. Usually this is when the instruction is committed. However stores do not actually complete until their data is written back. If the CPU modifies its own state anywhere inside its code (and not through its ThreadContext), be sure to call the right functions to keep the Checker up to date.  See the Detailed CPU for an example of using the Checker.\n"
},
{
	"uri": "http://localhost/docs/checkpoints/",
	"title": "Checkpoints",
	"tags": [],
	"description": "",
	"content": " Checkpoints Checkpoints are essentially snapshops of a simulation. You would want to use a checkpoint when your simulation takes an extremely long time (which is almost always the case) so you can resume from that checkpoint at a later time with the DerivO3CPU.\nCreation First of all, you need to create a checkpoint. Each checkpoint as saved in a new directory named \u0026lsquo;cpt.TICKNUMBER\u0026rsquo;, where TICKNUMBER refers to the tick value at which this checkpoint was created. There are several ways in which a checkpoint can be created \u0026ndash;\n After booting the gem5 simulator, execute the command m5 checkpoint. One can execute the command manually using m5term, or include it in a run script to do this automatically after the Linux kernel has booted up. There is a pseudo instruction that can be used for creating checkpoints. For example, one may include this pseduo instruction in an application program, so that the checkpoint is created when the application has reached a certain state. The option \u0026ndash;take-checkpoints can be provided to the python scripts (fs.py, ruby_fs.py) so that checkpoints are dumped periodically. The option \u0026ndash;checkpoint-at-end can be used for creating the checkpoint at the end of the simulation. Take a look at the file configs/common/Options.py for these options.  While creating checkpoints with Ruby memory model, it is necessary to use the MOESI hammer protocol. This is because checkpointing the correct memory state requires that the caches are flushed to the memory. This flushing operation is currently supported only with the MOESI hammer protocol.\nRestoring Restoring from a checkpoint can usually be easily done from the command line, e.g.:\nbuild/ALPHA/gem5.debug configs/example/fs.py -r N OR build/ALPHA/gem5.debug configs/example/fs.py --checkpoint-restore=N  The number N is integer that represents checkpoint number which usually starts from 1 then increases incrementally to 2,3,4\u0026hellip;\nBy default, gem5 assumes that the checkpoint is to be restored using Atomic CPUs. This may not work if the checkpoint was recorded using Timing / Detailed / Inorder CPU. One can mention the option \u0026ndash;restore-with-cpu  on the command line. The cpu type supplied with this option is then used for restoring from the checkpoint.\nDetailed example: Parsec In the following section we would describe how checkpoints are created for workloads PARSEC benchmark suite. However similar procedure can be followed to create checkpoint for other workloads beyond PARSEC suite. Following are the high level steps of creating checkpoint:\n Annotate each workload with start and end of Region of Interest and with start and end of work units in the program Take a checkpoint at the start of the Region of Interest Simulate the whole program in the Region of Interest and periodically take checkpoints Analyse the statistics corresponding to periodic checkpoints and select the most interesting section of the program execution Take warm up cache trace for Ruby before reaching most interesting portion of the program and take the final checkpoint.  In each of the following sections we explain each of the above steps in more details.\nAnnotating workloads Annotation is required for two purposes \u0026mdash; for defining region of program beyond the initialization section of a program and for defining logical units of work in each of the workloads.\nWorkloads in PARSEC benchmark suite, already has annotating demarcating start and end of portion of program without program initialization section and program finalization section. We just use gem5 specific annotation for start of Region of Interest. The start of the Region of Interest (ROI) is marked by m5_roi_begin() and the end of ROI is demarcated by m5_roi_end().\nDue to large simulation time its not always possible to simulate whole program. Moreover, unlike single threaded programs, simulating for a given number instructions in multi-threaded workloads is not a correct way to simulate portion of a program due to possible presence of instructions spinning on synchronization variable. Thus it is important define semantically meaningful logical units of work in each workload. Simulating for a given number of workuints in a multi-threaded workloads gives a reasonable way of simulating portion of workloads as the problem of instructions spinning on synchronization variables\nSwitchover/Fastforwarding Sampling Sampling (switching between functional and detailed models) can be implemented via your Python script. In your script you can direct the simulator to switch between two sets of CPUs. To do this, in your script setup a list of tuples of (oldCPU, newCPU). If there are multiple CPUs you wish to switch simultaneously, they can all be added to that list. For example:\nrun_cpu1 = SimpleCPU() switch_cpu1 = DetailedCPU(switched_out=True) run_cpu2 = SimpleCPU() switch_cpu2 = FooCPU(switched_out=True) switch_cpu_list = [(run_cpu1,switch_cpu1),(run_cpu2,switch_cpu2)]  Note that the CPU that does not immediately run should have the parameter \u0026ldquo;switched_out=True\u0026rdquo;. This keeps those CPUs from adding themselves to the list of CPUs to run; they will instead get added when you switch them in.\nIn order for gem5 to instantiate all of your CPUs, you must make the CPUs that will be switched in a child of something that is in the configuration hierarchy. Unfortunately at the moment some configuration limitations force the switch CPU to be placed outside of the System object. The Root object is the next most convenient place to place the CPU, as shown below:\nroot1 = Root() root1.system = System(cpu = run_cpu1) root1.switch_cpu = switch_cpu1 root2 = Root() root2.system = System(cpu = run_cpu2) root2.switch_cpu = switch_cpu2  This will add the swtich CPUs as children of each root object. Note that switch_cpu is not an actual parameter for Root, but is just an assignment to indicate that it has a child, switch_cpu.\nAfter the systems and the CPU list is setup, your script can direct gem5 to switch the CPUs at the appropriate cycle. This is achieved by calling switchCpus(cpus_list). For example, assuming the code above, and a system that is setup running run_cpu1 and run_cpu2 initially:\nm5.simulate(500) # simulate for 500 cycles m5.switchCpus(switch_cpu_list) m5.simulate(500) # simulate another 500 cycles after switching  Note that gem5 may have to simulate for a few cycles prior to switching CPUs due to any outstanding state that may be present in the CPUs being switched out.\n"
},
{
	"uri": "http://localhost/developer/classic-memory-system/",
	"title": "Classic Memory System",
	"tags": [],
	"description": "",
	"content": " MemObjects Caches The default cache is a non-blocking cache with MSHR (miss status holding register) and WB (Write Buffer) for read and write misses. The Cache can also be enabled with prefetch (typically in the last level of cache). The default replacement policy for the cache lines is LRU (least recently used).\nHooking them up test\nParameters Interconnects Crossbars The two types of traffic in the crossbar are memory-mapped packets and snooping packets. The memory-mapped requests go down the memory hierarchy, and responses go up the memory hierarchy (same route back). The snooping requests go horizontally and up the cache hierarchy, snooping responses go horizontally and down the hierarchy (same route back). Normal snoops go horizontally and express snoops go up the cache hierarchy.\nBridges Anything else? Coherence M5 2.0b4 introduced a substantially rewritten and streamlined cache model, including a new coherence protocol. (The old pre-2.0 cache model had been patched up to work with the new Memory System introduced in 2.0beta, but not rewritten to take advantage of the new memory system\u0026rsquo;s features.)\nThe key feature of the new coherence protocol is that it is designed to work with more-or-less arbitrary cache hierarchies (multiple caches each on multiple levels). In contrast, the old protocol restricted sharing to a single bus.\nIn the real world, a system architecture will have limits on the number or configuration of caches that the protocol can be designed to accommodate. It\u0026rsquo;s not practical to design a protocol that\u0026rsquo;s fully realistic and yet efficient for arbitrary configurations. In order to enable our protocol to work on (nearly) arbitrary configurations, we currently sacrifice a little bit of realism and a little bit of configurability. Our intent is that this protocol is adequate for researchers studying aspects of system behavior other than coherence mechanisms. Researchers studying coherence specifically will probably want to replace the default coherence mechanism with implementations of the specific protocols under investigation.\nThe protocol is a MOESI snooping protocol. Inclusion is not enforced; in a CMP configuration where you have several L1s whose total capacity is a significant fraction of the capacity of the common L2 they share, inclusion can be very inefficient.\nRequests from upper-level caches (those closer to the CPUs) propagate toward memory in the expected fashion: an L1 miss is broadcast on the local L1/L2 bus, where it is snooped by the other L1s on that bus and (if none respond) serviced by the L2. If the request misses in the L2, then after some delay (currently set equal to the L2 hit latency), the L2 will issue the request on its memory-side bus, where it will possibly be snooped by other L2s and then be issued to an L3 or memory.\nUnfortunately, propagating snoop requests incrementally back up the hierarchy in a similar fashion is a source of myriad nearly intractable race conditions. Real systems don\u0026rsquo;t typically do this anyway; in general you want a single snoop operation at the L2 bus to tell you the state of the block in the whole L1/L2 hierarchy. There are a handful of methods for this:\n just snoop the L2, but enforce inclusion so that the L2 has all the info you need about the L1s as well\u0026mdash;an idea we\u0026rsquo;ve already rejected above keep an extra set of tags for all the L1s at the L2 so those can be snooped at the same time (see the Compaq Piranha)\u0026mdash;reasonable, if you\u0026rsquo;re hierarchy\u0026rsquo;s not too deep, but now you\u0026rsquo;ve got to size the tags in the lower-level caches based on the number, size, and configuration of the upper-level caches, which is a configuration pain snoop the L1s in parallel with the L2, something that\u0026rsquo;s not hard if they\u0026rsquo;re all on the same die (I believe Intel started doing this with the Pentium Pro; not sure if they still do with the Core2 chips or not, or if AMD does this as well, but I suspect so)\u0026mdash;also reasonable, but adding explicit paths for these snoops would also make for a very cumbersome configuration process  We solve this dilemma by introducing \u0026ldquo;express snoops\u0026rdquo;, which are special snoop requests that get propagated up the hierarchy instantaneously and atomically (much like the atomic-mode accesses described on the Memory System page), even when the system is running in timing mode. Functionally this behaves very much like options 2 or 3 above, but because the snoops propagate along the regular bus interconnects, there\u0026rsquo;s no additional configuration overhead. There is some timing inaccuracy introduced, but if we assume that there are dedicated paths in the real hardware for these snoops (or for maintaining the additional copies of the upper-level tags at the lower-level caches) then the differences are probably minor.\n(More to come: how does a cache know when its request is completed? and other fascinating questions\u0026hellip;)\nNote: there are still some bugs in this protocol as of 2.0b4, particularly if you have multiple L2s each with multiple L1s behind it, but I believe it works for any configuration that worked in 2.0b3.\nDebugging There is a feature in the classic memory system for displaying the coherence state of a particular block from within the debugger (e.g., gdb). This feature is built on the classic memory system\u0026rsquo;s support for functional accesses. (Note that this feature is currently rarely used and may have bugs.)\nIf you inject a functional request with the command set to PrintReq, the packet traverses the memory system (like a regular functional request) but on any object that matches (other queued packet, cache block, etc.) it simply prints out some information about that object.\nThere\u0026rsquo;s a helper method on Port called printAddr() that takes an address and builds an appropriate PrintReq packet and injects it. Since it propagates using the same mechanism as a normal functional request, it needs to be injected from a port where it will propagate through the whole memory system, such as at a CPU. There are helper printAddr() methods on MemTest, AtomicSimpleCPU, and TimingSimpleCPU objects that simply call printAddr() on their respective cache ports. (Caveat: the latter two are untested.)\nPutting it all together, you can do this:\n(gdb) set print object (gdb) call SimObject::find(\u0026quot; system.physmem.cache0.cache0.cpu\u0026quot;) $4 = (MemTest *) 0xf1ac60 (gdb) p (MemTest*)$4 $5 = (MemTest *) 0xf1ac60 (gdb) call $5-\u0026gt;printAddr(0x107f40) system.physmem.cache0.cache0 MSHRs [107f40:107f7f] Fill state: Targets: cpu: [107f40:107f40] ReadReq system.physmem.cache1.cache1 blk VEM system.physmem 0xd0  \u0026hellip; which says that cache0.cache0 has an MSHR allocated for that address to serve a target ReadReq from the CPU, but it\u0026rsquo;s not in service yet (else it would be marked as such); the block is valid, exclusive, and modified in cache1.cache1, and the byte has a value of 0xd0 in physical memory.\nObviously it\u0026rsquo;s not necessarily all the info you\u0026rsquo;d want, but it\u0026rsquo;s pretty useful. Feel free to extend. There\u0026rsquo;s also a verbosity parameter that\u0026rsquo;s currently not used that could be exploited to have different levels of output.\nNote that the extra \u0026ldquo;p (MemTest*)$4\u0026rdquo; is needed since although \u0026ldquo;set print object\u0026rdquo; displays the derived type, internally gdb still considers the pointer to be of the base type, so if you try and call printAddr directly on the $4 pointer you get this:\n(gdb) call $4-\u0026gt;printAddr(0x400000) Couldn't find method SimObject::printAddr  "
},
{
	"uri": "http://localhost/docs/classic-mem-system/",
	"title": "Classic memory system",
	"tags": [],
	"description": "",
	"content": " M5\u0026rsquo;s new memory system (introduced in the first 2.0 beta release) was designed with the following goals:\n Unify timing and functional accesses in timing mode. With the old memory system the timing accesses did not have data and just accounted for the time it would take to do an operation. Then a separate functional access actually made the operation visible to the system. This method was confusing, it allowed simulated components to accidentally cheat, and prevented the memory system from returning timing-dependent values, which isn\u0026rsquo;t reasonable for an execute-in-execute CPU model. Simplify the memory system code \u0026ndash; remove the huge amount of templating and duplicate code. Make changes easier, specifically to allow other memory interconnects besides a shared bus.  For details on the new coherence protocol, introduced (along with a substantial cache model rewrite) in 2.0b4, see Coherence Protocol.\nMemObjects All objects that connect to the memory system inherit from MemObject. This class adds the pure virtual functions getMasterPort(const std::string \u0026amp;name, PortID idx) and getSlavePort(const std::string \u0026amp;name, PortID idx) which returns a port corresponding to the given name and index. This interface is used to structurally connect the MemObjects together.\nPorts The next large part of the memory system is the idea of ports. Ports are used to interface memory objects to each other. They will always come in pairs, with a MasterPort and a SlavePort, and we refer to the other port object as the peer. These are used to make the design more modular. With ports a specific interface between every type of object doesn\u0026rsquo;t have to be created. Every memory object has to have at least one port to be useful. A master module, such as a CPU, has one or more MasterPort instances. A slave module, such as a memory controller, has one or more SlavePorts. An interconnect component, such as a cache, bridge or bus, has both MasterPort and SlavePort instances.\nThere are two groups of functions in the port object. The send* functions are called on the port by the object that owns that port. For example to send a packet in the memory system a CPU would call myPort-\u0026gt;sendTimingReq(pkt) to send a packet. Each send function has a corresponding recv function that is called on the ports peer. So the implementation of the sendTimingReq() call above would simply be peer-\u0026gt;recvTimingReq(pkt) on the slave port. Using this method we only have one virtual function call penalty but keep generic ports that can connect together any memory system objects.\nMaster ports can send requests and receive responses, whereas slave ports receive requests and send responses. Due to the coherence protocol, a slave port can also send snoop requests and receive snoop responses, with the master port having the mirrored interface.\nConnections In Python, Ports are first-class attributes of simulation objects, much like Params. Two objects can specify that their ports should be connected using the assignment operator. Unlike a normal variable or parameter assignment, port connections are symmetric: A.port1 = B.port2 has the same meaning as B.port2 = A.port1. The notion of master and slave ports exists in the Python objects as well, and a check is done when the ports are connected together.\nObjects such as busses that have a potentially unlimited number of ports use \u0026ldquo;vector ports\u0026rdquo;. An assignment to a vector port appends the peer to a list of connections rather than overwriting a previous connection.\nIn C++, memory ports are connected together by the python code after all objects are instantiated.\nRequest A request object encapsulates the original request issued by a CPU or I/O device. The parameters of this request are persistent throughout the transaction, so a request object\u0026rsquo;s fields are intended to be written at most once for a given request. There are a handful of constructors and update methods that allow subsets of the object\u0026rsquo;s fields to be written at different times (or not at all). Read access to all request fields is provided via accessor methods which verify that the data in the field being read is valid.\nThe fields in the request object are typically not available to devices in a real system, so they should normally be used only for statistics or debugging and not as architectural values.\nRequest object fields include:\n Virtual address. This field may be invalid if the request was issued directly on a physical address (e.g., by a DMA I/O device). Physical address. Data size. Time the request was created. The ID of the CPU/thread that caused this request. May be invalid if the request was not issued by a CPU (e.g., a device access or a cache writeback). The PC that caused this request. Also may be invalid if the request was not issued by a CPU.  Packet A Packet is used to encapsulate a transfer between two objects in the memory system (e.g., the L1 and L2 cache). This is in contrast to a Request where a single Request travels all the way from the requester to the ultimate destination and back, possibly being conveyed by several different Packets along the way.\nRead access to many packet fields is provided via accessor methods which verify that the data in the field being read is valid.\nA packet contains the following all of which are accessed by accessors to be certain the data is valid:\n The address. This is the address that will be used to route the packet to its target (if the destination is not explicitly set) and to process the packet at the target. It is typically derived from the request object\u0026rsquo;s physical address, but may be derived from the virtual address in some situations (e.g., for accessing a fully virtual cache before address translation has been performed). It may not be identical to the original request address: for example, on a cache miss, the packet address may be the address of the block to fetch and not the request address. The size. Again, this size may not be the same as that of the original request, as in the cache miss scenario. A pointer to the data being manipulated.  Set by dataStatic(), dataDynamic(), and dataDynamicArray() which control if the data associated with the packet is freed when the packet is, not, with delete, and with delete [] respectively. Allocated if not set by one of the above methods allocate() and the data is freed when the packet is destroyed. (Always safe to call). A pointer can be retrived by calling getPtr() get() and set() can be used to manipulate the data in the packet. The get() method does a guest-to-host endian conversion and the set method does a host-to-guest endian conversion.  A status indicating Success, BadAddress, Not Acknowleged, and Unknown. A list of command attributes associated with the packet  Note: There is some overlap in the data in the status field and the command attributes. This is largely so that a packet an be easily reinitialized when nacked or easily reused with atomic or functional accesses.  A SenderState pointer which is a virtual base opaque structure used to hold state associated with the packet but specific to the sending device (e.g., an MSHR). A pointer to this state is returned in the packet\u0026rsquo;s response so that the sender can quickly look up the state needed to process it. A specific subclass would be derived from this to carry state specific to a particular sending device. A CoherenceState pointer which is a virtual base opaque structure used to hold coherence-related state. A specific subclass would be derived from this to carry state specific to a particular coherence protocol. A pointer to the request.  Access Types There are three types of accesses supported by the ports.\n Timing - Timing accesses are the most detailed access. They reflect our best effort for realistic timing and include the modeling of queuing delay and resource contention. Once a timing request is successfully sent at some point in the future the device that sent the request will either get the response or a NACK if the request could not be completed (more below). Timing and Atomic accesses can not coexist in the memory system. Atomic - Atomic accesses are a faster than detailed access. They are used for fast forwarding and warming up caches and return an approximate time to complete the request without any resource contention or queuing delay. When a atomic access is sent the response is provided when the function returns. Atomic and timing accesses can not coexist in the memory system. Functional - Like atomic accesses functional accesses happen instantaneously, but unlike atomic accesses they can coexist in the memory system with atomic or timing accesses. Functional accesses are used for things such as loading binaries, examining/changing variables in the simulated system, and allowing a remote debugger to be attached to the simulator. The important note is when a functional access is received by a device, if it contains a queue of packets all the packets must be searched for requests or responses that the functional access is effecting and they must be updated as appropriate. The Packet::intersect() and fixPacket() methods can help with this.  Packet allocation protocol The protocol for allocation and deallocation of Packet objects varies depending on the access type. (We\u0026rsquo;re talking about low-level C++ new/delete issues here, not anything related to the coherence protocol.)\n Atomic and Functional : The Packet object is owned by the requester. The responder must overwrite the request packet with the response (typically using the Packet::makeResponse() method). There is no provision for having multiple responders to a single request. Since the response is always generated before sendAtomic() or sendFunctional() returns, the requester can allocate the Packet object statically or on the stack.   Timing : Timing transactions are composed of two one-way messages, a request and a response. In both cases, the Packet object must be dynamically allocated by the sender. Deallocation is the responsibility of the receiver (or, for broadcast coherence packets, the target device, typically memory). In the case where the receiver of a request is generating a response, it may choose to reuse the request packet for its response to save the overhead of calling delete and then new (and gain the convenience of using makeResponse()). However, this optimization is optional, and the requester must not rely on receiving the same Packet object back in response to a request. Note that when the responder is not the target device (as in a cache-to-cache transfer), then the target device will still delete the request packet, and thus the responding cache must allocate a new Packet object for its response. Also, because the target device may delete the request packet immediately on delivery, any other memory device wishing to reference a broadcast packet past point where the packet is delivered must make a copy of that packet, as the pointer to the packet that is delivered cannot be relied upon to stay valid.  Timing Flow control Timing requests simulate a real memory system, so unlike functional and atomic accesses their response is not instantaneous. Because the timing requests are not instantaneous, flow control is needed. When a timing packet is sent via sendTiming() the packet may or may not be accepted, which is signaled by returning true or false. If false is returned the object should not attempt to sent anymore packets until it receives a recvRetry() call. At this time it should again try to call sendTiming(); however the packet may again be rejected. Note: The original packet does not need to be resent, a higher priority packet can be sent instead. Once sendTiming() returns true, the packet may still not be able to make it to its destination. For packets that require a response (i.e. pkt-\u0026gt;needsResponse() is true), any memory object can refuse to acknowledge the packet by changing its result to Nacked and sending it back to its source. However, if it is a response packet, this can not be done. The true/false return is intended to be used for local flow control, while nacking is for global flow control. In both cases a response can not be nacked.\nResponse and Snoop ranges Ranges in the memory system are handled by having devices that are sensitive to an address range provide an implementation for getAddrRanges in their slave port objects. This method returns an AddrRangeList of addresses it responds to. When these ranges change (e.g. from PCI configuration taking place) the device should call sendRangeChange() on its slave port so that the new ranges are propagated to the entire hierarchy. This is precisely what happens during init(); all memory objects call sendRangeChange(), and a flurry of range updates occur until everyones ranges have been propagated to all busses in the system.\nNOTOC\n"
},
{
	"uri": "http://localhost/developer/coding-style/",
	"title": "Coding Sytle",
	"tags": [],
	"description": "",
	"content": " We strive to maintain a consistent coding style in the M5 source code to make the source more readable and maintainable. This necessarily involves compromise among the multiple developers who work on this code. We feel that we have been successful in finding such a compromise, as each of the primary M5 developers is annoyed by at least one of the rules below. We ask that you abide by these guidelines as well if you develop code that you would like to contribute back to M5. An Emacs c++-mode style embodying the indentation rules is available in the source tree at util/emacs/m5-c-style.el.\nIndentation and Line Breaks Indentation will be 4 spaces per level, though namespaces should not increase the indentation.\n Exception: labels followed by colons (case and goto labels and public/private/protected modifiers) are indented two spaces from the enclosing context.  Indentation should use spaces only (no tabs), as tab widths are not always set consistently, and tabs make output harder to read when used with tools such as diff.\nLines must be a maximum of 79 characters long.\nBraces For control blocks (if, while, etc.), opening braces must be on the same line as the control keyword with a space between the closing parenthesis and the opening brace.\n Exception: for multi-line expressions, the opening brace may be placed on a separate line to distinguish the control block from the statements inside the block.  if (...) { ... } // exception case for (...; ...; ...) // brace could be up here { // but this is optionally OK *only* when the 'for' spans multiple lines ... }  \u0026lsquo;Else\u0026rsquo; keywords should follow the closing \u0026lsquo;if\u0026rsquo; brace on the same line, as follows:\nif (...) { ... } else if (...) { ... } else { ... }  Blocks that consist of a single statement that fits on a single line may optionally omit the braces. Braces are still required if the single statement spans multiple lines, or if the block is part of an else/if chain where other blocks have braces.\n// This is OK with or without braces if (a \u0026gt; 0) --a; // In the following cases, braces are still required if (a \u0026gt; 0) { obnoxiously_named_function_with_lots_of_args(verbose_arg1, verbose_arg2, verbose_arg3); } if (a \u0026gt; 0) { --a; } else { underflow = true; warn(\u0026quot;underflow on a\u0026quot;); }  For function definitions or class declarations, the opening brace must be in the first column of the following line.\nIn function definitions, the return type should be on one line, followed by the function name, left-justified, on the next line. As mentioned above, the opening brace should also be on a separate line following the function name.\nSee examples below:\nint exampleFunc(...) { ... } class ExampleClass { public: ... };  Functions should be preceded by a block comment describing the function.\nInline function declarations longer than one line should not be placed inside class declarations. Most functions longer than one line should not be inline anyway.\nSpacing There should be:\n one space between keywords (if, for, while, etc.) and opening parentheses one space around binary operators (+, -, \u0026lt;, \u0026gt;, etc.) including assignment operators (=, +=, etc.) no space around \u0026lsquo;=\u0026rsquo; when used in parameter/argument lists, either to bind default parameter values (in Python or C++) or to bind keyword arguments (in Python) no space between function names and opening parentheses for arguments no space immediately inside parentheses, except for very complex expressions. Complex expressions are preferentially broken into multiple simpler expressions using temporary variables.  For pointer and reference argument declarations, either of the following are acceptable:\nFooType *fooPtr; FooType \u0026amp;fooRef;  or\nFooType* fooPtr; FooType\u0026amp; fooRef;  However, style should be kept consistent within a file. If you are editing an existing file, please keep consistent with the existing code. If you are writing new code in a new file, feel free to choose the style of your preference.\nNaming Class and type names are mixed case, start with an uppercase letter, and do not contain underscores (e.g., ClassName). Exception: names that are acronyms should be all upper case (e.g., CPU). Class member names (method and variables, including const variables) are mixed case, start with a lowercase letter, and do not contain underscores (e.g., aMemberVariable). Class members that have accessor methods should have a leading underscore to indicate that the user should be using an accessor. The accessor functions themselves should have the same name as the variable without the leading underscore.\nLocal variables are lower case, with underscores separating words (e.g., local_variable). Function parameters should use underscores and be lower case.\nC preprocessor symbols (constants and macros) should be all caps with underscores. However, these are deprecated, and should be replaced with const variables and inline functions, respectively, wherever possible.\nclass FooBarCPU { private: static const int minLegalFoo = 100; // consts are formatted just like other vars int _fooVariable; // starts with '_' because it has public accessor functions int barVariable; // no '_' since it's internal use only public: // short inline methods can go all on one line int fooVariable() const { return _fooVariable; } // longer inline methods should be formatted like regular functions, // but indented void fooVariable(int new_value) { assert(new_value \u0026gt;= minLegalFoo); _fooVariable = new_value; } };  #includes Whenever possible favor C++ includes over C include. E.g. choose cstdio, not stdio.h.\nThe block of #includes at the top of the file should be organized. We keep several sorted groups. This makes it easy to find #include and to avoid duplicate #includes.\nAlways include Python.h first if you need that header. This is mandated by the integration guide. The next header file should be your main header file (e.g., for foo.cc you\u0026rsquo;d include foo.hh first). Having this header first ensures that it is independent and can be included in other places without missing dependencies.\n// Include Python.h first if you need it. #include \u0026lt;Python.h\u0026gt; // Include your main header file before any other non-Python headers (i.e., the one with the same name as your cc source file) #include \u0026quot;main_header.hh\u0026quot; // C includes in sorted order #include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;sys/time.h\u0026gt; // C++ includes #include \u0026lt;cerrno\u0026gt; #include \u0026lt;cstdio\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;vector\u0026gt; // Shared headers living in include/. These are used both in the simulator and utilities such as the m5 tool. #include \u0026lt;gem5/asm/generic/m5ops.h\u0026gt; // M5 includes #include \u0026quot;base/misc.hh\u0026quot; #include \u0026quot;cpu/base.hh\u0026quot; #include \u0026quot;params/BaseCPU.hh\u0026quot; #include \u0026quot;sim/system.hh\u0026quot;  File structure and modularity Source files (.cc files) should never contain extern declarations; instead, include the header file associated with the .cc file in which the object is defined. This header file should contain extern declarations for all objects exported from that .cc file. This header should also be included in the defining .cc file. The key here is that we have a single external declaration in the .hh file that the compiler will automatically check for consistency with the .cc file. (This isn\u0026rsquo;t as important in C++ as it was in C, since linker name mangling will now catch these errors, but it\u0026rsquo;s still a good idea.)\nWhen sufficient (i.e., when declaring only pointers or references to a class), header files should use forward class declarations instead of including full header files.\nHeader files should never contain using namespace declarations at the top level. This forces all the names in that namespace into the global namespace of any source file including that header file, which basically completely defeats the point of using namespaces. It is OK to use using namespace declarations at the top level of a source (.cc) file since the effect is entirely local to that .cc file. It\u0026rsquo;s also OK to use them in _impl.hh files, since for practical purposes these are source (not header) files despite their extension.\nDocumenting the code Each file/class/member should be documented using doxygen style comments. The documentation style to use is presented here: Documentation Guidelines\nM5 Status Messages Fatal v. Panic There are two error functions defined in src/base/misc.hh: panic() and fatal(). While these two functions have roughly similar effects (printing an error message and terminating the simulation process), they have distinct purposes and use cases. The distinction is documented in the comments in the header file, but is repeated here for convenience because people often get confused and use the wrong one.\n panic() should be called when something happens that should never ever happen regardless of what the user does (i.e., an actual m5 bug). panic() calls abort() which can dump core or enter the debugger. fatal() should be called when the simulation cannot continue due to some condition that is the user\u0026rsquo;s fault (bad configuration, invalid arguments, etc.) and not a simulator bug. fatal() calls exit(1), i.e., a \u0026ldquo;normal\u0026rdquo; exit with an error code.  The reasoning behind these definitions is that there\u0026rsquo;s no need to panic if it\u0026rsquo;s just a silly user error; we only panic if m5 itself is broken. On the other hand, it\u0026rsquo;s not hard for users to make errors that are fatal, that is, errors that are serious enough that the m5 process cannot continue.\nInform, Warn and Hack The file src/base/misc.hh also houses 3 functions that alert the user to various conditions happening within the simulation: inform(), warn() and hack(). The purpose of these functions is strictly to provide simulation status to the user so none of these functions will stop the simulator from running.\n inform() and inform_once() should be called for informative messages that users should know, but not worry about. inform_once() will only display the status message generated by the inform_once function the first time it is called.   warn() and warn_once() should be called when some functionality isn\u0026rsquo;t necessarily implemented correctly, but it might work well enough. The idea behind a warn() is to inform the user that if they see some strange behavior shortly after a warn() the description might be a good place to go looking for an error.   hack() should be called when some functionality isn\u0026rsquo;t implemented nearly as well as it could or should be but for expediency or history sake hasn\u0026rsquo;t been fixed. inform() Provides status messages and normal operating messages to the console for the user to see, without any connotations of incorrect behavior. For example it\u0026rsquo;s used when secondary CPUs being executing code on ALPHA.  "
},
{
	"uri": "http://localhost/deprecated/compiling-linux/",
	"title": "Compiling Linux",
	"tags": [],
	"description": "",
	"content": "If you\u0026rsquo;re interested in ARM Linux kernels, please see this page: ARM Linux Kernel\nWe supply a repository of patches against the Linux kernel that enables some M5 features and provides default configuration files that work with M5. The repository is a Mercurial Queue (MQ) that is intended to be applied on top of a Linux repository. To compile a kernel using our patches repository you\u0026rsquo;ll need to get a copy of the linux-2.6 repository first. Once you have the repository, you\u0026rsquo;ll need to select the version of Linux you wish to compile, select the appropriate version of patches for that version of Linux, apply the patches, and then compile. Step-by-step instructions are provided below.\nNote that only certain specific versions of Linux are supported (as of this writing, the list includes 2.6.13, 2.6.16, 2.6.18, 2.6.22, and 2.6.27). To see a full list, type hg qguard -l in the linux-2.6 repository with the m5 patch queue, and look for the version number to appear on the right side of one of the patches. There is nothing preventing the patches from working with other kernel versions. However, no one has gone through the effort of verifying that the patches apply cleanly to other versions. (If you do make this effort, please let us know so we can update the patch queue.)\nThe correct way to use the patches repository is the following (assuming 2.6.27):\n# Get a copy of the linux-2.6 mercurial repository hg clone http://www.kernel.org/hg/linux-2.6/ # Get a copy of our patches to linux cd linux-2.6/.hg hg clone http://repo.m5sim.org/linux-patches/ patches # Return to the root linux directory cd .. # Update the linux source to the desired version (can take 5 minutes) # (see discussion above for supported versions) hg update v2.6.27 # Select tho appropriate patches for the version of linux you selected hg qselect 2.6.27 # Apply the patches hg qpush -a # Copy the default configuration file, so it's used cp .config.m5 .config # Compile the kernel (assuming the cross compiler is in $PATH, otherwise full path would need to be specified) # The dash after gnu is required. make ARCH=alpha CROSS_COMPILE=alpha-unknown-linux-gnu- vmlinux  "
},
{
	"uri": "http://localhost/deprecated/compiling-m5/",
	"title": "Compiling M5",
	"tags": [],
	"description": "",
	"content": " gem5 runs on Linux and Mac OS X, but should be easily portable to other Unix-like OSes. At times in the past gem5 has worked on OpenBSD and Microsoft Windows (under Cygwin), but these platforms are not regularly tested. Cygwin in particular is no longer actively supported; if you must run on a Windows host, we recommend installing Linux (e.g., Ubuntu Server) under a VM and running gem5 there. Free virtualization solutions such as VirtualBox and VMware Player work well for this usage.\nCross-endian support has been mostly added, however this has not been extensively tested. Running a program with syscall emulation is supported regardless of host/target endianess, however full-system simulation may have cross-endian issues (ALPHA full-system is known not to work on big endian machines).\nTo build gem5, you need to ensure you have all the dependencies installed.\nPossible Targets gem5 can build many binaries each for a different guest architecture. The currently available architectures are ALPHA, ARM, MIPS, POWER, SPARC, and X86. In addition there is a NULL architecture.\nFor each possible architecture and mode, several different executables can be built:\n gem5.debug - A binary used for debugging without any optimizations. Since no optimizations are done this binary is compiled the fastest, however since no optimizations are done it executes very slowly. gem5.opt - A binary with debugging and optimization. This binary executes much faster than the debug binary and still provides all the debugging facility of the debug version. However when debugging source code it can be more difficult to use that the debug target. gem5.prof - This binary is like the opt target, however it also includes profiling support suitable for use with gprof. gem5.perf - Similar to prof, this target is aimed for CPU and heap profiling using the google perftools. gem5.fast - This binary is the fastest binary and all debugging support is removed from the binary (including trace support). By default it also uses Link Time Optimization  Compiling Starting in the root of the source tree, you can build gem5 using a command of the form:\n% scons build//gem5.\nwhere the items between \u0026lt;\u0026gt; are the architectures, modes and binaries listed above. For example:\n% cd gem5 % scons build/ALPHA/gem5.debug scons: Reading SConscript files ... Checking for C header file fenv.h... yes Building in /tmp/gem5/build/ALPHA Options file /tmp/gem5/build/options/ALPHA not found, using defaults in build_opts/ALPHA Compiling in ALPHA with MySQL support. scons: done reading SConscript files. scons: Building targets ... g++ -o build/ALPHA/base/circlebuf.do -c -pipe -fno-strict-aliasing -Wall -Wno-sign-compare -Werror -Wundef -g3 -gdwarf-2 -O0 -DTHE_ISA=ALPHA_ISA -DDEBUG -Iext/dnet -I/usr/include/python2.4 -Ibuild/libelf/include -I/usr/include/mysql -Ibuild/ALPHA build/ALPHA/base/circlebuf.cc ...  If your output looked like the above, then congratulations, you\u0026rsquo;ve compiled gem5! The final binary is located at the path you specified in the argument to scons, e.g., build/ALPHA/gem5.debug. For more build options and further details about the build system, see the SCons build system page.\nInstalling full system files If you want to run the full-system version (including the full-system regression tests), you will also need to download the full-system files (disk images and binaries) from the Download page.\nThe path to these files is determined in configs/common/SysPaths.py. There are a couple of default paths hard-coded into this script; you can place the system files at one of those paths, edit SysPaths.py to change those paths, or override the paths in that file by setting your M5_PATH environment variable. If this is not done correctly you will see an error like ImportError: Can't find a path to system files. when you first attempt to run the simulator in full-system mode.\nNote that the default path, /dist/m5/system, is designed for environments where you have root (sudo) access (to create /dist) and want the files in a place where they can be shared by multiple users. If both of these are true, you can follow this example to put the system files at the default location:\n% sudo mkdir -p /dist/m5/system % cd /dist/m5/system % sudo tar vxfj \u0026lt;path\u0026gt;/m5_system_2.0b3.tar.bz2 % sudo mv m5_system_2.0b3/* . ; sudo rmdir m5_system_2.0b3/ % sudo chgrp -R \u0026lt;grp\u0026gt; /dist # where \u0026lt;grp\u0026gt; is a group that contains all the m5 users  In most cases, it\u0026rsquo;s simplest to put the files wherever is convenient and then set M5_PATH to point to them.\nTesting your build Once you\u0026rsquo;ve compiled gem5, you can verify that the build worked by running regression tests. Regression tests are also run via scons. The command to run all tests for a particular is constructed as follows:\n% scons build//tests/\nFor example, to run the regression tests on ALPHA/gem5.opt, type:\n% scons build/ALPHA/tests/opt\nThe regression framework is integrated into the scons build process, so the command above will (re)build ALPHA/gem5.opt if necessary before running the tests. Also thanks to scons\u0026rsquo;s dependence tracking, tests will be re-run only if the binary has been rebuilt since the last time the test was run. If the previous test run is still valid (as far as scons can tell), only a brief pass/fail message will be printed out based on the result of that previous test, rather than the full output and statistics diff that is printed when the test is actually executed.\nRegression tests are further subdivided into three categories (\u0026ldquo;quick\u0026rdquo;, \u0026ldquo;medium\u0026rdquo;, and \u0026ldquo;long\u0026rdquo;) based on runtime. You can run only the tests in a particular category by adding that category name to the target path, e.g.:\n% scons build/ALPHA/tests/opt/quick\n(Note that currently the \u0026ldquo;medium\u0026rdquo; category is empty; all of the tests are \u0026ldquo;quick\u0026rdquo; or \u0026ldquo;long\u0026rdquo;.)\nSpecific tests can be run by appending the test name:\n% scons build/ALPHA/tests/opt/quick/fs/10.linux-boot\nFor more details, see Regression Tests.\n"
},
{
	"uri": "http://localhost/deprecated/compiling-workloads/",
	"title": "Compiling Workloads",
	"tags": [],
	"description": "",
	"content": " Cross Compilers A cross compiler is a compiler set up to run on one ISA but generate binaries which run on another. You may need one if you intend to simulate a system which uses a particular ISA, Alpha for instance, but don\u0026rsquo;t have access to actual Alpha hardware. There are various sources for cross compilers, listed here in roughly recommended order:\n Some architectures have professionally build cross-compilers available from Code Sourcery. These are updated frequently and a good starting point: ARM, MIPS You can build your own cross compiler using crosstools-ng: Download it from here and follow the instructions on that page. We have some available on our Download page.  Alternatively, you can use QEMU and a disk image to run the desired ISA in emulation. See\n{{#ev:youtube|Oh3NK12fnbg|400|center|A youtube video of working with image files using qemu on Ubuntu 12.04 64bit. Video resolution can be set to 1080}}\nNote: The video uses an Ubuntu Natty image. Since Natty is now quite old, you will need to update your /etc/apt/sources.list file, on the mounted image. Use, e.g., sudo vi /etc/apt/sources.list, and replace http://old-releases.ubuntu.com/ubuntu/ in place of http://ports.ubuntu.com/. Also, for things to work as show in the video, you will need to have have sshd running on the host machine.\nSyscall Emulation Mode SE mode workloads must be statically linked. Gem5 doesn\u0026rsquo;t yet support dynamic linking, so all benchmarks run in SE mode must be statically linked in order to be started properly by the simulator. In FS mode the simulated operating system takes care of any dynamic linking, so this restriction only applies to SE mode. If you\u0026rsquo;re using gcc you can pass it the \u0026ndash;static option.\n"
},
{
	"uri": "http://localhost/deprecated/scripts/",
	"title": "Config scripts ideas",
	"tags": [],
	"description": "",
	"content": " Background The configurations we have work, but are really hard to learn and the code is getting harder and harder to effectively maintain\nIssues A extensible configuration system is one of the road blocks to a better regression system Asymmetric configurations are problematic with the current system as we need to come up with a way to set CPUID information for each cluster and size them independently Having clocks to emulate some sort of DVFS (just frequency scaling) is hard. Ultimately you\u0026rsquo;d like to be able to change the clock and associated objects (caches etc) would all slow down or speed up.\nPossible Solutions There are a couple of things that I propose here, but generally it involves creating a CPU container object (or maybe more generally a container object). The container has a set of ports so that functions can always know how to connect to them, but it\u0026rsquo;s not a specific object. In this way we could wrap a core + l1s in a container and hand it to another function to build a cluster out of and similar we could hand multiple containers of objects to a system to build a multi-cluster system. The containers could have clocks and in that way we could use the\u0026rsquo;Nc\u0026rsquo; notation for latencies in gem5 where instead of specifying \u0026lsquo;10ns\u0026rsquo; you can specify \u0026lsquo;5c\u0026rsquo; (5 of the parent clocks), so that should deal with the first level of configuration issues. Th\nThe second part of the process is to make the various building function we have much more generic in what they do and move them to be part of gem5. This is somewhat like what addL1caches() or connectAllPorts() does on CPU models, but we should have more them and they should take objects to instantiate, not instantiate fixed object. Another part of it is better use of Base classes. If we name all L1 caches L1Cache (maybe BL1Cache/LL1Cache for an asymmetric system), then a script to change all the big or the little cache sizes is as stipple as BL1Cache = \u0026lsquo;32kB\u0026rsquo;, no crazy tree of if statements is required.\nFinally, it might let use use decorators to build a system, for example if we start with a working configuration we could then have decorators (http://stackoverflow.com/questions/739654/understanding-python-decorators) permute it:\n@detailedCPU(A15Like) @L1caches('64kb') @L2caches('2MB') @makeMultiCluster(2) makeArmSystem(cores=4)\nWith the underlying containers the decorators should be able to pull apart and recreate the system each time adding the functionality.\nRethinking configuration from the ground up  fix the paths stuff. Right now we\u0026rsquo;ve hard coded some paths that use and use a env variable to find the rest. We then hard code specific directories after those paths (e.g. binaries, disks, etc). This seems rather pointless and only allows a single monothlic location to store files in.We should have configuration file like ~/.gem5_paths that contains paths to search. The functions that find files should just search everywhere, and if we\u0026rsquo;re worried about name collisions we can record the md5 sum of files in (3) and warn if a file with the right md5 sum can\u0026rsquo;t be found or error if there are two or more possible files neither of which has a matching md5 sum. encapsulate the right things in SysConfig() re-do the notion of benchmarks where a benchmark is a directory with a configuration file in it; search for necessary files in this directory or the search paths; don\u0026rsquo;t need to edit a file to add the benchmarks just go looking for directories with benchmark.cfg or whatever in it Create generic classes within gem5 for as many objects as possible (e.g. L1ICache, etc) Create helper methods that live in gem5 such as connectAllPorts() to connect objects When objects are being instantiated, pass those objects as kv parameters (with defaults) rather then hard coding their names so that someone can utilize the function. For example rather than having a createCaches(options, system) we really should have createCaches(l1i=L1ICache, l1d=L1DCache, L2=L2Cache, L3=None) and the user can override these and still get a working system out as opposed to having a tree of ifs or hard coding specific names within the function. The creation of the objects and the configuration of them should be separate. Later in the config you can say L1Cache.size = \u0026lsquo;128kB\u0026rdquo; and not have to worry. Remove as much duplication as possible with the above in FSConfig.py Encapsulate units of the configuration in containers described above Provide a mechanism to supply \u0026ldquo;default\u0026rdquo; configs that can execute a request of L1ICache.size = \u0026hellip; commands and make the topology look like a particular configuration Re-write simulation.py to be comprehendible  "
},
{
	"uri": "http://localhost/deprecated/dacapo/",
	"title": "DaCapo",
	"tags": [],
	"description": "",
	"content": " This page describes how to get the Oracle-Sun JRE version 7 working on a disk image for use with ARM gem5. This will allow you to use the DaCapo benchmarks, which are on the Ubuntu image for gem5 ARM in the /benchmarks/dacapo directory.\nInstalling Oracle-Sun Java on an Ubuntu Image for ARM gem5 Download the standard edition(SE) of Java 7; the SE version is the free version of Java available from Oracle. Make sure to get the headless, SoftFP version of the JRE for ARMv6/7 from here. You may need to register with Oracle before being allowed to download. Once you have this, untar it.\nSetup the Java Directory  //mount the image sudo mkdir /mnt/ubuntu-gem5 sudo mount -o loop,offset=32256 arm-ubuntu-natty-headless-java.img /mnt/ubuntu-gem5 //setup the Java install directory and copy the Java directory over to your image cd /mnt/ubuntu-gem5/usr/lib sudo mkdir jvm cd ./jvm sudo cp -a /path_to_jre/jre_dir . sudo ln -s ./jre_dir java-7-sun\nInstall Java  cd /mnt/ubuntu-gem5 sudo mount -o bind /proc ./proc sudo mount -o bind /dev ./dev sudo mount -o bind /sys ./sys sudo chroot . exec ./bin/bash //now install Java cd /usr/lib/jvm update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-7-sun/bin/java 1 update-alternatives --install /usr/bin/keytool keytool /usr/lib/jvm/java-7-sun/bin/keytool 1 //setup .jinfo file echo \u0026quot;name=java-7-sun\u0026quot; \u0026gt; .java-7-sun.jinfo echo \u0026quot;alias=java-7-sun\u0026quot; \u0026gt;\u0026gt; .java-7-sun.jinfo echo \u0026quot;priority=1\u0026quot; \u0026gt;\u0026gt; .java-7-sun.jinfo echo \u0026quot;section=non-free\u0026quot; \u0026gt;\u0026gt; .java-7-sun.jinfo echo \u0026quot;jre java /usr/lib/jvm/java-7-sun/bin/java\u0026quot; \u0026gt;\u0026gt; .java-7-sun.jinfo echo \u0026quot;jre keytool /usr/lib/jvm/java-7-sun/bin/keytool\u0026quot; \u0026gt;\u0026gt; .java-7-sun.jinfo //update java alternatives update-java-alternatives -s java-7-sun\nOnce you have unmounted the image, you should be able to run Java with this disk image. Boot this image on gem5 and, from the terminal, type java -version to verify the Java version on your image. E.g., the version tested on gem5 is:\n root@gem5sim:~# java -version java version \u0026quot;1.7.0_04-ea\u0026quot; Java(TM) SE Runtime Environment for Embedded (build 1.7.0_04-ea-b20, headless) Java HotSpot(TM) Embedded Client VM (build 23.0-b21, mixed mode)\n"
},
{
	"uri": "http://localhost/docs/debugging-simulated/",
	"title": "Debugging simulated code",
	"tags": [],
	"description": "",
	"content": " gem5 has built-in support for gdb\u0026rsquo;s remote debugger interface. If you are interested in monitoring what the code on the simulated machine is doing\u0026mdash;the kernel (in FS mode) or program (in SE mode)\u0026mdash;you can fire up gdb on the host platform and have it talk to the simulated gem5 system as if it were a real machine/process (only better, since gem5 executions are deterministic and gem5\u0026rsquo;s remote debugger interface is guaranteed not to perturb execution on the simulated system).\nIf you are simulating a system that uses a different ISA from the host you\u0026rsquo;re running on, you\u0026rsquo;ll need a cross-architecture gdb; see below for instructions. If you are simulating the native ISA of your host, you can very likely just use the pre-installed native gdb.\nWhen gem5 is run, each CPU listens for a remote debugging connection on a TCP port. The first port allocated is generally 7000, though if a port is in use, the next port will be tried.\nTo attach the remote debugger, it\u0026rsquo;s necessary to have a copy of the kernel and of the source. Also to view the kernel\u0026rsquo;s call stack, you must make sure Linux was built with the necessary debug configuration parameters enabled. To run the remote debugger, do the following:\nziff% gdb-linux-alpha arch/alpha/boot/vmlinux GNU gdb Copyright 2002 Free Software Foundation, Inc. GDB is free software, covered by the GNU General Public License, and you are welcome to change it and/or distribute copies of it under certain conditions. Type \u0026quot;show copying\u0026quot; to see the conditions. There is absolutely no warranty for GDB. Type \u0026quot;show warranty\u0026quot; for details. This GDB was configured as \u0026quot;--host=i686-pc-linux-gnu --target=alpha-linux\u0026quot;... (no debugging symbols found)... (gdb) set remote Z-packet on [ This can be put in .gdbinit ] (gdb) target remote ziff:7000 Remote debugging using ziff:7000 0xfffffc0000496844 in strcasecmp (a=0xfffffc0000b13a80 \u0026quot;\u0026quot;, b=0x0) at arch/alpha/lib/strcasecmp.c:23 23 } while (ca == cb \u0026amp;\u0026amp; ca != '\\0'); (gdb)  The gem5 simulator is already running and the target remote command connects to the already running simulator and stops it in the middle of execution. You can set breakpoints and use the debugger to debug the kernel. It is also possible to use the remote debugger to debug console code and palcode. Setting that up is similar, but a how to will be left for future work.\nIf you\u0026rsquo;re using both the remote debugger and the debugger on the simulator, it is possible to trigger the remote debugger from the main debugger by doing a \u0026lsquo;call debugger()\u0026rsquo;. Before you do this you\u0026rsquo;ll need to figure out what CPU (the cpu id) you want to debug and set current_debugger to that cpuid. If you only have one cpu, then it will be cpuid 0, however if there are multiple cpus you will need to match the cpu id with the corresponding port number for the remote gdb session. For example, using the following sample output from M5, calling the kernel debugger for cpu 3 requires the kernel debugger to be listening on port 7001.\n%./build/ALPHA/gem5.debug configs/example/fs.py ... making dual system Global frequency set at 1000000000000 ticks per second Listening for testsys connection on port 3456 Listening for drivesys connection on port 3457 0: testsys.remote_gdb.listener: listening for remote gdb #0 on port 7002 0: testsys.remote_gdb.listener: listening for remote gdb #1 on port 7003 0: testsys.remote_gdb.listener: listening for remote gdb #2 on port 7000 0: testsys.remote_gdb.listener: listening for remote gdb #3 on port 7001 0: drivesys.remote_gdb.listener: listening for remote gdb #4 on port 7004 0: drivesys.remote_gdb.listener: listening for remote gdb #5 on port 7005 0: drivesys.remote_gdb.listener: listening for remote gdb #6 on port 7006 0: drivesys.remote_gdb.listener: listening for remote gdb #7 on port 7007  Getting a cross-architecture gdb To use a remote debugger with gem5, the most important part is that you have gdb compiled to work with the target system you\u0026rsquo;re simulating (e.g. alpha-linux if simulating an Alpha target, arm-linux if simulating an ARM target, etc). It is possible to compile an non-native architecture gdb on an x86 machine for example. All that must be done is add the \u0026ndash;target= option to configure when you compile gdb. You may also get pre-compiled debuggers with cross compilers. See Download for links to some cross compilers that include debuggers.\n% wget http://ftp.gnu.org/gnu/gdb/gdb-6.3.tar.gz --08:05:33-- http://ftp.gnu.org/gnu/gdb/gdb-6.3.tar.gz =\u0026gt; `gdb-6.3.tar.gz' Resolving ftp.gnu.org... done. Connecting to ftp.gnu.org[199.232.41.7]:80... connected. HTTP request sent, awaiting response... 200 OK Length: 17,374,476 [application/x-tar] 100%[====================================\u0026gt;] 17,374,476 216.57K/s ETA 00:00 08:06:52 (216.57 KB/s) - `gdb-6.3.tar.gz' saved [17374476/17374476] % tar xfz gdb-6.3.tar.gz % cd gdb-6.3 % ./configure --target=alpha-linux \u0026lt;configure output....\u0026gt; % make \u0026lt;make output...this may take a while\u0026gt;  The end result is gdb/gdb which will work for remote debugging.\nTarget-specific instructions ARM Target If you\u0026rsquo;re planning to debug an ARM kernel you\u0026rsquo;ll need a reasonably new version of gdb (7.1 or greater). Additionally, you\u0026rsquo;ll have to manually specify the tspecs like this (port number may be different). The tspec file is available in the gdb source code:\nset remote Z-packet on set tdesc filename path/to/features/arm-with-neon.xml symbol-file target remote \u0026lt;ip addr of host running gem5 or if local host 127.0.0.1\u0026gt;:7000\n"
},
{
	"uri": "http://localhost/docs/debugging-gdb/",
	"title": "Debugging w/gdb",
	"tags": [],
	"description": "",
	"content": " If traces alone are not sufficient, you\u0026rsquo;ll need to inspect what gem5 is doing in detail using a debugger (e.g., gdb). You definitely want to use the gem5.debug binary if you reach this point. Ideally, looking at traces should at least allow you to narrow down the range of cycles in which you think something is going wrong. The fastest way to reach that point is to use a DebugEvent, which goes on gem5\u0026rsquo;s event queue and forces entry into the debugger when the specified cycle is reached by sending the process a SIGTRAP signal. You\u0026rsquo;ll need to to start gem5 under the debugger or have the debugger attached to the gem5 process for this to work.\nYou can create one or more DebugEvents when you invoke gem5 using the \u0026quot;--debug-break=100\u0026quot; parameter. You can also create new DebugEvents from the debugger prompt using the schedBreak() function. The following example session illustrates both of these approaches:\n% gdb m5/build/ALPHA/gem5.debug GNU gdb 6.1 Copyright 2002 Free Software Foundation, Inc. [...] (gdb) run --debug-break=2000 configs/run.py Starting program: /z/stever/bk/m5/build/ALPHA/gem5.debug --debug-break=2000 configs/run.py M5 Simulator System [...] warn: Entering event queue @ 0. Starting simulation... Program received signal SIGTRAP, Trace/breakpoint trap. 0xffffe002 in ?? () (gdb) p curTick $1 = 2000 (gdb) c Continuing. (gdb) call schedBreak(3000) (gdb) c Continuing. Program received signal SIGTRAP, Trace/breakpoint trap. 0xffffe002 in ?? () (gdb) p _curTick $3 = 3000 (gdb)  gem5 includes a number of functions specifically intended to be called from the debugger (e.g., using the gdb \u0026lsquo;call\u0026rsquo; command, as in the schedBreak() example above). Many of these are \u0026ldquo;dump\u0026rdquo; functions which display internal simulator data structures. For example, eventq_dump() displays the events scheduled on the main event queue. Most of the other dump functions are associated with particular objects, such as the instruction queue and the ROB in the detailed CPU model. These include:\n   function Effect     schedBreak() Schedule a SIGTRAP to occur at    setDebugFlag(\u0026quot;\u0026quot;) Enable a debug flag from the debugger   clearDebugFlag(\u0026quot;\u0026quot;) Disable a debug flags from the debugger   eventqDump() Print out all events on the event queue   takeCheckpoint() Create a checkpoint at cycle     Additional gdb-accessible features for debugging coherence protocols in the classic memory system are documented here.\nDebugging Python with PDB You can debug configuration scripts with the Python debugger (PDB) just as you would other Python scripts. You can enter PDB before your configuration script is executed by giving the --pdb argument to the gem5 binary. Another approach is to put the following line in your configuration script (e.g., fs.py or se.py) wherever you would like to enter the debugger:\nimport pdb; pdb.set_trace()  Note that the Python files under src are compiled in to the gem5 binary, so you must rebuild the binary if you add this line (or make other changes) in these files. Alternatively, you can set the M5_OVERRIDE_PY_SOURCE environment variable to \u0026ldquo;true\u0026rdquo; (see src/python/importer.py).\nSee the official PDB documentation for more details on using PDB.\n"
},
{
	"uri": "http://localhost/developer/defining-cpu-models/",
	"title": "Defining CPU models",
	"tags": [],
	"description": "",
	"content": " Overview First, make sure you have basic understanding of how the CPU models function within the M5 framework. A good start is the CPU Models page.\nThis brief tutorial will show you how to create a custom CPU model called \u0026lsquo;MyCPU\u0026rsquo;, which will just be a renamed version of the AtomicSimpleCPU. After you learn how to compile and build \u0026lsquo;MyCPU\u0026rsquo;, then you have the liberty to edit the \u0026lsquo;MyCPU\u0026rsquo; code at your heart\u0026rsquo;s content without worrying about breaking any existing M5 CPU Models.\nPort C++ Code for MyCPU The easiest way is to derive a new C++ class of your CPU Model from M5 CPU Models that are already defined and the easiest model to start with is the \u0026lsquo;AtomicSimpleCPU\u0026rsquo; located in the \u0026lsquo;m5/src/cpu/simple\u0026rsquo; directory.\nFor this example, we\u0026rsquo;ll just copy the files from the \u0026lsquo;m5/src/cpu/simple\u0026rsquo; and place them in our own CPU directory: m5/src/cpu/mycpu.\nme@mymachine:~/m5$ cd src/cpu me@mymachine:~/m5/src/cpu$ mkdir mycpu me@mymachine:~/m5/src/cpu$ cp -r simple/* mycpu  Now check the mycpu directory to make sure you\u0026rsquo;ve copied the files successfully:\nme@mymachine:~/m5/src/cpu$ cd mycpu me@mymachine:~/m5/src/cpu/mycpu$ ls AtomicSimpleCPU.py BaseSimpleCPU.py SConscript SConsopts TimingSimpleCPU.py atomic.cc atomic.hh base.cc base.hh timing.cc timing.hh  Let\u0026rsquo;s move the AtomicSimpleCPU.py object file to MyCPU.py and remove the TimingSimpleCPU.py file. We\u0026rsquo;ll edit MyCPU.py a little later:\nme@mymachine:~/m5/src/cpu/mycpu$ mv AtomicSimpleCPU.py MyCPU.py me@mymachine:~/m5/src/cpu/mycpu$ rm TimingSimpleCPU.py me@mymachine:~/m5/src/cpu/mycpu$ ls BaseSimpleCPU.py MyCPU.py SConscript SConsopts atomic.cc atomic.hh base.cc base.hh timing.cc timing.hh  Since we want to change \u0026lsquo;AtomicSimpleCPU\u0026rsquo; to \u0026lsquo;MyCPU\u0026rsquo; we will just replace all the names in the atomic.* files and name them mycpu.* files:\nme@mymachine:~/m5/src/cpu/mycpu$ perl -pe s/AtomicSimpleCPU/MyCPU/g atomic.hh \u0026gt; mycpu.hh me@mymachine:~/m5/src/cpu/mycpu$ perl -pe s/AtomicSimpleCPU/MyCPU/g atomic.cc \u0026gt; mycpu.cc me@mymachine:~/m5/src/cpu/mycpu$ ls BaseSimpleCPU.py MyCPU.py SConscript SConsopts atomic.cc atomic.hh base.cc base.hh mycpu.hh mycpu.cc timing.cc timing.hh  The last thing you need to do is edit your mycpu.cc file to contain a reference to the \u0026lsquo;cpu/mycpu/mycpu.hh\u0026rdquo; header file instead of the \u0026lsquo;cpu/simple/atomic.hh\u0026rsquo; file:\n#include \u0026quot;arch/locked_mem.hh\u0026quot; ... #include \u0026quot;cpu/mycpu/mycpu.hh\u0026quot; ...  NOTE: The AtomicSimpleCPU is really just based off the BaseSimpleCPU (src/cpu/simple/base.hh) so your new CPU Model MyCPU is really a derivation off of this CPU model. Additionally, the BaseSimpleCPU model is derived from the BaseCPU (src/cpu/base.hh). As you can see, M5 is heavily object oriented.\nMaking M5 Recognize MyCPU Now that you\u0026rsquo;ve created a separate directory and files for your MyCPU code (i.e. m5/src/cpu/mycpu), there are a couple files that need to be updated so that M5 can recognize M5 as a build option:\n m5/src/cpu/mycpu/MyCPU.py: Edit your MyCPU python script file (e.g. MyCPU.py) so that your CPU can be recognized as a simulation object. For this example, we will just use the same code tha was previously in AtomicSimpleCPU.py file but replace the name \u0026lsquo;AtomicSimpleCPU\u0026rsquo; with \u0026lsquo;MyCPU\u0026rsquo;.  from m5.params import * from m5 import build_env from BaseSimpleCPU import BaseSimpleCPU class MyCPU(BaseSimpleCPU): type = 'MyCPU' width = Param.Int(1, \u0026quot;CPU width\u0026quot;) simulate_data_stalls = Param.Bool(False, \u0026quot;Simulate dcache stall cycles\u0026quot;) simulate_inst_stalls = Param.Bool(False, \u0026quot;Simulate icache stall cycles\u0026quot;) icache_port = Port(\u0026quot;Instruction Port\u0026quot;) dcache_port = Port(\u0026quot;Data Port\u0026quot;) physmem_port = Port(\u0026quot;Physical Memory Port\u0026quot;) _mem_ports = BaseSimpleCPU._mem_ports + \\ ['icache_port', 'dcache_port', 'physmem_port']   m5/src/cpu/mycpu/SConscript: Edit the SConscript for your CPU model and add the relevant files that need to be built in here. Your file should only contain this code:  Import('*') if 'MyCPU' in env['CPU_MODELS']: Source('mycpu.cc') SimObject('MyCPU.py') TraceFlag('MyCPU') # make sure that SimpleCPU is part of the TraceFlag # Otherwise, DPRINTF may not work properly TraceFlag('SimpleCPU') Source('base.cc') SimObject('BaseSimpleCPU.py')   m5/src/cpu/mycpu/SConsopts: Edit the SConscript for your CPU model and add the relevant files that need to be built in here. Your file should only contain this code:  Import('*') all_cpu_list.append('MyCPU') default_cpus.append('MyCPU')   m5/src/cpu/static_inst.hh: Put a forward class declaration of your model in here  ... class CheckerCPU; class FastCPU; class AtomicSimpleCPU; class TimingSimpleCPU; class InorderCPU; class MyCPU; ...   m5/src/cpu/cpu_models.py: Add in CPU Model-specific information for the ISA Parser. The ISA Parser will use this when referring to the \u0026ldquo;Execution Context\u0026rdquo; for executing instructions. For instance, the AtomicSimpleCPU\u0026rsquo;s instructions get all of their information from the actual CPU (since it\u0026rsquo;s a 1 CPI machine). Thus, instructions only need to know the current state or \u0026ldquo;Execution Context\u0026rdquo; of the \u0026lsquo;AtomicSimpleCPU\u0026rsquo; object. However, the instructions in a O3CPU needed to know the register values (\u0026amp; other state) only known to that current instruction so it\u0026rsquo;s \u0026ldquo;Execution Context\u0026rdquo; is the O3DynInst object. (check out the ISA Description Language documentation page for more details)  ... CpuModel('AtomicSimpleCPU', 'atomic_simple_cpu_exec.cc', '#include \u0026quot;cpu/simple/atomic.hh\u0026quot;', { 'CPU_exec_context': 'AtomicSimpleCPU' }) CpuModel('MyCPU', 'mycpu_exec.cc', '#include \u0026quot;cpu/mycpu/mycpu.hh\u0026quot;', { 'CPU_exec_context': 'MyCPU' }) ...  Building MyCPU Navigate to the M5 top-level directory and build your model:\nme@mymachine:~/m5/src/python/objects$cd ~/m5 me@mymachine:~/m5$scons build/MIPS_SE/m5.debug CPU_MODELS=MyCPU scons: Reading SConscript files ... Checking for C header file Python.h... (cached) yes ... Building in /y/ksewell/research/m5-sim/newmem-clean/build/MIPS_SE Options file /y/ksewell/research/m5-sim/newmem-clean/build/options/MIPS_SE not found, using defaults in build_opts/MIPS_SE Compiling in MIPS_SE with MySQL support. scons: done reading SConscript files. scons: Building targets ... make_hh([\u0026quot;build/MIPS_SE/base/traceflags.hh\u0026quot;], [\u0026quot;build/MIPS_SE/base/traceflags.py\u0026quot;]) Generating switch header build/MIPS_SE/arch/interrupts.hh Generating switch header build/MIPS_SE/arch/isa_traits.hh Defining FULL_SYSTEM as 0 in build/MIPS_SE/config/full_system.hh. Generating switch header build/MIPS_SE/arch/regfile.hh Generating switch header build/MIPS_SE/arch/types.hh Defining NO_FAST_ALLOC as 0 in build/MIPS_SE/config/no_fast_alloc.hh. ... g++ -o build/MIPS_SE/arch/mips/faults.do -c -pipe -fno-strict-aliasing -Wall -Wno-sign-compare -Werror -Wundef -ggdb3 -DTHE_ISA=MIPS_ISA -DDEBUG -DTRACING_ON=1 -Iext/dnet -I/usr/include/python2.4 -Ibuild/libelf/include -I/usr/include/mysql -Ibuild/MIPS_SE build/MIPS_SE/arch/mips/faults.cc g++ -o build/MIPS_SE/arch/mips/isa_traits.do -c -pipe -fno-strict-aliasing -Wall -Wno-sign-compare -Werror -Wundef -ggdb3 -DTHE_ISA=MIPS_ISA -DDEBUG -DTRACING_ON=1 -Iext/dnet -I/usr/include/python2.4 -Ibuild/libelf/include -I/usr/include/mysql -Ibuild/MIPS_SE build/MIPS_SE/arch/mips/isa_traits.cc g++ -o build/MIPS_SE/arch/mips/utility.do -c -pipe -fno-strict-aliasing -Wall -Wno-sign-compare -Werror -Wundef -ggdb3 -DTHE_ISA=MIPS_ISA -DDEBUG -DTRACING_ON=1 -Iext/dnet -I/usr/include/python2.4 -Ibuild/libelf/include -I/usr/include/mysql -Ibuild/MIPS_SE build/MIPS_SE/arch/mips/utility.cc ... cat build/MIPS_SE/m5.debug.bin build/MIPS_SE/m5py.zip \u0026gt; build/MIPS_SE/m5.debug chmod +x build/MIPS_SE/m5.debug scons: done building targets.  If you are compiling on a dual-core CPU, use this command-line to speed-up the compilation:\nscons -j2 build/MIPS_SE/m5.debug CPU_MODELS=MyCPU  Creating Configuration Scripts for MyCPU Now that you have a M5 binary built for use with the MIPS Architecture in a M5, MyCPU Model, you are almost ready to simulate. Note that the standard M5 command line requires taht your provide at least a configuration script for the M5 binary to use.\nThe easiest way to get up and running is to use the sample Syscall-Emulation script: configs/example/se.py.\nYou\u0026rsquo;ll note that line 9 of the se.py Python script imports the details of what type of Simulation will be run (e.g. what CPU Model?) from the Simulation.py file found here: m5/configs/common/Simulation.py. And then, the Simulation.py file imports it\u0026rsquo;s CPU Model options from the Options.py file in the same directory. Edit those two files and your M5 binary will be ready to simulate.\n m5/configs/common/Options.py: Add a option for your model in this file\u0026hellip;  ... parser.add_option(\u0026quot;--my_cpu\u0026quot;, action=\u0026quot;store_true\u0026quot;, help=\u0026quot;Use MyCPU Model\u0026quot;) ...   m5/configs/common/Simulation.py: Edit the Simulation script to recognize your model as a CPU class. After your edits, the setCPUClass function should look like this:  ... def setCPUClass(options): atomic = False if options.timing: class TmpClass(TimingSimpleCPU): pass elif options.my_cpu: class TmpClass(MyCPU): pass atomic = True elif options.detailed: if not options.caches: print \u0026quot;O3 CPU must be used with caches\u0026quot; sys.exit(1) class TmpClass(DerivO3CPU): pass elif options.inorder: if not options.caches: print \u0026quot;InOrder CPU must be used with caches\u0026quot; sys.exit(1) class TmpClass(InOrderCPU): pass else: class TmpClass(AtomicSimpleCPU): pass atomic = True CPUClass = None test_mem_mode = 'atomic' if not atomic: if options.checkpoint_restore != None or options.fast_forward: CPUClass = TmpClass class TmpClass(AtomicSimpleCPU): pass else: test_mem_mode = 'timing' return (TmpClass, test_mem_mode, CPUClass) ...  Testing MyCPU Once you\u0026rsquo;ve edited the configuration scripts, you can run a M5 simulation from the top level directory with this command\nline: me@mymachine:~/m5$build/MIPS_SE/m5.debug configs/example/se.py --my_cpu --cmd=tests/test-progs/hello/bin/mips/linux/hello  NOTE: This binary path refers to the m5/tests directory.\nSummary So the above \u0026ldquo;tutorial\u0026rdquo; showed you how to build your own CPU model in M5. Now, it\u0026rsquo;s up to you to customize the CPU Model as you like and do your experiments! Good luck!\n"
},
{
	"uri": "http://localhost/developer/defining-isa/",
	"title": "Defining an ISa",
	"tags": [],
	"description": "",
	"content": " Overview First, make sure you have basic understanding of how an ISA description generates instructions within the M5 framework. A good start is the The M5 ISA description language page.\nFor this example, we will be constructing an ISA called MyISA which will just be a renamed version of the MIPS ISA. We will go through the steps of creating the files and configuration opions for an M5 ISA description.\nYour new ISA description, MyISA, will need to generate correct instructions for the different CPU models. More specifically, your MyISA description will allow your MyISA architecture (analagous to ALPHA,MIPS,SPARC,etc.) to be plugged into System-Call Emulation (SE) and Full-System (FS) simulations of any M5 CPU Model.\nSyscall Emulation (SE) MyISA Creating the Files for MyISA The correct place to insert your ISA files in M5 is in the src/arch directory. In this directory, create another directory called \u0026lsquo;myisa\u0026rsquo; to keep your code. For this example, we will copy the relevant files needed from the M5 MIPS ISA description.\ncd src/arch mkdir myisa cd myisa cp -r ../mips/*.hh ./ cp -r ../mips/*.cc ./  The relevant files are as follows:\n isa_traits.hh/cc -  regfile.hh/cc, regfile/* -  process.hh/cc -  linux/* -  isa/* -  faults.hh/cc -   You need to make sure that all instances of MipsISA needs to be replaced with MyISA in the files.\nperl -pe s/Mips/My/g ????  Making M5 Recognize MyISA  m5/build_opts/MYISA_SE - Create this file that allows M5 to recognize MyISA as a Syscall Emulation build option. It\u0026rsquo;s contents should contain:  TARGET_ISA = 'myisa' FULL_SYSTEM = 0   m5/src/arch/isa_specific.hh - Edit this file by adding a constant for MyISA and then adding MyISA to the #define if/else structure.  ... #define ALPHA_ISA 21064 ... #define MY_ISA 6400 ... #if THE_ISA == ALPHA_ISA #define TheISA AlphaISA #elif THE_ISA == SPARC_ISA #define TheISA SparcISA ... #elif THE_ISA == MY_ISA #define TheISA MyISA #else #error \u0026quot;THE_ISA not set\u0026quot; #endif   m5/src/arch/myisa/SConsopts - Edit the file so that the SCons build system will recognize your ISA  Import('*') all_isa_list.append('myisa')  MyISA Decoding \u0026amp; Instruction Object Creation - src/arch/MyISA/isa/* At this point, the next major component for defining your own ISA is setting up the MyISA decoder. Please refer to the ISA description page for detailed specifics of the instruction object decoding and construction process.\nWe\u0026rsquo;ll go over the files you will be using for your MyISA description here:\n isa/*  operands.isa base.isa decoder.isa main.isa includes.isa  isa/formats/*  formats.isa basic.isa int.isa branch.isa control.isa fp.isa mem.isa noop.isa tlbop.isa trap.isa unimp.isa unknown.isa util.isa   Testing MyISA **Test That your decoder\nbuilds:** scons build/MYISA_SE/arch/MyISA/atomic_simple_cpu_exec.cc CPU_MODELS=AtomicSimpleCPU  "
},
{
	"uri": "http://localhost/deprecated/",
	"title": "Deprecated",
	"tags": [],
	"description": "",
	"content": "The content in this section is old and likely incorrect. It should probably be deleted.\n There might be some small portions of the pages that are still relevant at should be incorporated in other parts of the website\n"
},
{
	"uri": "http://localhost/ruby/memory-components/",
	"title": "Deprecated",
	"tags": [],
	"description": "",
	"content": " System This is a high level container for few of the important components of the Ruby which may need to be accessed from various parts and components of Ruby. Only ONE instance of this class is created. The instance of this class is globally available through a pointer named g_system_ptr. It holds pointer to the Ruby\u0026rsquo;s profiler object. This allows any component of Ruby to get hold of the profiler and collect statistics in a central location by accessing it though g_system_ptr. It also holds important information about the memory hierarchy like Cache blocks size, Physical memory size and makes them available to all parts of Ruby as and when required. It also holds the pointer to the on-chip network in Ruby. Another important objects that it hold pointer to is the Ruby\u0026rsquo;s wrapper for the simulator event queue (called RubyEventQueue). It also contains pointer to the simulated physical memory (pointed by variable name m_mem_vec_ptr). Thus in sum, System class in Ruby just acts as a container to pointers to some important objects of Ruby\u0026rsquo;s memory system and makes them available globally to all places in Ruby through exposing it self through g_system_ptr.\nParameters  random_seed is seed to randomize delays in Ruby. This allows simulating multiple runs with slightly perturbed delays/timings. randomization is the parameter when turned on, asks Ruby to randomly delay messages. This falg is useful when stress testing a system to expose corner cases. This flag should NOT be turned on when collecting simulation statistics. clock is the parameter for setting the clock frequency (on-chip). block_size_bytes specifies the size of cache blocks in bytes. mem_size specifies the physical memory size of the simulated system. network gives the pointer to the on-chip network for Ruby. profiler gives the pointer to the profiler of Ruby. tracer is the pointer to the Ruby \u0026rsquo;s memory request tracer. Tracer is primarily used to playback memory request trace in order to warm up Ruby\u0026rsquo;s caches before actual simulation starts.  Related files  src/mem/ruby/system  System.hh/cc: contains code for the System RubySystem.py :the corresponding python file with parameters.   Sequencer Sequencer is one of the most important classes in Ruby, through which every memory request must pass through at least twice \u0026ndash; once before getting serviced by the cache coherence mechanism and once just after being serviced by the cache coherence mechanism. There is one instantiation of Sequencer class for each of the hardware thread being simulated. For example, if we are simulating a 16-core system with each core has single hardware thread context, then there would be 16 Sequencer objects in the system, with each being responsible for managing memory requests (Load, Store, Atomic operations etc) from one of the given hardware thread context. ith Sequencer object handles request from only ith hardware context (in case of above example it is ith core). Each Sequencer assumes that it has access to the L1 Instruction and Data caches that are attached to a given core.\nFollowing are the primary responsibilities of Sequencer:\n Injecting and accounting for all memory request to the underlying cache hierarchy (and coherence). Resource allocation and accounting (e.g. makes sure a particular core/hardware thread does not have more than specified number of outstanding memory requests). Making sure atomic operations are handled properly. Making sure that underlying cache hierarchy and coherence protocol is making forward progress. Once a request is serviced by the underlying cache hierarchy, Sequencer is responsible for returning the result to the corresponding port of the frontend (i.e.M5).  Parameters  icache is the parameter where the L1 Instruction cache pointer is passed. dcache is the parameter where the L1 Data cache pointer is passed. max_outstanding_requests is the parameter for specifying maximum allowed number of outstanding memory request from a given core or hardware thread. deadlock_threshold is the parameter that specifies number of cycle (ruby cycles) after which if a given memory request is not satisfied by the cache hierarchy, a possibility of deadlock (or lack of forward progress) is declared.  Related files  src/mem/ruby/system  Sequencer.hh/cc: contains code for the Sequencer Sequencer.py :the corresponding python file with parameters.   More detailed operation description In this section, we will describe the operations of Sequencer in more details.\n Injection of memory request to cache hierarchy, request accounting and resource allocation:  -\nThe entry point for a memory request to the Sequencer is method called ***makeRequest***. This method is called from the corresponding RubyPort (Ruby's wrapper for M5 port). Once it gets the request, Sequencer checks for whether required resource limitations need to be enforced or not. This is done by calling a method called ***getRequestStatus***. In this method, it is made sure that a given Sequencer (i.e. a core or hardware thread context) can does NOT issue multiple simultaneous requests to same cache block. If the current request indeed for a cache block for which another request is still pending from the same Sequencer then the current request is not issued to the cache hierarchy and instead the current request wait for the previous request from the same cache block to satisfied first. This is done in the code by checking for two requesting accounting table called *m_writeRequestTable* and *m_readRequestTable* and setting the status to *RequestSatus_Aliased*. It also makes sure that number of outsatnding memory request from a given Sequencer does not overshoot. If it is found that the current request won't violate any of the above constrained then the request is registered for accounting purposes. This is done by calling the method named ***insertRequest***. In this method, depending upon the type of the request, an entry for the request is created in the either *m_writeRequestTable* or *m_readRequestTable*. These two tables keep record for write request and read requests, respectively, that are issued to the cache hierarchy by the given Sequencer but still to be satisfied. Finally the memory request is finally pushed to the cache hierarchy by calling the method named ***issueRequest***. This method is responsible of creating the request structure that is understood by the underlying SLICC generated coherence protocol implementation and cache hierarchy. This is done by setting the request type and mode accordingly and creating an object of class *RubyRequest* for the current request. Finally, L1 Instruction or L1 Data cache accesses latencies are accounted for an the request is pushed to the Cache hierarchy and the coherence mechanism for processing. This is done by *enqueue*-ing the request to the pointer to the mandatory queue (*m_mandatory_q_ptr*). Every request is passed to the corresponding L1 Cache controller through this mandatory queue. Cache hierarchy is then responsible for satisfying the request.   \u0026rdquo;\u0026rsquo; Deadlock/lack of forward progress detection: \u0026ldquo;\u0026rsquo;  -\nAs mentioned earlier, one other responsibility of the Sequencer is to make sure that Cache hierarchy is making progress in servicing the memory requests that have been issued. This is done by periodically waking up and scanning through the *m_writeRequestTable* and *m_readRequestTable* tables. which holds the currently outstanding requests from the given Sequencer and finds out which requests have been issued but have not satisfied by the cache hierarchy. If it finds any unsatisfied request that have been issues more than *m_deadlock_threshold* (parameter) cycles back, it reports a possible deadlock by the Cache hierarchy. Note that, although it reports possible deadlock, it actually detects lack of forward progress. Thus there may be false positives in this deadlock detection mechanism.   \u0026rsquo; Send back result to the front-end and making sure Atomic operations are handled properly:\u0026lsquo;  -\nOnce the Cache hierarchy satisfies a request it calls Sequencer's **''readCallback**'' or *writeCallback*''' method depending upon the type of the request. Note that the time taken to service the request is automatically accounted for through event scheduling as the ***readCallback*** or '**'writeCallback**' are called only after number of cycles required to satisfy the request has been accounted for. In these two methods the corresponding record of the request from the '' m_readRequestTable'' or *m_writeRequestTable* is removed. Also if the request was found to be part of a Atomic operations (e.g. RMW or LL/SC), then appropriate actions are taken to make sure that semantics of atomic operations are respected. After that a method called ***hitCallback*** is called. In this method, some statics is collected by calling functions on the Ruby's profiler. For write request, the data is actually updated in this function (and not while simulating the request through the cache hierarchy and coherence protocol). Finally ***ruby_hit_callback*** is called ultimately sends back the packet to the front-end, signifying completion of the processing of the memory request from Ruby's side.  CacheMemory and Cache Replacement Polices This module can model any Set-associative Cache structure with a given associativity and size. Each instantiation of the following module models a single bank of a cache. Thus different types of caches in system (e.g. L1 Instruction, L1 Data , L2 etc) and every banks of a cache needs to have separate instantiation of this module. This module can also model Fully associative cache when the associativity is set to\n In Ruby memory system, this module is primarily expected to be accessed by the SLICC generated codes of the given Coherence protocol being modeled.  Basic Operation This module models the set-associative structure as a two dimensional (2D) array. Each row of the 2D array represents a set of in the set-associative cache structure, while columns represents ways. The number of columns is equal to the given associativity (parameter), while the number of rows is decided depending on the desired size of the structure (parameter), associativity (parameter) and the size of the cache line (parameter). This module exposes six important functionalities which Coherence Protocols uses to manage the caches.\n It allows to query if a given cache line address is present in the set-associative structure being modeled through a function named isTagPresent. This function returns true, iff the given cache line address is present in it. It allows a lookup operation which returns the cache entry for a given cache line address (if present), through a function named lookup. It returns NULL if the blocks with given address is not present in the set-associative cache structure. It allows to allocate a new cache entry in the set-associative structure through a function named allocate. It allows to deallocate a cache entry of a given cache line address through a function named deallocate. It can be queried to find out whether to allocate an entry with given cache line address would require replacement of another entry in the designated set (derived from the cache line address) or not. This functionality is provided through cacheAvail function, which for a given cache line address, returns True, if NO replacement of another entry the same set as the given address is required to make space for a new entry with the given address. The function cacheProbe is used to find out cache line address of a victim line, in case placing a new entry would require victimizing another cache blocks in the same set. This function returns the cache line address of the victim line given the address of the address of the new cache line that would have to be allocated.  Parameters There are four important parameters for this class.\n size is the parameter that provides the size of the set-associative structure being modeled in units of bytes. assoc specifies the set-associativity of the structure. *replacement_policy\u0026rdquo; is the name of the replacement policy that would be used to select victim cache line when there is conflict in a given set. Currently, only two possible choices are available (*PSEUDO_LRU\u0026rdquo; and LRU). Finally, start_index_bit parameter specifies the bit position in the address from where indexing into the cache should start. This is a tricky parameter and if not set properly would end up using only portion of the cache capacity. Thus how this value should be specified is explained through couple of examples. Let us assume the cache line size if 64 bytes and a single core machine with a L1 cache with only one bank and a L2 cache with 4 banks. For the CacheMemory module that would model the L1 cache should have start_index_bit set to log2(64) = 6 (this is the default value assuming 64 bytes cache line). This is required as addresses passed around in the Ruby is full address (i.e. equal to the number of bits required to access any address in the physical address range) and as the caches would be accessed in granularity of cache line size (here 64 bytes), the lower order 6 bits in the address would be essentially 0. So we should discard last 6 bits of the given address while calculating which set (index) in the set associative structure the given address should go to. Now let\u0026rsquo;s look into a more complicated case of L2 cache, which has 4 banks. As mentioned previously, this modules models a single bank of a set-associative cache. Thus there will be four instantiation of the CacheMemory class to model the whole L2 cache. Assuming which cache bank a request goes to is statically decided by the low oder log2(4) = 2 bits of the cache line address, the value of the bits in the address at the position 6 and 7 would be same for all accesses coming to a given bank (i.e. a instance of CacheMemory here). Thus indexing within the set associative structure (CacheMemory instance) modeling a given bank should use address bits 8 and higher for finding which set a cache block should go to. Thus start_index_bit parameter should be set to 8 for the banks of L2 in this example. If erroneously if this is set 6, only a fourth of desired L2 capacity would be utilized !!!  More detailed description of operation As mentioned previously, the set-associative structure is modeled as a 2D array in the CacheMemory class. The variable m_cache is this all important 2D array containing the set-associative structure. Each element of this 2D array is derived from type AbstractCacheEntry class. Beside the minimal required functionality and contents of each cache entry, it can be extended inside the Coherence protocol files. This allows CacheMemory to be generic enough to hold any type of cache entry as desired by a given Coherence protocol as long as it derives from AbstractCacheEntry interface. The m_cache 2D array has number of rows equal to the number of sets in the set-associative structure being modeled while the number of columns is equal to associativity.\nAs should happen in any set-associative structure, which set (row) a cache entry should reside is decided by part of the cache block address used for indexing. The function addressToCacheSet calculates this index given an address. The way in which a cache entry reside in its designated set (row) is noted in the a hash_map object named m_tag_index. So to access an cache entry in the set-associative structure, first the set number where the cache block should reside is calculated and then m_tag_index is looked-up to find out the way in which the required cache block resides. If an cache entry holds invalid entry or its empty then its set to NULL or its permission is set to NotPresent.\nOne important aspect of the Ruby\u0026rsquo;s caches are the segregation of the set-associative structure for the cache and its replacement policy. This allows modular design where structure of the cache is independent of the replacement policy in the cache. When a victim needs to be selected to make space for a new cache block (by calling cacheProbe function), getVictim function of the class implementing replacement policy is called for the given set. getVictim returns the way number of the victim. The replacement policy is updated about accesses by calling touch function of the replacement policy, which allows it to update the access recency. Currently there are two replacement policies are supported \u0026ndash; LRU and PseudoLRU. LRU policy has a straight forward implementation where it keeps track of the absolute time when each way within each set is accessed last time and it always victimizes the entry which was last accessed furthest back in time. PseudoLRU implements a binary-tree based Non-Recently-Used policy. It arranges the ways in each set in an implicit binary tree like structure. Every node of the binary tree encodes the information which of its two subtrees was accessed more recently. During victim selection process, it starts from the root of the tree and traverse down such that it chooses the subtree which was touched less recently. Traversal continues until it reaches a leaf node. It then returns the id of the leaf node reached.\nRelated files  src/mem/ruby/system  CacheMemory.cc: contains CacheMemory class which models a cache bank CacheMemory.hh: Interface for the CacheMemory class Cache.py: Python configuration file for CacheMemory class AbstractReplacementPolicy.hh: Generic interface for Replacement policies for the Cache LRUPolicy.hh: contains LRU replacement policy PseudoLRUPolicy.hh: contains Pseudo-LRU replacement policy  src/mem/ruby/slicc_interface  AbstarctCacheEntry.hh: contains the interface of Cache Entry   DMASequencer This module implements handling for DMA transfers. It is derived from the RubyPort class. There can be a number of DMA controllers that interface with the DMASequencer. The DMA sequencer has a protocol-independent interface and implementation. The DMA controllers are described with SLICC and are protocol-specific.\nTODO: Fix documentation to reflect latest changes in the implementation.\nNote:\n At any time there can be only 1 request active in the DMASequencer. Only ordinary load and store requests are handled. No other request types such as Ifetch, RMW, LL/SC are handled  Related Files  src/mem/ruby/system  DMASequencer.hh: Declares the DMASequencer class and structure of a DMARequest DMASequencer.cc: Implements the methods of the DMASequencer class, such as request issue and callbacks.   Configuration Parameters Currently there are no special configuration parameters for the DMASequencer.\nBasic Operation A request for data transfer is split up into multiple requests each transferring cache-block-size chunks of data. A request is active as long as all the smaller transfers are not completed. During this time, the DMASequencer is in a busy state and cannot accepts any new transfer requests.\nDMA requests are made through the makeRequest method. If the sequencer is not busy and the request is of the correct type (LD/ST), it is accepted. A sequence of requests for smaller data chunks is then issued. The issueNext method issues each of the smaller requests. A data/acknowledgment callback signals completion of the last transfer and triggers the next call to issueNext as long as all of the original data transfer is not complete. There is no separate event scheduler within the DMASequencer.\nMemory Controller This module simulates a basic DDR-style memory controller. It models a single channel, connected to any number of DIMMs with any number of ranks of DRAMs each. The following picture shows an overview of the memory organization and connections to the memory controller. General information about memory controllers can be found here.\n\u0026rdquo; Note: \u0026ldquo;\n The product of the memory bus cycle multiplier, memory controller latency, and clock cycle time(=1/processor frequency) gives a first-order approximation of the latency of memory requests in time. The Memory Controller module refines this further by considering bank \u0026amp; bus contention, queueing effects of finite queues, and refreshes.  Data sheet values for some components of the memory latency are specified in time (nanoseconds), whereas the Memory Controller module expects all delay configuration parameters in cycles. The parameters should be set appropriately taking into account the processor and bus frequencies. The current implementation does not consider pin-bandwidth contention. Infinite bandwidth is assumed. Only closed bank policy is currently implemented; that is, each bank is automatically closed after a single read or write. The current implementation handles only a single channel. If you want multiple address/data channels, you need to instantiate multiple copies of this module.  This is the only controller that is NOT specified in SLICC, but in C++.  Documentation source: Most (but not all) of the writeup in this section is taken verbatim from documentation in the gem5 source files, rubyconfig.defaults file of GEMS, and a ppt created by Andy Phelps on Jan 18, 2008.\nRelated Files  src/mem/ruby/system  MemoryControl.hh: This file declares the Memory Controller class. MemoryControl.cc: This file implements all the operations of the memory controller. This includes processing of input packets, address decoding and bank selection, request scheduling and contention handling, handling refresh, returning completed requests to the directory controller. MemoryControl.py: Configuration parameters   Configuration Parameters  dimms_per_channel: Currently the only thing that matters is the number of ranks per channel, i.e. the product of this parameter and ranks_per_dimm. But if and when this is expanded to do FB-DIMMs, the distinction between the two will matter.   Address Mapping: This is controlled by configuration parameters banks_per_rank, bank_bit_0, ranks_per_dimm, rank_bit_0, dimms_per_channel, dimm_bit_0. You could choose to have the bank bits, rank bits, and DIMM bits in any order. For the default values, we assume this format for addresses:  Offset within line: [5:0] Memory controller #: [7:6] Bank: [10:8] Rank: [11] DIMM: [12] Row addr / Col addr: [top:13]   If you get these bits wrong, then some banks won\u0026rsquo;t see any requests; you need to check for this in the .stats output.\n mem_bus_cycle_multiplier: Basic cycle time of the memory controller. This defines the period which is used as the memory channel clock period, the address bus bit time, and the memory controller cycle time. Assuming a 200 MHz memory channel (DDR-400, which has 400 bits/sec data), and a 2 GHz processor clock, mem_bus_cycle_multiplier=10.   mem_ctl_latency: Latency to returning read request or writeback acknowledgement. Measured in memory address cycles. This equals tRCD + CL + AL + (four bit times) + (round trip on channel) + (memory control internal delays). It\u0026rsquo;s going to be an approximation, so pick what you like. Note: The fact that latency is a constant, and does not depend on two low-order address bits, implies that our memory controller either: (a) tells the DRAM to read the critical word first, and sends the critical word first back to the CPU, or (b) waits until it has seen all four bit times on the data wires before sending anything back. Either is plausible. If (a), remove the \u0026ldquo;four bit times\u0026rdquo; term from the calculation above.   rank_rank_delay: This is how many memory address cycles to delay between reads to different ranks of DRAMs to allow for clock skew.   read_write_delay: This is how many memory address cycles to delay between a read and a write. This is based on two things: (1) the data bus is used one cycle earlier in the operation; (2) a round-trip wire delay from the controller to the DIMM that did the reading. Usually this is set to 2.   basic_bus_busy_time: Basic address and data bus occupancy. If you are assuming a 16-byte-wide data bus (pairs of DIMMs side-by-side), then the data bus occupancy matches the address bus occupancy at 2 cycles. But if the channel is only 8 bytes wide, you need to increase this bus occupancy time to 4 cycles.   mem_random_arbitrate: By default, the memory controller uses round-robin to arbitrate between ready bank queues for use of the address bus. If you wish to add randomness to the system, set this parameter to one instead, and it will restart the round-robin pointer at a random bank number each cycle. If you want additional nondeterminism, set the parameter to some integer n \u0026gt;= 2, and it will in addition add a n% chance each cycle that a ready bank will be delayed an additional cycle. Note that if you are in mem_fixed_delay mode (see below), mem_random_arbitrate=1 will have no effect, but mem_random_arbitrate=2 or more will.   mem_fixed_delay: If this is nonzero, it will disable the memory controller and instead give every request a fixed latency. The nonzero value specified here is measured in memory cycles and is just added to MEM_CTL_LATENCY. It will also show up in the stats file as a contributor to memory delays stalled at head of bank queue.   tFAW: This is an obscure DRAM parameter that says that no more than four activate requests can happen within a window of a certain size. For most configurations this does not come into play, or has very little effect, but it could be used to throttle the power consumption of the DRAM. In this implementation (unlike in a DRAM data sheet) TFAW is measured in memory bus cycles; i.e. if TFAW = 16 then no more than four activates may happen within any 16 cycle window. Refreshes are included in the activates.   refresh_period: This is the number of memory cycles between refresh of row x in bank n and refresh of row x+1 in bank n. For DDR-400, this is typically 7.8 usec for commercial systems; after 8192 such refreshes, this will have refreshed the whole chip in 64 msec. If we have a 5 nsec memory clock, 7800 / 5 = 1560 cycles. The memory controller will divide this by the total number of banks, and kick off a refresh to somebody every time that amount is counted down to zero. (There will be some rounding error there, but it should have minimal effect.)   Typical Settings for configuration parameters: The default values are for DDR-400 assuming a 2GHz processor clock. If instead of DDR-400, you wanted DDR-800, the channel gets faster but the basic operation of the DRAM core is unchanged. Busy times appear to double just because they are measured in smaller clock cycles. The performance advantage comes because the bus busy times don\u0026rsquo;t actually quite double. You would use something like these values:  -\n - mem_bus_cycle_multiplier: 5 bank_busy_time: 22 rank_rank_delay: 2 read_write_delay: 3 basic_bus_busy_time: 3 mem_ctl_latency: 20 refresh_period: 3120  Basic Operation  Data Structures  Requests are enqueued into a single input queue. Responses are dequeued from a single response queue. There is a single bank queue for each DRAM bank (the total number of banks is the number of DIMMs per channel x number of ranks per DIMM x number of banks per rank). Each bank also has a busy counter. tFAW shift registers are maintained per rank.\n Timing  The “Act” (Activate) and “Rd” (Read) commands (or activate and write) always come as a pair, because we are modeling posted-CAS mode. (In non-posted-CAS, the read or write command would be scheduled separately later.) We do not explicitly model the separate commands; we simply say that the address bus occupancy is 2 cycles.\nSince the data bus is also occupied for 2 cycles at a fixed offset in time as shown above, we do not need to explicitly model it; memory channel occupancy is still 2 cycles.\nFor back-to-back requests the data for the 2nd request could be delayed due to the following reasons:\n - Read happens from a different rank  Read is followed by a write 2nd request has a busy bank Basic request time \u0026gt; 2 (e.g. needs 8 data phits)    Scheduling and Bank Contention  The wakeup function, and in turn, the executeCycle function is tiggered once every memory clock cycle.\nEach memory request is placed in a queue associated with a specific memory bank. This queue is of finite size; if the queue is full the request will back up in an (infinite) common queue and will effectively throttle the whole system. This sort of behavior is intended to be closer to real system behavior than if we had an infinite queue on each bank. If you want the latter, just make the bank queues unreasonably large.\nThe head item on a bank queue is issued when all of the following are true:\n The bank is available The address path to the DIMM is available The data path to or from the DIMM is available  Note that we are not concerned about fixed offsets in time. The bank will not be used at the same moment as the address path, but since there is no queue in the DIMM or the DRAM it will be used at a constant number of cycles later, so it is treated as if it is used at the same time.\nWe are assuming \u0026ldquo;posted CAS\u0026rdquo;; that is, we send the READ or WRITE immediately after the ACTIVATE. This makes scheduling the address bus trivial; we always schedule a fixed set of cycles. For DDR-400, this is a set of two cycles; for some configurations such as DDR-800 the parameter tRRD forces this to be set to three cycles.\nWe assume a four-bit-time transfer on the data wires. This is the minimum burst length for DDR-2. This would correspond to (for example) a memory where each DIMM is 72 bits wide and DIMMs are ganged in pairs to deliver 64 bytes at a shot.This gives us the same occupancy on the data wires as on the address wires (for the two-address-cycle case).\nThe only non-trivial scheduling problem is the data wires. A write will use the wires earlier in the operation than a read will; typically one cycle earlier as seen at the DRAM, but earlier by a worst-case round-trip wire delay when seen at the memory controller. So, while reads from one rank can be scheduled back-to-back every two cycles, and writes (to any rank) scheduled every two cycles, when a read is followed by a write we need to insert a bubble. Furthermore, consecutive reads from two different ranks may need to insert a bubble due to skew between when one DRAM stops driving the wires and when the other one starts. (These bubbles are parameters.)\nThis means that when some number of reads and writes are at the heads of their queues, reads could starve writes, and/or reads to the same rank could starve out other requests, since the others would never see the data bus ready. For this reason, we have implemented an anti-starvation feature. A group of requests is marked \u0026ldquo;old\u0026rdquo;, and a counter is incremented each cycle as long as any request from that batch has not issued. If the counter reaches twice the bank busy time, we hold off any newer requests until all of the \u0026ldquo;old\u0026rdquo; requests have issued.\n"
},
{
	"uri": "http://localhost/deprecated/submitting-contributions/",
	"title": "Deprecated submitting contributions",
	"tags": [],
	"description": "",
	"content": " OLD CONTRIBUTION DOCUMENTATION All of the contribution details have been moved into the gem5 source tree. The information below is out of date!\nIf you\u0026rsquo;ve made changes to gem5 that might benefit others, we strongly encourage you to contribute those changes to the public gem5 repository. There are several reasons to do this:\n Once your changes are part of the main repo, you no longer have to merge them back in every time you update your local repo. This can be a huge time savings! Once your code is in the main repo, other people have to make their changes work with your code, and not the other way around. Others may build on your contributions to make them even better, or extend them in ways you did not have time to do. You will have the satisfaction of contributing back to the community.  In the common case, the process is fairly simple:\n Organize your changes into one or more self-contained, documented patches with appropriate commit messages. Test your patches using the regression tests, as well as any other specific tests required to exercise your code. Post your patch(es) on our Gerrit server. Wait for reviews. Reviewers may ask you to modify your patch, or may engage you in some discussion. If necessary, update your patch based on initial reviews and wait for re-reviews. Once you\u0026rsquo;ve resolved any outstanding reviewer concerns and received a few \u0026lsquo;ship it!\u0026rsquo; reviews, someone with commit access should volunteer to commit your code. If they don\u0026rsquo;t, please make an explicit request on the gem5-dev mailing list. If you submit enough high-quality patches that people find it annoying to keep committing your patches for you, you will likely be given commit access yourself :).  The sections below provide more detail on these steps.\nCreating/Submitting Patches to Gerrit Please see the contributing documentation for gem5-specific contributing notes and the gerrit documentation for gerrit-specific instructions.\nCreating Patches See Managing Local Changes with Mercurial Queues to learn how to set up an use MQ to manage your code changes. The rest of this section will assume that you are using MQ to manage the patches you wish to contribute.\n When you\u0026rsquo;ve got something that you want to commit, think about what it does and who it might affect. Is it self contained? Can it be broken up into more logical segments? gem5 is a big project and there are a lot of people working on it. No one person knows everything, so we rely on the revision history and on comments to understand what is going on in the code. It is very important that you make change sets self contained. It is far easier to understand and review a series of ten 200 line changesets and what they do compared to a single 2000 change set that does a whole lot of stuff. This will also help you as a developer. If you keep your changesets small and self contained and you read them regularly, you will find your own bugs before you commit them.   Please don\u0026rsquo;t forget to put commit messages on your patches. The -e, -l, or -m options to hg qref will allow you to do this (run hg help qref for details).   We want to be able to identify everyone who makes a change to the repository, so we need your name to show up in a consistent manner in the repository. Please add this to your $HOME/.hgrc file before you start making changesets (hg commit or hg qfinish)  [ui] username=Nathan Binkert \u0026lt;nate@binkert.org\u0026gt;   Please try to follow the Coding Style. They make search and replace much more effective. It also makes it easier for developers to follow code if it all has a similar look. I\u0026rsquo;m sure that there are things that you hate about the style. I\u0026rsquo;ve never worked on a project where everyone liked the style, unless they\u0026rsquo;ve all worked on the project for a long time and just gotten used to it.  Commit Messages A canonical commit message consists of three parts:\n A short summary line describing the change. This line starts with one or more keywords separated by commas followed by a colon and a description of the change. This line should be no more than 65 characters long since version control systems usually add a prefix that causes line-wrapping for longer lines. (Optional, but highly recommended) A detailed description. This describes what you have done and why. If the change isn\u0026rsquo;t obvious, you might want to motivate why it is needed. Lines need to be wrapped to 75 characters or less. (Optional) Tags describing patch metadata. You are highly recommended to use tags to acknowledge reviewers for their work.  For example:\nscons, arch: Fixed the way the build system handled ISAs Note the keyword and colon on the first line, and that it's \u0026lt; 65 chars. Please keep your detailed commit comments to 75 columns or less. These comments will likely have to be wrapped manually, but that's not so bad a thing to have to do. Reported-by: Dedicated gem5 User \u0026lt;email\u0026gt; Signed-off-by: Original Developer \u0026lt;email\u0026gt; Reviewed-by: Grey Beard \u0026lt;email\u0026gt; [Committer: Rebased patch onto latest gem5] Signed-off-by: Committer \u0026lt;email\u0026gt;  The keyword should be one or more of the following separated by commas:\n Architecture name in lower case (e.g., arm or x86): Anything that is target-architecture specific. base ext stats sim syscall_emul config: mem: Classic memory system. Ruby uses its own keyword. ruby: Ruby memory models. cpu: CPU-model specific (except for kvm) kvm: KVM-specific. Changes to host architecture specific components should include an architecture keyword (e.g., arm or x86) as well. gpu-compute energy dev arch: General architecture support (src/arch/) scons: Build-system related. Trivial changes as a side effect of doing something unrelated (e.g., adding a source file to a SConscript) don\u0026rsquo;t require this. tests style: Changes to the style checkers of style fixes. misc  Tags are an optional mechanism to store additional metadata about a patch and acknowledge people who reported a bug or reviewed that patch. Tags are generally appended to the end of the commit message in the order they happen. We currently use the following tags:\n Signed-off-by: Added by the author and the submitter (if different). This tag is a statement saying that you believe the patch to be correct and have the right to submit the patch according to the license in the affected files. Similarly, if you commit someone else\u0026rsquo;s patch, this tells the rest of the world that you have have the right to forward it to the main repository. If you need to make any changes at all to submit the change, these should be described within hard brackets just before your Signed-off-by tag. Reviewed-by: Used to acknowledge patch reviewers. It\u0026rsquo;s generally considered good form to add these. Some code review systems add them automatically. Reported-by: Used to acknowledge someone for finding and reporting a bug. Reviewed-on: Link to the review request corresponding to this patch. Some review systems add these automatically. Change-Id: Used by Gerrit to track changes across rebases. Tested-by: Used to acknowledge people who tested a patch. Sometimes added automatically by review systems that integrate with CI systems.  Testing Patches  Before you circulate your patch for review, ask yourself: Have I compiled this code for all ISAs? Have I run the regression tests that are relevant? (You should run at least the quick regressions and the long too if you think you\u0026rsquo;re going to change any results.) You should avoid embarrassing yourself, annoying reviewers, or worst of all breaking the tree with trivial things that can be caught by these two steps.  Posting Patches  Now that you\u0026rsquo;ve made small, self contained changesets, you should seek feedback from people. We have our own Reviewboard server for this process. You can use the Reviewboard Extension which provides the hg postreview command to send out your changesets for review. Some Tips:  If you want to post a review for the changeset at the tip, use \u0026ldquo;hg postreview -o\u0026ldquo; If you want to update a review for the changeset at the tip, use \u0026ldquo;hg postreview -o -u -e\u0026rdquo;, where  is the review number on reviewboard. Everything is easier if you are using Mercurial Queues as you can use \u0026ldquo;hg qpush\u0026rdquo; and \u0026ldquo;hg qpop\u0026rdquo; to get your diff to the tip and then use the postreview commands above.   If your patch touches Ruby code, please get a review from Brad or Nilay before committing. If your patch touches ARM code please get a review from Ali or Andreas before committing.\nResponding to Reviews  Your reviews may request some changes; if so, make those and (unless the changes are trivial) re-post the modified patch for review, updating the existing review as described above.  Committing Patches  Once the reviewers are satisfied with your change, make sure your change is based on the up-to-date head of the tree. If you\u0026rsquo;re using mq, this is pretty trivial: hg qpop -a; hg pull -u; hg qpush (assuming the tree hasn\u0026rsquo;t changed too much out from under you) If you made any changes in the last two steps, recompile and re-run the quick regression tests. Even if your changes were totally trivial, you should at least recompile and run the \u0026ldquo;hello world\u0026rdquo; test to make sure you haven\u0026rsquo;t done something dumb (speaking from experience). If you\u0026rsquo;re a new developer and don\u0026rsquo;t have commit access, you\u0026rsquo;ll have to get someone with commit access to push the patch to the repository on your behalf. If you do have commit access, follow these steps:  Now that you\u0026rsquo;re ready to commit, you can hg qfinish the relevant patches to turn them into changesets. Ok, so you\u0026rsquo;re ready to push your changesets to the repository. One last step. Run hg outgoing. Are you about to push what you think you are? Do the commit messages all make sense? Ok, go ahead and hg push.   If your patch has been on reviewboard for a while without getting any reviews (or re-revires after you\u0026rsquo;ve posted changes), please email the gem5-dev list. If you have commit access, it is fair to give warning via email that you intend to commit the changes at some future date (e.g., a week out from the date of the email) if you do not hear any objections. Please do not simply commit a patch without giving warning on the gem5-dev list.\nCommitting Patches for other contributors  Download patch from reviewboard Import the patch (hg qimport) Push the patch (hg qpush) Fix any style errors, etc. Copy description from reviewboard (hg qref -e OR vi .hg/patches/) Update the patch user with reviewboard info for the person who wrote the patch (hg qref -u “Name ”). Put your ID as “Signed-off-by: Name ” Check that it looks right (hg log -l1 -v) Test the patch(es) (something like scons build/ARM/tests/opt/quick/se/). Do what’s appropriate, the submitter should have already tested! MAKE SURE NO OTHER PATCHES HAVE BEEN COMMITTED BEFORE DOING THE FOLLOWING! Commit the patches locally (hg qfinish -a) Make sure the commit messages still look right (hg log) Push the patches (hg push ssh://hg@repo.gem5.org/gem5) Ask the submitters to close the reviews on reviewboard  "
},
{
	"uri": "http://localhost/publications/projects/",
	"title": "Derivative projects",
	"tags": [],
	"description": "",
	"content": " Below is a list of projects that are based on gem5, are extensions of gem5, or use gem5.\nMV5  MV5 is a reconfigurable simulator for heterogeneous multicore architectures. It is based on M5v2.0 beta 4. Typical usage: simulating data-parallel applications on SIMT cores that operate over directory-based cache hierarchies. You can also add out-of-order cores to have a heterogeneous system, and all different types of cores can operate under the same address space through the same cache hierarchy. Research projects based on MV5 have been published in ISCA\u0026rsquo;10, ICCD\u0026rsquo;09, and IPDPS\u0026rsquo;10.  Features  Single-Instruction, Multiple-Threads (SIMT) cores Directory-based Coherence Cache: MESI/MSI. (Not based on gems/ruby) Interconnect: Fully connected and 2D Mesh. (Not based on gems/ruby) Threading API/library in system emulation mode (No support for full-system simulation. A benchmark suite using the thread API is provided)  Resources  Home Page: 1 Tutorial at ISPASS \u0026lsquo;11: 2 Google group: 3  gem5-gpu  Merges 2 popular simulators: gem5 and gpgpu-sim Simulates CPUs, GPUs, and the interactions between them Models a flexible memory system with support for heterogeneous processors and coherence Supports full-system simulation through GPU driver emulation  Resources  Home Page: 4 Overview slides: 5 Mailing list: 6  "
},
{
	"uri": "http://localhost/ruby/directed-tests/",
	"title": "Directed tests",
	"tags": [],
	"description": "",
	"content": "The Directed Tester is for generating a stream of requests, where two consecutive requests are separated by a fixed number of bytes. The requests can be a mix of reads and writes, or they can be invalidations. This can be useful in testing a prefetcher or for generally testing a coherence protocol implemented in Ruby. The source files related to the tester are present in the directory src/cpu/testers/directedtest. The file configs/examples/ruby_direct_test.py is used for configuration and execution of the test. For example, the following command can be used for testing \u0026ndash;\n./build/X86/gem5.opt ./configs/example/ruby_direct_test.py  Though one can specify many different options to the directed tester, some of them are note worthy.\n   Parameter Description     -n, \u0026ndash;num-cpus Number of cpus injecting load/store requests to the memory system.   -m, \u0026ndash;maxtick Number of cycles to simulate.   -l, \u0026ndash;checks Number of requests to be performed.   \u0026ndash;test-type SeriesGetx / SeriesGets / Invalidate   \u0026ndash;random_seed Seed for initialization of the random number generator.    "
},
{
	"uri": "http://localhost/docs/disk-images/",
	"title": "Disk images",
	"tags": [],
	"description": "",
	"content": " Background   Disk image basics A disk device in gem5 gets its initial contents from a file called a disk image. This file stores all the bytes present on the disk just as you would find them on an actual device. Some other systems also use disk images which are in more complicated formats and which provide compression, encryption, etc. gem5 currently only supports raw images, so if you have an image in one of those other formats, you\u0026rsquo;ll have to convert it into a raw image before you can use it in a simulation. There are often tools available which can convert between the different formats.\nBecause a disk image represents all the bytes on the disk itself, it contains more than just a file system. For harddrives on most systems, the image starts with a partition table. Each of the partitions in the table (frequently only one) is also in the image. If you want to manipulate the entire disk you\u0026rsquo;ll use the entire image, but if you want to work with just one partition and/or the file system on it, you\u0026rsquo;ll need to specifically select that part of the image. The losetup command (discussed below) has a -o option which lets you specify where to start in an image.\nCreating an empty image The recommended method to create a disk image is to use ./util/gem5img.py\nIt\u0026rsquo;s a good idea to understand how to build an image in case something goes wrong or you need to do something in an unusual way. However, gem5img.py script which will go through the process of building and formatting an image. If you want to understand the guts of what it\u0026rsquo;s doing see below. You can use the \u0026ldquo;init\u0026rdquo; command to create an empty image, \u0026ldquo;new\u0026rdquo;, \u0026ldquo;partition\u0026rdquo;, or \u0026ldquo;format\u0026rdquo; to perform those parts of init independently, and \u0026ldquo;mount\u0026rdquo; or \u0026ldquo;umount\u0026rdquo; to mount or unmount an existing image.\nMounting an image {{#ev:youtube|OXH1oxQbuHA|400|center|A youtube video of add file using mount on Ubuntu 12.04 64bit. Video resolution can be set to 1080}}\nTo mount a file system on your image file, first find a loopback device and attach it to your image with an appropriate offset as described in the \u0026ldquo;Formatting\u0026rdquo; section above.\nmount -o loop,offset=32256 foo.img\nUnmounting To unmount an image, use the umount command like you normally would.\numount\nImage contents Now that you can create an image file and mount it\u0026rsquo;s file system, you\u0026rsquo;ll want to actually put some files in it. You\u0026rsquo;re free to use whatever files you want, but the gem5 developers have found that Gentoo stage3 tarballs are a great starting point. They\u0026rsquo;re essentially an almost bootable and fairly minimal Linux installation and are available for a number of architectures.\nIf you choose to use a Gentoo tarball, first extract it into your mounted image. The /etc/fstab file will have placeholder entries for the root, boot, and swap devices. You\u0026rsquo;ll want to update this file as apporpriate, deleting any entries you aren\u0026rsquo;t going to use (the boot partition, for instance). Next, you\u0026rsquo;ll want to modify the inittab file so that it uses the m5 utility program (described elsewhere) to read in the init script provided by the host machine and to run that. If you allow the normal init scripts to run, the workload you\u0026rsquo;re interested in may take much longer to get started, you\u0026rsquo;ll have no way to inject your own init script to dynamically control what benchmarks are started, for instance, and you\u0026rsquo;ll have to interact with the simulation through a simulated terminal which introduces non-determinism.\nModifications By default gem5 does not store modifications to the disk back to the underlying image file. Any changes you make will be stored in an intermediate COW layer and thrown away at the end of the simulation. You can turn off the COW layer if you want to modify the underlying disk.\nKernel and bootloader Also, generally speaking, gem5 skips over the bootloader portion of boot and loads the kernel into simulated memory itself. This means that there\u0026rsquo;s no need to install a bootloader like grub to your disk image, and that you don\u0026rsquo;t have to put the kernel you\u0026rsquo;re going to boot from on the image either. The kernel is provided separately and can be changed out easily without having to modify the disk image.\nHow to create an Ubuntu image for ARM_FS A howto on creating an Ubuntu based full system image for gem5 can be found at Ubuntu Disk Image for ARM Full System.\nManipulating images with loopback devices Loopback devices Linux supports loopback devices which are devices backed by files. By attaching one of these to your disk image, you can use standard Linux commands on it which normally run on real disk devices. You can use the mount command with the \u0026ldquo;loop\u0026rdquo; option to set up a loopback device and mount it somewhere. Unfortunately you can\u0026rsquo;t specify an offset into the image, so that would only be useful for a file system image, not a disk image which is what you need. You can, however, use the lower level losetup command to set up a loopback device yourself and supply the proper offset. Once you\u0026rsquo;ve done that, you can use the mount command on it like you would on a disk partition, format it, etc. If you don\u0026rsquo;t supply an offset the loopback device will refer to the whole image, and you can use your favorite program to set up the partitions on it.\nWorking with image files To create an empty image from scratch, you\u0026rsquo;ll need to create the file itself, partition it, and format (one of) the partition(s) with a file system.\nCreate the actual file First, decide how large you want your image to be. It\u0026rsquo;s a good idea to make it large enough to hold everything you know you\u0026rsquo;ll need on it, plus some breathing room. If you find out later it\u0026rsquo;s too small, you\u0026rsquo;ll have to create a new larger image and move everything over. If you make it too big, you\u0026rsquo;ll take up actual disk space unnecessarily and make the image harder to work with. Once you\u0026rsquo;ve decided on a size you\u0026rsquo;ll want to actually create the file. Basically, all you need to do is create a file of a certain size that\u0026rsquo;s full of zeros. One approach is to use the dd command to copy the right number of bytes from /dev/zero into the new file. Alternatively you could create the file, seek in it to the last byte, and write one zero byte. All of the space you skipped over will become part of the file and is defined to read as zeroes, but because you didn\u0026rsquo;t explicitly write any data there, most file systems are smart enough to not actually store that to disk. You can create a large image that way but take up very little space on your physical disk. Once you start writing to the file later that will change, and also if you\u0026rsquo;re not careful, copying the file may expand it to its full size.\nPartitioning First, find an available loopback device using the losetup command with the -f option.\nlosetup -f\nNext, use losetup to attach that device to your image. If the available device was /dev/loop0 and your image is foo.img, you would use a command like this.\nlosetup /dev/loop0 foo.img\n/dev/loop0 (or whatever other device you\u0026rsquo;re using) will now refer to your entire image file. Use whatever partitioning program you like on it to set up one (or more) paritions. For simplicity it\u0026rsquo;s probably a good idea to create only one parition that takes up the entire image. We say it takes up the entire image, but really it takes up all the space except for the partition table itself at the beginning of the file, and possibly some wasted space after that for DOS/bootloader compatibility.\nFrom now on we\u0026rsquo;ll want to work with the new partition we created and not the whole disk, so we\u0026rsquo;ll free up the loopback device using losetup\u0026rsquo;s -d option\nlosetup -d /dev/loop0\nFormatting First, find an available loopback device like we did in the partitioning step above using losetup\u0026rsquo;s -f option.\nlosetup -f\nWe\u0026rsquo;ll attach our image to that device again, but this time we only want to refer to the partition we\u0026rsquo;re going to put a file system on. For PC and Alpha systems, that partition will typically be one track in, where one track is 63 sectors and each sector is 512 bytes, or 63 * 512 = 32256 bytes. The correct value for you may be different, depending on the geometry and layout of your image. In any case, you should set up the loopback device with the -o option so that it represents the partition you\u0026rsquo;re interested in.\nlosetup -o 32256 /dev/loop0 foo.img\nNext, use an appropriate formating command, often mke2fs, to put a file system on the partition.\nmke2fs /dev/loop0\nYou\u0026rsquo;ve now successfully created an empty image file. You can leave the loopback device attached to it if you intend to keep working with it (likely since it\u0026rsquo;s still empty) or clean it up using losetup -d.\nlosetup -d /dev/loop0\nDon\u0026rsquo;t forget to clean up the loopback device attached to your image with the losetup -d command.\nlosetup -d /dev/loop0\n"
},
{
	"uri": "http://localhost/developer/events/",
	"title": "Events",
	"tags": [],
	"description": "",
	"content": " This page provides some high-level information about Events in M5 including how to create, schedule, and remove them.\nThe simplest way to use events is to create an EventWrapper which lets you wrap an arbitrary function (that is based on a SimObject with an event that can occur). An illustrative example is probably the easiest way to describe the issue:\nclass Bar : public SimObject { void foo(); EventWrapper\u0026lt;Bar, \u0026amp;Bar::foo\u0026gt; fooEvent; };  When the class Bar is constructed the SimObject (this) pointer must be passed to the fooEvent initializer. This creates an event with default priority that hasn’t been scheduled. If you want to change the priority you need to pass that to the initializer as well fooEvent(this, false, priority);\nInside Bar, you can call schedule(fooEvent, curTick + some_delay_of_interest); which will schedule the event for some_delay_of_interest ticks in the future.\nTo see if the event is scheduled you can call fooEvent.scheduled() and to see when it is scheduled you can call fooEvent.when();\nIf you want to deschedule the event (so it never fires) you can deschedule like: deschedule(fooEvent). Finally, if you want to change when an event is scheduled for you can call reschedule(fooEvent, new_time);\nAdditionally, there is a separate lower-level Event class that the EventWrapper builds upon, however it’s use is generally not required.\nEvents Event queue EventManager objects (I don’t know a lot about these) Event objects Time sync "
},
{
	"uri": "http://localhost/developer/execution-basics/",
	"title": "Execution Basics",
	"tags": [],
	"description": "",
	"content": " Predecoding StaticInsts The StaticInst provides all static information and methods for a binary instruction.\nIt holds the following information/methods:\n Flags to tell what kind of instruction it is (integer, floating point, branch, memory barrier, etc.) The op class of the instruction The number of source and destination registers The number of integer and FP registers used Method to decode a binary instruction into a StaticInst Virtual function execute(), which defines how the specific architectural actions taken for an instruction (e.g. read r1, r2, add them and store in r3.) Virtual functions to handle starting and completing memory operations Virtual functions to execute the address calculation and memory access separately for models that split memory operations into two operations Method to disassemble the instruction, printing it out in a human readable format. (e.g. addq r1 r2 r3)  It does not have dynamic information, such as the PC of the instruction or the values of the source registers or the result. This allows a 1 to 1 mapping of StaticInst to unique binary machine instructions. We take advantage of this fact by caching the mapping of a binary instruction to a StaticInst in a hash_map, allowing us to decode a binary instruction only once, and directly using the StaticInst the rest of the time.\nEach ISA instruction derives from StaticInst and implements its own constructor, the execute() function, and, if it is a memory instruction, the memory access functions. See ISA_description_system for details about how these ISA instructions are specified.\nDynInsts The DynInst is used to hold dynamic information about instructions. This is necessary for more detailed models or out-of-order models, both of which may need extra information beyond the StaticInst in order to correctly execute instructions.\nSome of the dynamic information that it stores includes:\n The PC of the instruction The renamed register indices of the source and destination registers The predicted next-PC The instruction result The thread number of the instruction The CPU the instruction is executing on Whether or not the instruction is squashed  Additionally the DynInst provides the ExecContext interface. When ISA instructions are executed, the DynInst is passed in as the ExecContext, handling all accesses of the ISA to CPU state.\nDetailed CPU models can derive from DynInst and create their own specific DynInst subclasses that implement any additional state or functions that might be needed. See src/cpu/o3/alpha/dyn_inst.hh for an example of this.\nMicrocode support ExecContext The ExecContext describes the interface that the ISA uses to access CPU state. Although there is a file src/cpu/exec_context.hh, it is purely for documentation purposes and classes do not derive from it. Instead, ExecContext is an implicit interface that is assumed by the ISA.\nThe ExecContext interface provides methods to:\n Read and write PC information Read and write integer, floating point, and control registers Read and write memory Record and return the address of a memory access, prefetching, and trigger a system call Trigger some full-system mode functionality  Example implementations of the ExecContext interface include:\n SimpleCPU DynInst  See the ISA description page for more details on how an instruction set is implemented.\nThreadContext ThreadContext is the interface to all state of a thread for anything outside of the CPU. It provides methods to read or write state that might be needed by external objects, such as the PC, next PC, integer and FP registers, and IPRs. It also provides functions to get pointers to important thread-related classes, such as the ITB, DTB, System, kernel statistics, and memory ports. It is an abstract base class; the CPU must create its own ThreadContext by either deriving from it, or using the templated ProxyThreadContext class.\nProxyThreadContext The ProxyThreadContext class provides a way to implement a ThreadContext without having to derive from it. ThreadContext is an abstract class, so anything that derives from it and uses its interface will pay the overhead of virtual function calls. This class is created to enable a user-defined Thread object to be used wherever ThreadContexts are used, without paying the overhead of virtual function calls when it is used by itself. The user-defined object must simply provide all the same functions as the normal ThreadContext, and the ProxyThreadContext will forward all calls to the user-defined object. See the code of SimpleThread for an example of using the ProxyThreadContext.\nDifference vs. ExecContext The ThreadContext is slightly different than the ExecContext. The ThreadContext provides access to an individual thread\u0026rsquo;s state; an ExecContext provides ISA access to the CPU (meaning it is implicitly multithreaded on SMT systems). Additionally the ThreadState is an abstract class that exactly defines the interface; the ExecContext is a more implicit interface that must be implemented so that the ISA can access whatever state it needs. The function calls to access state are slightly different between the two. The ThreadContext provides read/write register methods that take in an architectural register index. The ExecContext provides read/write register methdos that take in a StaticInst and an index, where the index refers to the i\u0026rsquo;th source or destination register of that StaticInst. Additionally the ExecContext provides read and write methods to access memory, while the ThreadContext does not provide any methods to access memory.\nThreadState The ThreadState class is used to hold thread state that is common across CPU models, such as the thread ID, thread status, kernel statistics, memory port pointers, and some statistics of number of instructions completed. Each CPU model can derive from ThreadState and build upon it, adding in thread state that is deemed appropriate. An example of this is SimpleThread, where all of the thread\u0026rsquo;s architectural state has been added in. However, it is not necessary (or even feasible in some cases) for all of the thread\u0026rsquo;s state to be centrally located in a ThreadState derived class. The DetailedCPU keeps register values and rename maps in its own classes outside of ThreadState. ThreadState is only used to provide a more convenient way to centrally locate some state, and provide sharing across CPU models.\nFaults "
},
{
	"uri": "http://localhost/developer/execution-tracing/",
	"title": "Execution Tracing",
	"tags": [],
	"description": "",
	"content": " Tracing Interface Each cpu/thread has a \u0026ldquo;tracer\u0026rdquo; sim object. This defaults to an object which will write to the the same file descriptor as DPRINTFs. If you want, you can nullify the tracer to disable tracing for that cpu/thread. For output you want to be interleaved, use the same trace object (or maybe file?) You could also set up several different files to catch information from each cpu/thread.\nThere would be other tracer objects you could use instead if you wanted to do something else at each instruction boundary. You could, for instance, create a tracer object that synced with legion. You could also create one that works with statetrace.\nConfiguration If there are tracer objects in the python script, it becomes harder for the main m5 executable to set tracing parameters on the command line in a uniform way. Maybe these options would manipulate the default tracer object? That way, you would get the same behavior you\u0026rsquo;re used to (mostly) but still have the ability to change things up if you wanted to.\nMicrocode It would be very nice to be able to trace both macroops and microops simultaneously and separately. Right now, the microops are the only thing that\u0026rsquo;s traced because they\u0026rsquo;re the only instructions that are executed or commit. It would be ideal to be able to trace macroops only to see the macroop disassembly, microops only like now, or an interleaving of the two. The later option would be useful to see what was going on, but still have everything in the context of the original program. Currently, this isn\u0026rsquo;t possible because there is no link from a microop back to the macroop that generated it, and the macroops get thrown out before they could be traced. It would be nice to print the macroop before any block of microops that come from it, even if the instruction doesn\u0026rsquo;t commit. That way if it doesn\u0026rsquo;t commit, you can still tell where the instructions came from. To see that the macroop actually finished, the final microop could be marked in the disassembly somehow.\nFor instance:\nADD [$RAX+4], $RCX ---- ld t1, DS:[0*t0+rax+4] ---- add t1, t1, rcx ==== st t1, DS:[0*t0+rax+4]  or\nADD [$RAX+4], $RCX { ld t1, DS:[0*t0+rax+4] add t1, t1, rcx st t1, DS:[0*t0+rax+4] }  I prefer the first because all the \u0026ldquo;}\u0026rdquo; lines could take considerable space, and because non-microcode instructions still fit in easily.\n"
},
{
	"uri": "http://localhost/ruby/gems-transition/",
	"title": "GEMS transition to gem5",
	"tags": [],
	"description": "",
	"content": " This page covers the relevant changes that have been made to SLICC since the last and final GEMS release (2.1). It can be used as a guide to port SLICC protocols that formerly worked in GEMS to gem5.\nReasoning Most changes to the SLICC language were made in order to support new features of gem5, including the parameterized object model (SimObject), atomic instruction support, and others. All syntactic changes are relatively simple and mechanical.\nHigh Level Changes The major conceptual change that occurred in the gem5 transition relates to the classes that SLICC generates. Previously, SLICC would generate a C++ class for each machine as well as a Chip class that served as a container for objects that roughly belonged \u0026ldquo;on-chip\u0026rdquo;. The Chip class was responsible for defining, constructing, and initializing each machine (e.g., L1Cache_Controller) and related objects.\nIn the new version of SLICC for gem5, the Chip class no longer exists. The only C++ object that SLICC generates is the machine (and associated helper structs/classes like States). Notably, this means that non-generated code is now responsible for orchestrating the construction and initialization of machines.\nIn addition to removing the Chip class, the SLICC machines are also more modular now. Each machine can define parameters that are set at configuration time, allowing for the construction of heterogeneous memory system configurations. Also because of the enhanced configuration, any object used by a SLICC machine that can itself be configured (i.e., is a SimObject) must be passed as a parameter rather than created by the machine so that the object can be initialized properly by the configuration system. You\u0026rsquo;ll notice this, for example, in machines that accept a Sequencer object as a parameter. Parameters to a protocol are set in Python at configuration time (more details below).\nDetails Protocol Parameters SLICC machine definitions have been modified to fit better into the gem5 object model. Parameters are defined between the machine definition and body, like so:\nmachine(Name, desc)   : { body }\nParameters can be any type SLICC knows about, whether it be a primitive or external_type. Typically, parameters will include latencies, SimObjects used by the machine (e.g., Sequencer, CacheMemory), and possibly other protocol-specific parameters.\nParameters are declared with a type, and can optionally take a default value:\nmachine(L1Cache, \u0026quot;MSI Directory L1 Cache CMP\u0026quot;)  : Sequencer * sequencer,  CacheMemory * L1IcacheMemory,  CacheMemory * L1DcacheMemory,  int l2_select_num_bits,  int l1_request_latency = 2,  int l1_response_latency = 2,  int to_l2_latency = 1  { ... }\nEach protocol has an associated Python file in configs/ruby/ that is responsible for passing parameters to controllers. For example, below is a snippit from configs/ruby/MESI_CMP_directory.py that corresponds to the machine definition above:\nl1_cntrl = L1Cache_Controller(version = i,  cntrl_id = cntrl_count,  sequencer = cpu_seq,  L1IcacheMemory = l1i_cache,  L1DcacheMemory = l1d_cache,  l2_select_num_bits = l2_bits)\nAny parameter that is a SimObject (e.g., l1d_cache) is converted to a pointer type before being passed to SLICC.\nLatencies Previously, latencies were in an enqueue function as a string corresponding to a parameter in the Ruby configuration file. Now, latencies are integer parameters and the latency= portion of enqueue takes an integer rather than a string.\nenqueue(requestIntraChipL1Network_out, RequestMsg, latency=l1_request_latency) { ... }\nAtomic Instruction Support Because Ruby/gem5 must now correctly account for data, SLICC protocols have support for handling atomic (Read-Modify-Write) accesses. Most of that functionality is hidden in the generated code, but SLICC protocol writers are responsible for identifying which field in a message contains the address to block on. This is done using the block_on parameter to the peek function used in an in port. For example, the line\npeek(responseIntraChipNetwork_in, ResponseMsg, block_on=\u0026quot;Address\u0026quot;) { ... }\ntells the generated controller that when an atomic access occurs, to use the field ResponseMsg:Address as the address for comparison.\nState Declarations States are now declared using a special state_declaration function rather than a generic enumeration function as was done previously. Additionally, AccessPermissions are now directly tied to states. For example:\nstate_declaration(State, desc=\u0026quot;...\u0026quot;, default=\u0026quot;L1Cache_State_I\u0026quot;) {  NP, AccessPermission:Invalid, desc=\u0026quot;Not present in either cache\u0026quot;;  I, AccessPermission:Invalid, desc=\u0026quot;a L1 cache entry Idle\u0026quot;;   S, AccessPermission:Read_Only, desc=\u0026quot;a L1 cache entry Shared\u0026quot;;   E, AccessPermission:Read_Only, desc=\u0026quot;a L1 cache entry Exclusive\u0026quot;;   M, AccessPermission:Read_Write, desc=\u0026quot;a L1 cache entry Modified\u0026quot;, format=\u0026quot;!b\u0026quot;;  }\nThe format= option is also new to SLICC, and does \u0026lt;TODO: what does this do?\u0026gt;.\nEntry Types and Casting Cache entry definitions are now properly abstracted. Rather than relying on the convention that all protocols name fields of their cache entries the same, the common parts of a cache entry needed by CacheMemory to function (e.g., Address or locked) are declared in the non-generated AbstractCacheEntry class. Entries defined in a SLICC machine must inherit from that abstract class in order to be usable by CacheMemory.\nAs a consequence of inheriting from an abstract object inside of SLICC, we\u0026rsquo;ve also added a mechanism to cache to the correct entry type using static_cast.\nExample\nstructure(Entry, desc=\u0026quot;...\u0026quot;, interface=\u0026quot;AbstractCacheEntry\u0026quot;) {  State CacheState, desc=\u0026quot;cache state\u0026quot;;   ... }\nAn entry can later be used:\n Entry getL1DCacheEntry(Address addr), return_by_pointer=\u0026quot;yes\u0026quot; {  Entry L1Dcache_entry := static_cast(Entry, \u0026quot;pointer\u0026quot;, L1DcacheMemory[addr]);  return L1Dcache_entry;  }\n"
},
{
	"uri": "http://localhost/docs/gpu/",
	"title": "GPU Models",
	"tags": [],
	"description": "",
	"content": " AMD\u0026rsquo;s Compute-GPU Model MICRO-48 Tutoral A tutorial was held in conjunction with MICRO-48. We have made the slides available from our 2015 tutorial titled: The AMD gem5 APU Simulator: Modeling Heterogeneous Systems in gem5.\nCompute GPU Workloads Emualted CL Runtime  Download the emulated OpenCL runtime.  OpenCL Compiler CLOC is used to compile OpenCL kernels for use with gem5\u0026rsquo;s GPU compute model. The most recent revision of CLOC that is known to work with gem5 is:\ncommit cf777856cfce86d11ea97c245992971159b85a4d\nRondinia Benchmark Suite ARM\u0026rsquo;s NoMali GPU Model The NoMali GPU model models the interface used by ARM Mali GPUs. The model does not render or compute anything, but can be used to fake a GPU. This enables Android and ChromeOS experiments without software rendering which would otherwise make simulation results extremely misleading. It was presented in the 2015 gem5 User Workshop.\nGetting started instructions are currently available for Android 4.4 (KitKat).\n"
},
{
	"uri": "http://localhost/ruby/garnet/",
	"title": "Garnet",
	"tags": [],
	"description": "",
	"content": " More details of the gem5 Ruby Interconnection Network are here.\nGarnet Network Model \u0026rdquo;\u0026lsquo;Note: This model of garnet is no longer supported in gem5. The updated model is garnet2.0.\nGarnet is a detailed interconnection network model inside gem5. The details can be found in the ISPASS 2009 Paper.\nIf your use of Garnet contributes to a published paper, please cite the following paper:\n@inproceedings{garnet, title={GARNET: A detailed on-chip network model inside a full-system simulator}, author={Agarwal, Niket and Krishna, Tushar and Peh, Li-Shiuan and Jha, Niraj K}, booktitle={Performance Analysis of Systems and Software, 2009. ISPASS 2009. IEEE International Symposium on}, pages={33--42}, year={2009}, organization={IEEE} }  Garnet consists of 2 pipeline models: a detailed fixed-pipeline model, and an approximate flexible-pipeline model.\nThe fixed-pipeline model is intended for low-level interconnection network evaluations and models the detailed micro-architectural features of a 5-stage Virtual Channel router with credit-based flow-control. Researchers interested in investigating different network microarchitectures can readily modify the modeled microarchitecture and pipeline. Also, for system level evaluations that are not concerned with the detailed network characteristics, this model provides an accurate network model and should be used as the default model.\nThe flexible-pipeline model is intended to provide a reasonable abstraction of all interconnection network models, while allowing the router pipeline depth to be flexibly adjusted. A router pipeline might range from a single cycle to several cycles. For evaluations that wish to easily change the router pipeline depth, the flexible-pipeline model provides a neat abstraction that can be used.\n Related Files:  src/mem/ruby/network/Network.py src/mem/ruby/network/garnet/BaseGarnetNetwork.py src/mem/ruby/network/garnet/fixed-pipeline src/mem/ruby/network/garnet/GarnetNetwork_d.py src/mem/ruby/network/garnet/flexible-pipeline src/mem/ruby/network/garnet/flexible-pipeline/GarnetNetwork.py   Invocation The garnet networks can be enabled by adding \u0026ndash;garnet-network=fixed, or \u0026ndash;garnet-network=flexible on the command line, respectively.\nConfiguration Garnet uses the generic network parameters in Network.py:\n - number_of_virtual_networks: This is the maximum number of virtual networks. The actual number of active virtual networks is determined by the protocol.  control_msg_size: The size of control messages in bytes. Default is 8. m_data_msg_size in Network.cc is set to the block size in bytes + control_msg_size.   Additional parameters are specified in garnet/BaseGarnetNetwork.py:\n - ni_flit_size: flit size in bytes. Flits are the granularity at which information is sent from one router to the other. Default is 16 (=\u0026gt; 128 bits). [This default value of 16 results in control messages fitting within 1 flit, and data messages fitting within 5 flits]. Garnet requires the ni_flit_size to be the same as the bandwidth_factor (in network/BasicLink.py) as it does not model variable bandwidth within the network.  vcs_per_vnet: number of virtual channels (VC) per virtual network. Default is 4.   The following parameters in garnet/fixed-pipeline/GarnetNetwork_d.py are only valid for fixed-pipeline:\n - buffers_per_data_vc: number of flit-buffers per VC in the data message class. Since data messages occupy 5 flits, this value can lie between 1-5. Default is 4.  buffers_per_ctrl_vc: number of flit-buffers per VC in the control message class. Since control messages occupy 1 flit, and a VC can only hold one message at a time, this value has to be  Default is 1.    The following parameters in garnet/flexible-pipeline/GarnetNetwork.py are only valid for flexible-pipeline:\n - buffer_size: Size of buffers per VC. A value of 0 implies infinite buffering.  number_of_pipe_stages: number of pipeline stages in each router in the flexible-pipeline model. Default is 4.    Additional features  Routing: Currently, garnet only models deterministic routing using the routing tables described earlier. Modeling variable link bandwidth: The bandwidth_factor specifies the link bandwidth as the number of bytes per cycle per network link. ni_flit_size has to be same as this value. Links which have low bandwidth can be modeled by specifying a longer latency across them in the topology file (as explained earlier). Multicast messages: The network modeled does not have hardware multi-cast support within the network. A multi-cast message gets broken into multiple uni-cast messages at the interface to the network.   Garnet fixed-pipeline network The garnet fixed-pipeline models a classic 5-stage Virtual Channel router. The 5-stages are:\n Buffer Write (BW) + Route Compute (RC): The incoming flit gets buffered and computes its output port. VC Allocation (VA): All buffered flits allocate for VCs at the next routers. [The allocation occurs in a separable manner: First, each input VC chooses one output VC, choosing input arbiters, and places a request for it. Then, each output VC breaks conflicts via output arbiters]. All arbiters in ordered virtual networks are queueing to maintain point-to-point ordering. All other arbiters are round-robin. Switch Allocation (SA): All buffered flits try to reserve the switch ports for the next cycle. [The allocation occurs in a separable manner: First, each input chooses one input VC, using input arbiters, which places a switch request. Then, each output port breaks conflicts via output arbiters]. All arbiters in ordered virtual networks are queueing to maintain point-to-point ordering. All other arbiters are round-robin. Switch Traversal (ST): Flits that won SA traverse the crossbar switch. Link Traversal (LT): Flits from the crossbar traverse links to reach the next routers.  The flow-control implemented is credit-based.\nGarnet flexible-pipeline network The garnet flexible-pipeline model should be used when one desires a router pipeline different than 5 stages (the 5 stages include the link traversal stage). All the components of a router (buffers, VC and switch allocators, switch etc) are modeled similar to the fixed-pipeline design, but the pipeline depth is not modeled, and comes as an input parameter number_of_pipe_stages. The flow-control is implemented by monitoring the availability of buffers at each output port before sending.\n"
},
{
	"uri": "http://localhost/ruby/garnet-2/",
	"title": "Garnet 2.0",
	"tags": [],
	"description": "",
	"content": " More details of the gem5 Ruby Interconnection Network are here.\nGarnet2.0: An On-Chip Network Model for Heterogeneous SoCs Garnet2.0 is a detailed interconnection network model inside gem5. It is in active development, and patches with more features will be periodically pushed into gem5. Additional garnet-related patches and tool support under development (not part of the repo) can be found at the Garnet page at Georgia Tech.\nGarnet2.0 builds upon the original Garnet model which was published in 2009.\nIf your use of Garnet contributes to a published paper, please cite the following paper:\n@inproceedings{garnet, title={GARNET: A detailed on-chip network model inside a full-system simulator}, author={Agarwal, Niket and Krishna, Tushar and Peh, Li-Shiuan and Jha, Niraj K}, booktitle={Performance Analysis of Systems and Software, 2009. ISPASS 2009. IEEE International Symposium on}, pages={33--42}, year={2009}, organization={IEEE} }  Garnet2.0 provides a cycle-accurate micro-architectural implementation of an on-chip network router. It leverages the Topology and Routing infrastructure provided by gem5\u0026rsquo;s ruby memory system model. The default router is a state-of-the-art 1-cycle pipeline. There is support to add additional delay of any number of cycles in any router, by specifying it within the topology.\nGarnet2.0 can also be used to model an off-chip interconnection network by setting appropriate delays in the routers and links.\n Related Files:  src/mem/ruby/network/Network.py src/mem/ruby/network/garnet2.0/GarnetNetwork.py src/mem/ruby/network/Topology.cc   Invocation The garnet networks can be enabled by adding \u0026ndash;network=garnet2.0.\nConfiguration Garnet2.0 uses the generic network parameters in Network.py:\n - number_of_virtual_networks: This is the maximum number of virtual networks. The actual number of active virtual networks is determined by the protocol.  control_msg_size: The size of control messages in bytes. Default is 8. m_data_msg_size in Network.cc is set to the block size in bytes + control_msg_size.   Additional parameters are specified in garnet2.0/GarnetNetwork.py:\n - ni_flit_size: flit size in bytes. Flits are the granularity at which information is sent from one router to the other. Default is 16 (=\u0026gt; 128 bits). [This default value of 16 results in control messages fitting within 1 flit, and data messages fitting within 5 flits]. Garnet requires the ni_flit_size to be the same as the bandwidth_factor (in network/BasicLink.py) as it does not model variable bandwidth within the network. This can also be set from the command line with \u0026ndash;link-width-bits.  vcs_per_vnet: number of virtual channels (VC) per virtual network. Default is 4. This can also be set from the command line with \u0026ndash;vcs-per-vnet. buffers_per_data_vc: number of flit-buffers per VC in the data message class. Since data messages occupy 5 flits, this value can lie between 1-5. Default is 4. buffers_per_ctrl_vc: number of flit-buffers per VC in the control message class. Since control messages occupy 1 flit, and a VC can only hold one message at a time, this value has to be  Default is 1.  routing_algorithm: 0: Weight-based table (default), 1: XY, 2: Custom. More details below.   Topology Garnet2.0 leverages the Topology infrastructure provided by gem5\u0026rsquo;s ruby memory system model. Any heterogeneous topology can be modeled. Each router in the topology file can be given an independent latency, which overrides the default. In addition, each link has 2 optional parameters: src_outport and dst_inport, which are strings with names of the output and input ports of the source and destination routers for each link. These can be used inside garnet2.0 to implement custom routing algorithms, as described next. For instance, in a Mesh, the west to east links have src_outport set to \u0026ldquo;west\u0026rdquo; and dst_inport\u0026rdquo; set to \u0026ldquo;east\u0026rdquo;.\n Network Components:  GarnetNetwork: This is the top level object that instantiates all network interfaces, routers, and links. Topology.cc calls the methods to add \u0026ldquo;external links\u0026rdquo; between NIs and routers, and \u0026ldquo;internal links\u0026rdquo; between routers. NetworkInterface: Each NI connects to one coherence controller via MsgBuffer interfaces on one side. It has a link to a router on the other. Every protocol message is put into a one-flit control or multi (default=5)-flit data (depending on its vnet), and injected into the router. Multiple NIs can connect to the same router (for e.g., in the Mesh topology, cache and dir controllers connect via individual NIs to the same router). Router: The router manages arbitration for output links, and flow control between routers. NetworkLink: Network links carry flits. They can be of one of 3 types: EXTOUT (router to NI), EXTIN (NI to router), and INT_ (internal router to router) CreditLink: Credit links carry VC/buffer credits between routers for flow control.   Routing Garnet2.0 leverages the Routing infrastructure provided by gem5\u0026rsquo;s ruby memory system model. The default routing algorithm is a deterministic table-based routing algorithm with shortest paths. Link weights can be used to prioritize certain links over others. See src/mem/ruby/network/Topology.cc for details about how the routing table is populated.\nCustom Routing: To model custom routing algorithms, say adaptive, we provide a framework to name each link with a src_outport and dst_inport direction, and use these inside garnet to implement routing algorithms. For instance, in a Mesh, West-first can be implemented by sending a flit along the \u0026ldquo;west\u0026rdquo; outport link till the flit no longer has any X- hops remaining, and then randomly (or based on next router VC availability) choosing one of the remaining links. See how outportComputeXY() is implemented in src/mem/ruby/network/garnet2.0/RoutingUnit.cc. Similarly, outportComputeCustom() can be implemented, and invoked by adding \u0026ndash;routing-algorithm=2 in the command line.\nMulticast messages: The network modeled does not have hardware multi-cast support within the network. A multi-cast message gets broken into multiple uni-cast messages at the Network Interface.\nFlow Control Virtual Channel Flow Control is used in the design. Each VC can hold one packet. There are two kinds of VCs in the design - control and data. The buffer depth in each can be independently controlled from GarnetNetwork.py. The default values are 1-flit deep control VCs, and 4-flit deep data VCs. Default size of control packets is 1-flit, and data packets is 5-flit.\nRouter Microarchitecture The garnet2.0 router performs the following actions:\n Buffer Write (BW): The incoming flit gets buffered in its VC. Route Compute (RC) The buffered flit computes its output port, and this information is stored in its VC. Switch Allocation (SA): All buffered flits try to reserve the switch ports for the next cycle. [The allocation occurs in a separable manner: First, each input chooses one input VC, using input arbiters, which places a switch request. Then, each output port breaks conflicts via output arbiters]. All arbiters in ordered virtual networks are queueing to maintain point-to-point ordering. All other arbiters are round-robin. VC Selection (VS): The winner of SA selects a free VC (if HEAD/HEAD_TAIL flit) from its output port. Switch Traversal (ST): Flits that won SA traverse the crossbar switch. Link Traversal (LT): Flits from the crossbar traverse links to reach the next routers.  In the default design, BW, RC, SA, VS, and ST all happen in 1-cycle. LT happens in the next cycle.\nMulti-cycle Router: Multi-cycle routers can be modeled by specifying a per-router latency in the topology file, or changing the default router latency in src/mem/ruby/network/BasicRouter.py. This is implemented by making a buffered flit wait in the router for (latency-1) cycles before becoming eligible for SA.\nBuffer Management Each router input port has number_of_virtual_networks Vnets, each with vcs_per_vnet VCs. VCs in control Vnets have a depth of buffers_per_ctrl_vc (default = 1) and VCs in data Vnets have a depth of buffers_per_data_vc (default = 4). Credits are used to relay information about free VCs, and number of buffers within each VC.\nLifecycle of a Network Traversal  NetworkInterface.cc::wakeup()  Every NI connected to one coherence protocol controller on one end, and one router on the other. receives messages from coherence protocol buffer in appropriate vnet and converts them into network packets and sends them into the network.  garnet2.0 adds the ability to capture a network trace at this point [under development].  receives flits from the network, extracts the protocol message and sends it to the coherence protocol buffer in appropriate vnet. manages flow-control (i.e., credits) with its attached router. The consuming flit/credit output link of the NI is put in the global event queue with a timestamp set to next cycle. The eventqueue calls the wakeup function in the consumer.    NetworkLink.cc::wakeup()  receives flits from NI/router and sends it to NI/router after m_latency cycles delay Default latency value for every link can be set from command line (see configs/network/Network.py) Per link latency can be overwritten in the topology file The consumer of the link (NI/router) is put in the global event queue with a timestamp set after m_latency cycles. The eventqueue calls the wakeup function in the consumer.    Router.cc::wakeup()  Loop through all InputUnits and call their wakeup() Loop through all OutputUnits and call their wakeup() Call SwitchAllocator\u0026rsquo;s wakeup() Call CrossbarSwitch\u0026rsquo;s wakeup() The router\u0026rsquo;s wakeup function is called whenever any of its modules (InputUnit, OutputUnit, SwitchAllocator, CrossbarSwitch) have a ready flit/credit to act upon this cycle.    InputUnit.cc::wakeup()  Read input flit from upstream router if it is ready for this cycle For HEAD/HEAD_TAIL flits, perform route computation, and update route in the VC. Buffer the flit for (m_latency - 1) cycles and mark it valid for SwitchAllocation starting that cycle.  Default latency for every router can be set from command line (see configs/network/Network.py) Per router latency (i.e., num pipeline stages) can be set in the topology file.     OutputUnit.cc::wakeup()  Read input credit from downstream router if it is ready for this cycle Increment the credit in the appropriate output VC state. Mark output VC as free if the credit carries is_free_signal as true    SwitchAllocator.cc::wakeup()  Note: SwitchAllocator performs VC arbitration and selection within it. SA-I (or SA-i): Loop through all input VCs at every input port, and select one in a round robin manner.  For HEAD/HEAD_TAIL flits only select an input VC whose output port has at least one free output VC. For BODY/TAIL flits, only select an input VC that has credits in its output VC.  Place a request for the output port from this VC. SA-II (or SA-o): Loop through all output ports, and select one input VC (that placed a request during SA-I) as the winner for this output port in a round robin manner.  For HEAD/HEAD_TAIL flits, perform outvc allocation (i.e., select a free VC from the output port. For BODY/TAIL flits, decrement a credit in the output vc.  Read the flit out from the input VC, and send it to the CrossbarSwitch Send a increment_credit signal to the upstream router for this input VC.  for HEAD_TAIL/TAIL flits, mark is_free_signal as true in the credit. The input unit sends the credit out on the credit link to the upstream router.  Reschedule the Router to wakeup next cycle for any flits ready for SA next cycle.    CrossbarSwitch.cc::wakeup()  Loop through all input ports, and send the winning flit out of its output port onto the output link. The consuming flit output link of the router is put in the global event queue with a timestamp set to next cycle. The eventqueue calls the wakeup function in the consumer.    NetworkLink.cc::wakeup()  receives flits from NI/router and sends it to NI/router after m_latency cycles delay Default latency value for every link can be set from command line (see configs/network/Network.py) Per link latency can be overwritten in the topology file The consumer of the link (NI/router) is put in the global event queue with a timestamp set after m_latency cycles. The eventqueue calls the wakeup function in the consumer.   Running Garnet2.0 with Synthetic Traffic Garnet2.0 can be run in a standalone manner and fed with synthetic traffic. The details are described here: Garnet Synthetic Traffic\n"
},
{
	"uri": "http://localhost/ruby/garnet-standalone/",
	"title": "Garnet standalone",
	"tags": [],
	"description": "",
	"content": " This is a dummy cache coherence protocol that is used to operate Garnet in a standalone manner. This protocol works in conjunction with the Garnet Synthetic Traffic injector.\nRelated Files  src/mem/protocols  Garnet_standalone-cache.sm: cache controller specification Garnet_standalone-dir.sm: directory controller specification Garnet_standalone-msg.sm: message type specification Garnet_standalone.slicc: container file   Cache Hierarchy This protocol assumes a 1-level cache hierarchy. The role of the cache is to simply send messages from the cpu to the appropriate directory (based on the address), in the appropriate virtual network (based on the message type). It does not track any state. Infact, no CacheMemory is created unlike other protocols. The directory receives the messages from the caches, but does not send any back. The goal of this protocol is to enable simulation/testing of just the interconnection network.\nStable States and Invariants    States Invariants     I Default state of all cache blocks    Cache controller  Requests, Responses, Triggers:  Load, Instruction fetch, Store from the core.   The network tester (in src/cpu/testers/networktest/networktest.cc) generates packets of the type ReadReq, INST_FETCH, and WriteReq, which are converted into RubyRequestType:LD, RubyRequestType:IFETCH, and RubyRequestType:ST, respectively, by the RubyPort (in src/mem/ruby/system/RubyPort.hh/cc). These messages reach the cache controller via the Sequencer. The destination for these messages is determined by the traffic type, and embedded in the address. More details can be found here.\n Main Operation:  The goal of the cache is only to act as a source node in the underlying interconnection network. It does not track any states. On a LD from the core:  it returns a hit, and maps the address to a directory, and issues a message for it of type MSG, and size Control (8 bytes) in the request vnet (0). Note: vnet 0 could also be made to broadcast, instead of sending a directed message to a particular directory, by uncommenting the appropriate line in the a_issueRequest action in Network_test-cache.sm  On a IFETCH from the core:  it returns a hit, and maps the address to a directory, and issues a message for it of type MSG, and size Control (8 bytes) in the forward vnet (1).  On a ST from the core:  it returns a hit, and maps the address to a directory, and issues a message for it of type MSG, and size Data (72 bytes) in the response vnet (2).  Note: request, forward and response are just used to differentiate the vnets, but do not have any physical significance in this protocol.   Directory controller  Requests, Responses, Triggers:  MSG from the cores    Main Operation:  The goal of the directory is only to act as a destination node in the underlying interconnection network. It does not track any states. The directory simply pops its incoming queue upon receiving the message.   Other features  - This protocol assumes only 3 vnets.  It should only be used when running Garnet Synthetic Traffic.   "
},
{
	"uri": "http://localhost/contributing/governance/",
	"title": "Governance",
	"tags": [],
	"description": "",
	"content": " Overview gem5 is a meritocratic, consensus-based community project. Anyone with an interest in the project can join the community, contribute to the project design and participate in the decision-making process. Historically, gem5 development has been carried out both in industry and in academia. This document describes how that participation takes place and how to set about earning merit within the project community.\nThe document is broken into a number of sections. Philosophy describes the ideas behind the gem5 community. The Roadmap section points to the roadmap document for gem5’s development. Users and Responsibilities describes the classes of users that use gem5, the types of gem5 contributors, and their responsibilities. Support describes how the community supports users and the Contribution process describes how to contribute. Finally, the Decision Process describes how decisions are made and then we conclude.\nPhilosophy The goal of gem5 is to provide a tool to further the state of the art in computer architecture. gem5 can be used for (but is not limited to) computer-architecture research, advanced development, system-level performance analysis and design-space exploration, hardware-software co-design, and low-level software performance analysis. Another goal of gem5 is to be a common framework for computer architecture. A common framework in the academic community makes it easier for other researchers to share workloads as well as models and to compare and contrast with other architectural techniques.\nThe gem5 community strives to balance the needs of its three user types (academic researchers, industry researchers, and students, detailed below). For instance, gem5 strives to balance adding new features (important to researchers) and a stable code base (important for students). Specific user needs important to the community are enumerated below:\n Effectively and efficiently emulate the behavior of modern processors in a way that balances simulation performance and accuracy Serve as a malleable baseline infrastructure that can easily be adapted to emulate the desired behaviors Provide a core set of APIs and features that remain relatively stable Incorporate features that make it easy for companies and research groups to stay up to date with the tip and continue contributing to the project  Additionally, the gem5 community is committed to openness, transparency, and inclusiveness. Participants in the gem5 community of all backgrounds should feel welcome and encouraged to contribute.\ngem5 Roadmap The roadmap for gem5 can be found on Roadmap page. The roadmap document details the short and long term goals for the gem5 software. Users of all types are encouraged to contribute to this document and shape the future of gem5. Users are especially encouraged to update the roadmap (and get consensus) before submitting large changes to gem5.\nRoles And Responsibilities Users Users are community members who have a need for the project. They are the most important members of the community and without them the project would have no purpose. Anyone can be a user; there are no special requirements. There are currently three main categories of gem5 users: academic researchers, industry researchers, and students. Individuals may transition between categories, e.g., when a graduate student takes an industry internship, then returns to school; or when a student graduates and takes a job in industry. These three users are described below.\nAcademic Researchers This type of user primarily encompasses individuals that use gem5 in academic research. Examples include, but are not limited to, graduate students, research scientists, and post-graduates. This user often uses gem5 as a tool to discover and invent new computer architecture mechanisms. Academic Researchers often are first exposed to gem5 as Students (see below) and transition from Students to Academic Researchers over time.\nBecause of these users’ goals, they primarily add new features to gem5. It is important to the gem5 community to encourage these users to contribute their work to the mainline gem5 repository. By encouraging these users to commit their research contributions, gem5 will make it much easier for other researchers to compare and contrast with other architectural techniques (see Philosophy section).\nIndustry Researchers This type of user primarily encompasses individuals working for companies that use gem5. These users are distinguished from academic researchers in two ways. First, industry researchers are often part of a larger team, rather than working individually on gem5. Second, industry researchers often want to incorporate proprietary information into private branches of gem5. Therefore, industry researchers tend to have rather sophisticated software infrastructures built around gem5. For these users, the stability of gem5 features and baseline source code is important. Another key consideration is the fidelity of the models, and their ability to accurately reflect realistic implementations. To enable industry participation, it is critical to maintain licensing terms that do not restrict or burden the use of gem5 in conjunction with proprietary IP.\nStudents This type of user primarily encompasses individuals that are using gem5 in a classroom setting. These users typically have some foundation in computer architecture, but they have little or no background using simulation tools. Additionally, these users may not use gem5 for an extended period of time, after finishing their short-term goals (e.g., a semester-long class).\nThe project asks its users to participate in the project and community as much as possible. User contributions enable the project team to ensure that they are satisfying the needs of those users. Common user contributions include (but are not limited to):\n evangelising about the project (e.g., a link on a website and word-of-mouth awareness raising) informing developers of strengths and weaknesses from a new user perspective providing moral support (a ‘thank you’ goes a long way) providing financial support (the software is open source, but its developers need to eat)  Users who continue to engage with the project and its community will often become more and more involved. Such users may find themselves becoming contributors, as described in the next section.\nContributors Contributors are community members who contribute in concrete ways to the project. Anyone can become a contributor, and contributions can take many forms. There are no specific skill requirements and no selection process.\n There is only one expectation of commitment to the project: contributors must be respectful to each other during the review process and work together to reach compromises. See the “Reviewing Patches” section for more on the process of contributing.\n In addition to their actions as users, contributors may also find themselves doing one or more of the following:\n answering questions on the mailing lists, particularly the “easy” questions from new users (existing users are often the best people to support new users), or those that relate to the particular contributor’s experiences reporting bugs identifying requirements providing graphics and web design programming assisting with project infrastructure writing documentation fixing bugs adding features acting as an ambassador and helping to promote the project  Contributors engage with the project through the Review Board and mailing list, or by writing or editing documentation. They submit changes to the project source code via patches submitted to Review Board, which will be considered for inclusion in the project by existing committers (see next section). The developer mailing list is the most appropriate place to ask for help when making that first contribution.\nAs contributors gain experience and familiarity with the project, their profile within, and commitment to, the community will increase. At some stage, they may find themselves being nominated for committership.\nCommitters Committers are community members who have shown that they are committed to the continued development of the project through ongoing engagement with the community. Committership allows contributors to more easily carry on with their project related activities by giving them direct access to the project’s resources. That is, they can make changes directly to project outputs, although they still have to submit code changes via Review Board. Additionally, committers are expected to have an ongoing record of contributions in terms of code, reviews, and/or discussion.\nCommitters have no more authority over the project than contributors. While committership indicates a valued member of the community who has demonstrated a healthy respect for the project’s aims and objectives, their work continues to be reviewed by the community. The key difference between a committer and a contributor is committers have the extra responsibility of pushing patches to the mainline. Additionally, committers are expected to contribute to discussions on the gem5-dev list and review patches.\nAnyone can become a committer. The only expectation is that a committer has demonstrated an ability to participate in the project as a team player. Specifically, refer to the 2nd paragraph of the Contributors section.\nTypically, a potential committer will need to show that they have an understanding of the project, its objectives and its strategy (see Philosophy section). They will also have provided valuable contributions to the project over a period of time.\nNew committers can be nominated by any existing committer. Once they have been nominated, there will be a vote by the project management committee (PMC; see below). Committer nomination and voting is one of the few activities that takes place on the project’s private management list. This is to allow PMC members to freely express their opinions about a nominee without causing embarrassment. Once the vote has been held, the nominee is notified of the result. The nominee is entitled to request an explanation of any ‘no’ votes against them, regardless of the outcome of the vote. This explanation will be provided by the PMC Chair (see below) and will be anonymous and constructive in nature.\nNominees may decline their appointment as a committer. However, this is unusual, as the project does not expect any specific time or resource commitment from its community members. The intention behind the role of committer is to allow people to contribute to the project more easily, not to tie them into the project in any formal way.\nIt is important to recognise that commitership is a privilege, not a right. That privilege must be earned and once earned it can be removed by the PMC (see next section) in extreme circumstances. However, under normal circumstances committership exists for as long as the committer wishes to continue engaging with the project.\nA committer who shows an above-average level of contribution to the project, particularly with respect to its strategic direction and long-term health, may be nominated to become a member of the PMC. This role is described below.\nProject management committee The project management committee consists of those individuals identified as ‘project owners’ on the development site. The PMC has additional responsibilities over and above those of a committer. These responsibilities ensure the smooth running of the project. PMC members are expected to review code contributions, participate in strategic planning, approve changes to the governance model and manage how the software is distributed and licensed.\nSome PMC members are responsible for specific components of the gem5 project. This includes gem5 source modules (e.g., classic caches, O3CPU model, etc.) and project assets (e.g., the website). A list of the current components and the responsible members can be found on Module owners.\nMembers of the PMC do not have significant authority over other members of the community, although it is the PMC that votes on new committers. It also makes decisions when community consensus cannot be reached. In addition, the PMC has access to the project’s private mailing list. This list is used for sensitive issues, such as votes for new committers and legal matters that cannot be discussed in public. It is never used for project management or planning.\nMembership of the PMC is by invitation from the existing PMC members. A nomination will result in discussion and then a vote by the existing PMC members. PMC membership votes are subject to consensus approval of the current PMC members. Additions to the PMC require unanimous agreement of the PMC members. Removing someone from the PMC requires N-1 positive votes, where N is the number of PMC members not including the individual who is being voted out.\nMembers  Ali Saidi Andreas Hansson Andreas Sandberg Anthony Gutierrez Brad Beckmann Jason Lowe-Power Nathan Binkerg Steve Reinhardt  PMC Chair The PMC Chair is a single individual, voted for by the PMC members. Once someone has been appointed Chair, they remain in that role until they choose to retire, or the PMC casts a two-thirds majority vote to remove them.\nThe PMC Chair has no additional authority over other members of the PMC: the role is one of coordinator and facilitator. The Chair is also expected to ensure that all governance processes are adhered to, and has the casting vote when any project decision fails to reach consensus.\nSupport All participants in the community are encouraged to provide support for new users within the project management infrastructure. This support is provided as a way of growing the community. Those seeking support should recognise that all support activity within the project is voluntary and is therefore provided as and when time allows.\nContribution Process Anyone, capable of showing respect to others, can contribute to the project, regardless of their skills, as there are many ways to contribute. For instance, a contributor might be active on the project mailing list and issue tracker, or might supply patches. The various ways of contributing are described in more detail in a separate document Submitting Contributions.\nThe developer mailing list is the most appropriate place for a contributor to ask for help when making their first contribution. See the Submitting Contributions page on the gem5 wiki for details of the gem5 contribution process. Each new contribution should be submitted as a patch to our Review Board site. Then, other gem5 developers will review your patch, possibly asking for minor changes. After the patch has received consensus (see Decision Making Process), the patch is ready to be committed to the gem5 tree. For committers, this is as simple as pushing the changeset. For contributors, a committer should push the changeset for you. If a committer does not push the changeset within a reasonable window (a couple of days), send a friendly reminder email to the gem5-dev list. Before a patch is committed to gem5, it must receive at least 2 \u0026ldquo;Ship its\u0026rdquo; from reviewboard. If there are no reviews on a patch, users should send follow up emails to the gem5-dev list asking for reviews.\nReviewing Patches  An important part of the contribution process is providing feedback on patches that other developers submit. The purpose of reviewing patches is to weed out obvious bugs and to ensure that the code in gem5 is of sufficient quality.\nAll users are encouraged to review the contributions that are posted on Review Board. If you are an active gem5 user, it\u0026rsquo;s a good idea to keep your eye on the contributions that are posted there (typically by subscribing to the gem5-dev mailing list) so you can speak up when you see a contribution that could impact your use of gem5. It is far more effective to contribute your opinion in a review before a patch gets committed than to complain after the patch is committed, you update your repository, and you find that your simulations no longer work.\nWe greatly value the efforts of reviewers to maintain gem5\u0026rsquo;s code quality and consistency. However, it is important that reviews balance the desire to maintain the quality of the code in gem5 with the need to be open to accepting contributions from a broader community. People will base their desire to contribute (or continue contributing) on how they and other contributors are received. With that in mind, here are some guidelines for reviewers:\n Remember that submitting a contribution is a generous act, and is very rarely a requirement for the person submitting it. It\u0026rsquo;s always a good idea to start a review with something like “thank you for submitting this contribution”. A thank-you is particularly important for new or occasional submitters. Overall, the attitude of a reviewer should be “how can we take this contribution and put it to good use”, not “what shortcomings in this work must the submitter address before the contribution can be considered worthy”. As the saying goes, “the perfect is the enemy of the good”. While we don\u0026rsquo;t want gem5 to deteriorate, we also don\u0026rsquo;t want to bypass useful functionality or improvements simply because they are not optimal. If the optimal solution is not likely to happen, then accepting a suboptimal solution may be preferable to having no solution. A suboptimal solution can always be replaced by the optimal solution later. Perhaps the suboptimal solution can be incrementally improved to reach that point. When asking a submitter for additional changes, consider the cost-benefit ratio of those changes. In particular, reviewers should not discount the costs of requested changes just because the cost to the reviewer is near zero. Asking for extensive changes, particularly from someone who is not a long-time gem5 developer, may be imposing a significant burden on someone who is just trying to be helpful by submitting their code. If you as a reviewer really feel that some extensive reworking of a patch is necessary, consider volunteering to make the changes yourself. Not everyone uses gem5 in the same way or has the same needs. It\u0026rsquo;s easy to reject a solution due to its flaws when it solves a problem you don\u0026rsquo;t have—so there\u0026rsquo;s no loss to you if we end up with no solution. That\u0026rsquo;s probably not an acceptable result for the person submitting the patch though. Another way to look at this point is as the flip side of the previous item: just as your cost-benefit analysis should not discount the costs to the submitter of making changes, just because the costs to you are low, it should also not discount the benefits to the submitter of accepting the submission, just because the benefits to you are low. Be independent and unbiased while commenting on review requests. Do not support a patch just because you or your organization will benefit from it or oppose it because you will need to do more work. Whether you are an individual or someone working with an organization, think about the patch from community’s perspective. Try to keep the arguments technical and the language simple. If you make some claim about a patch, substantiate it.  Decision Making Process Decisions about the future of the project are made through discussion with all members of the community, from the newest user to the most experienced PMC member. All non-sensitive project management discussion takes place on the gem5-dev mailing list. Occasionally, sensitive discussion occurs on a private list.\nIn order to ensure that the project is not bogged down by endless discussion and continual voting, the project operates a policy of lazy consensus. This allows the majority of decisions to be made without resorting to a formal vote.\nLazy consensus Decision making typically involves the following steps:\n Proposal Discussion Vote (if consensus is not reached through discussion) Decision  Any community member can make a proposal for consideration by the community. In order to initiate a discussion about a new idea, they should send an email to the gem5-dev list or submit a patch implementing the idea to Review Board. This will prompt a review and, if necessary, a discussion of the idea. The goal of this review and discussion is to gain approval for the contribution. Since most people in the project community have a shared vision, there is often little need for discussion in order to reach consensus.\nIn general, as long as nobody explicitly opposes a proposal, it is recognised as having the support of the community. This is called lazy consensus—that is, those who have not stated their opinion explicitly have implicitly agreed to the implementation of the proposal.\nLazy consensus is a very important concept within the project. It is this process that allows a large group of people to efficiently reach consensus, as someone with no objections to a proposal need not spend time stating their position, and others need not spend time reading such mails.\nFor lazy consensus to be effective, it is necessary to allow at least two weeks before assuming that there are no objections to the proposal. This requirement ensures that everyone is given enough time to read, digest and respond to the proposal. This time period is chosen so as to be as inclusive as possible of all participants, regardless of their location and time commitments. For Review Board requests, if there are no reviews after two weeks, the submitter should send a reminder email to the mailing list. Reviewers may ask patch submitters to delay submitting a patch when they have a desire to review a patch and need more time to do so. As discussed in the Contributing Section, each patch should have at least two \u0026ldquo;Ship its\u0026rdquo; before it is committed.\nVoting Not all decisions can be made using lazy consensus. Issues such as those affecting the strategic direction or legal standing of the project must gain explicit approval in the form of a vote. Every member of the community is encouraged to express their opinions in all discussion and all votes. However, only project committers and/or PMC members (as defined above) have binding votes for the purposes of decision making. A separate document on the voting within a meritocratic governance model (http://oss-watch.ac.uk/resources/meritocraticgovernancevoting) describes in more detail how voting is conducted in projects following the practice established within the Apache Software Foundation.\nThis document is based on the example (http://oss-watch.ac.uk/resources/meritocraticgovernancemodel) by Ross Gardler and Gabriel Hanganu and is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License\n"
},
{
	"uri": "http://localhost/deprecated/hetrogenous/",
	"title": "Heterogenous Systems",
	"tags": [],
	"description": "",
	"content": "(from Gabe)\nI was thinking about how to set up m5 to support other architectures, and decision which has a major impact on that is whether we want to support heterogeneous systems. Here are the arguements I saw for and against.\nAgainst\nAdding support for heterogeneous systems will be a big change. There are many systems of m5 that aren\u0026rsquo;t ready to support something that\u0026rsquo;s not alpha, let alone two things. If there are heterogeneous systems, the code which implements each must be present. That means that a system has to be in place to generate the appropriate pieces, which could be using templates, namespaces, or setting up standard interfaces and using a building block style approach. In all cases, this would mean carefully and totally cutting the pieces apart from one another, since it can\u0026rsquo;t be assumed that any one thing will be there.\nFor\nAllowing heterogeneous systems will allow simulation of mixed systems, such as a satellite embedded device talking to a server implemented on, for instance, MIPS and SPARC architectures respectively. Another setup which also becomes possible are asymmetric multiprocessing systems which use different ISAs for the different computing elements. I believe the Cell processor is an example of this. Having a heterogeneous architecture won\u0026rsquo;t be as hard to implement once there have been sufficient changes to allow non-alpha architectures at all. I would estimate that the majority of the work involved will be to remove the alphacentric nature of existing code. Code set up in this way should be more modular and non-specialized, which would lead to better implementation overall.\nMy thoughts\nI favor implementing a system which supports hetergeneous architectures, since that will be almost the same thing as supporting other architectures at all. I think being able to support multi ISA asymmetric multiprocessor systems would be a useful feature. Also, modularizing the code fully would make m5 more usable for us, and more approachable for people who want to use it for other things.\nI thought this was a settled issue: as we fix all the things that need to be fixed to enable multiple ISAs with a single ISA selected at compile time, we will:\n not consciously do anything that makes it unreasonably difficult to add support for heterogeneous systems, and make specific features support heterogeneous systems if the amount of work is equivalent to or only slightly more than not adding that support, but not commit to supporting heterogeneous systems fully at this time since we don\u0026rsquo;t need it. That is, if there are features that don\u0026rsquo;t fall under rule #2, (they\u0026rsquo;re needed solely for heterogeneous systems and/or are significantly more effort), then we\u0026rsquo;re not going to do them right now because we have more important things to work on.  But if there aren\u0026rsquo;t any necessary features that fall under rule #3, then it\u0026rsquo;s possible that heterogeneous system support will just fall out.\nSteve\nThe problem is that there is a possible contradiction between the first and second points. There very well may be situations where we\u0026rsquo;ll need to go out of our way to make heterogeneous systems not require a lot of effort later. For example, the ISA compilation strategy you outlined won\u0026rsquo;t work for hetergenous systems since the switch header will point to only one ISA. A system built around that will be hard to change later. The direction things are headed seems to be to not worry about heterogeneous systems, which is ok by me, but we should resolve which of those two points is dominant.\nGabe\nAnything that\u0026rsquo;s ISA-specific will be encapsulated as a specific type that knows what ISA it belongs to. If we want to get fancier we can have the Python figure it out.\nFor example, FreeBSDSystem should have an AlphaFreeBSDSystem subclass to add on the Alpha-specific parts (via abstract virtual functions etc.).\n--141.213.120.65 16:03, 7 February 2006 (EST)\n"
},
{
	"uri": "http://localhost/developer/new-isa/",
	"title": "How to create an ISA",
	"tags": [],
	"description": "",
	"content": " arguments.hh Move out of isa?\nfaults.hh ISA specific fault classes to be used in the ISA.\nFunctions I\u0026rsquo;d like to get rid of\n genMachineCheckFault genAlignmentFault  interrupts.hh Interrupts class Interrupt Level Shouldn\u0026rsquo;t be capitalized. Gets a request to set the interrupt level and returns what the interrupt level was actually set to.\npost Request that an interrupt happen. Like asserting the interrupt line.\nclear Request that an interrupt not happen. Like deasserting a level triggered interrupt line.\nclear_all Breaks naming convention. Clear all interrupts\ncheck_interrupts Breaks naming convention. Not sure if this is right. Check to see if there are any interrupts pending\ngetInterrupt Get an interrupt fault object to invoke. The Interrupts object should internally prioritize interrupts so that they are handled in the right order.\nupdateIntrInfo Breaks naming convention. Don\u0026rsquo;t remember what this does. Need to find the email from Ali.\nget_vec Breaks naming convention. Returns the vector which describes the pending(?) interrupts in SPARC which is available as a (memory mapped?) register. This function panics in anything currently implemented other than SPARC, and it breaks the abstraction of the Interrupts object. Its existence allows necessary communication between two ISA specific components but goes through the ISA agnostic cpu. An argument for this function is that the cpu may need to modify the value returned by this function, so it needs to be able to intercept values and change them. This -really- breaks the abstraction of the Interrupts object by forcing the cpu to know what the vector means and what values are appropriate. This function needs to go away.\nserialize/unserialize Functions to serialize and unserialize the state in the Interrupts object.\nisa_traits.hh MachineBytes Not sure if this is right The number of bytes in a machine instruction? If that\u0026rsquo;s the case, this doesn\u0026rsquo;t make sense for x86 and may need to be handled differently.\nSet endianness A \u0026ldquo;using\u0026rdquo; directive which selects which endianness conversion functions are available.\ndelay slot define A preprocessor define which sets whether that architecture has delay slots. This needs to go away.\nNoopMachInst A machine instruction which encodes a noop\nDependenceTags Offsets which separate the different register types from one another in a flattened indexing space. It would be nice to phase these out.\nZeroReg The integer register which is defined by the architecture to always contain the value zero. Not all ISAs actually have such a register, so this needs to be done differently. This is simultaneously used as the offset of the integer zero register and the floating point zero register. This is correct in Alpha, but SPARC doesn\u0026rsquo;t have a zero floating point register, and not all architectures will necessarily have the same index for both registers.\nABI significant register indices It\u0026rsquo;s not clear how these should be defined for architectures like x86 which keep some of these values exclusively on the stack.\n StackPointerReg ReturnAddressReg ReturnValueReg FramePointerReg ArgumentReg0 ArgumentReg1 ArgumentReg2 ArgumentReg3 ArgumentReg4 ArgumentReg5  SyscallPseudoReturnReg Second syscall return register\nMaxInstSrcRegs Maybe have the isa parser generate this value? Maximum number of source registers used by any instruction.\nMaxInstDestRegs Maybe have the isa parser generate this value? Maximum number of destination registers used by any instruction.\nLogVMPageSize, VMPageSize Describes the size of a virtual memory page. Where is this used outside of translation? Where is translation not ISA specific? If there is no such place, maybe this should be defined internally to the ISA.\nSegKPMEnd SegKPBase Not sure what these are for. If they only make sense for one ISA or are used only in ISA specific places, they might not need to be part of the ISA interface\nPageShift PageBytes Not sure how this is different from VMPageSize and LogVMPageSize respectively.\nBranchPredAddrShiftAmt The number of bits a PC value should be shifted in order to provide a compacted instruction addressing space. This makes no sense for x86 or any other variable instruction width ISA.\ndecodeInst Prototype for the decodeInst function. Maybe this should be generated automatically? Not a big deal.\nFull system stuff LoadAddrMask Yes, a mask for the address used by loads, but to produce an address with what properties and why?\nTLB stuff Tlbs are ISA specific, so it\u0026rsquo;s probably a good idea to put these in a new ISA internal header file, or in the existing tlb.hh.\nInterruptTypes This might belong in sparc_traits.hh (SPARC specific ISA parameters), or in interrupt.hh. There should be an alpha_traits.hh for analogous Alpha specific ISA parameters\nkernel_stats.hh Not sure\nlocked_mem.hh handleLockedRead handleLockedWrite Not sure what these do, although I\u0026rsquo;m sure it has to do with LL/SC.\nmmaped_ipr.hh handleIprRead handleIprWrite These are for handling accesses to memory mapped registers. I believe IPR is an Alpha-ism and a different name might be more appropriate. Are these called from anywhere outside of ISA specific code? If not, they should be moved out of the ISA interface.\nprocess.hh \u0026rdquo; Need to document exactly what this does and how.\u0026rdquo; Defines an isa specific process object.\npredecoder.hh getTC setTC These should both be eliminated. A better mechanism should be used to attach a predecoder to a thread context, perhaps just passing one in to relevant functions.\nmoreBytes Feed more bytes to the predecoder.\nneedMoreBytes Returns whether or not the predecoder needs more bytes, or in other words, whether you -don\u0026rsquo;t- have left over data to start working with immediately.\nextMachInstReady Returns whether a whole ExtMachInst has been generated and is ready to be used.\ngetExtMachInst Returns the ExtMachInst the predecoder has generated.\nregfile.hh readPC/setPC Provide access to the current pc.\nreadNextPC/setNextPC Provide access to the next pc.\nreadNextNPC/setNextNPC Provide access to the next NPC. Most relevant for architectures with a delay slot.\nclear Blank register file contents.\nFlattenIntIndex Shouldn\u0026rsquo;t be capitalized. Should link to an explanation of this whole system. Turn an architected register index into a canonical one.\nreadMiscRegNoEffect/setMiscRegNoEffect Provides access to a misc register without triggering ISA defined side effects.\nreadMiscReg/setMiscReg Provides access to a misc register and triggers ISA defined side effects.\ninstAsid dataAsid Alpha specificish but hasn\u0026rsquo;t yet been removed.\nreadFloatReg setFloatReg Provides access to the floating point registers.\nreadFloatRegBits setFloatRegBits Provide access to the floating point registers as an integer. This allows writing/reading floating point values as text without introducing rounding error.\nremote_gdb.hh RemoteGDB object This might not be complete. Need to document these functions.\nacc getregs setregs clearSingleStep setSingleStep stacktrace.hh ProcessInfo Class to describe a process? Is this for Alpha\u0026rsquo;s process control blocks? (or whatever those are called)\nStackTrace syscallreturn.hh Should there be an underscore in this file name? Takes the generic SyscallReturn value and a thread context and sets up state as if a syscall was returning the value described by the SyscallReturn.\ntlb.hh lookup Looks up a tlb entry. Is this used only internally by the tlb?\nITLB DTLB translate Translates a request object\ndoMmuRegRead doMmuRegWrite For consistency, maybe these should be called readMmuReg and setMmuReg? Access an mmu control register\nGetTsbPtr Shouldn\u0026rsquo;t be capitalized Is this part of the interface? Should look again.\ntypes.hh Typedefs for the following types.\n MachInst ExtMachInst IntReg LargestRead MiscReg FloatReg FloatRegBits AnyReg   RegContextParam RegContextVal   RegIndex  utility.hh inUserMode Returns whether a thread context is operating in user mode. \u0026ldquo;User mode\u0026rdquo; is a little vague, but not too vague to be useful.\nis(Callee|Caller)Save(Integer|Float)Register Can these be purged? They don\u0026rsquo;t seem to be used by anything.\nrealPCToFetchPC fetchPCToRealPC What are these used for?\nfetchInstSize What is this used for? It doesn\u0026rsquo;t make sense for x86 regardless.\nzeroRegisters Writes zero to the \u0026ldquo;zero\u0026rdquo; registers. This really needs to go away, for efficiency\u0026rsquo;s sake if nothing else.\ninitCPU Initialize a cpu to the state it would have immediate after being powered on.\nstartupCPU Actually activate a cpu to start \u0026ldquo;doing something\u0026rdquo;.\nvtophys.hh kernel_pte_lookup Breaks naming convention. What does this do, precisely?\nvtophys In what context is this supposed to be used? SE? FS? Translates a virtual address to a physical one.\n"
},
{
	"uri": "http://localhost/developer/isa-code-parsing/",
	"title": "ISA code parsing",
	"tags": [],
	"description": "",
	"content": " To a large extent, the power and flexibility of the ISA description mechanism stem from the fact that the mapping from a brief instruction definition provided in the decode block to the resulting C++ code is performed in a general-purpose programming language (Python). (This function is performed by the \u0026ldquo;instruction format\u0026rdquo; definition described above in Format definitions.) Technically, the ISA description language allows any arbitrary Python code to perform this mapping. However, the parser provides a library of Python classes and functions designed to automate the process of deducing an instruction\u0026rsquo;s characteristics from a brief description of its operation, and generating the strings required to populate declaration and decode templates. This library represents roughly half of the code in isa_parser.py.\nInstruction behaviors are described using C++ with two extensions: bitfield operators and operand type qualifiers. To avoid building a full C++ parser into the ISA description system (or conversely constraining the C++ that could be used for instruction descriptions), these extensions are implemented using regular expression matching and substitution. As a result, there are some syntactic constraints on their usage. The following two sections discuss these extensions in turn. The third section discusses operand parsing, the technique by which the parser automatically infers most instruction characteristics. The final two sections discuss the Python classes through which instruction formats interact with the library: CodeBlock, which analyzes and encapsulates instruction description code; and the instruction object parameter class, InstObjParams, which encapsulates the full set of parameters to be substituted into a template.\nBitfield operators Simple bitfield extraction can be performed on rvalues using the \u0026lt;:\u0026gt; postfix operator. Bit numbering matches that used in global bitfield definitions (see Bitfield definitions). For example, Ra\u0026lt;7:0\u0026gt; extracts the low 8 bits of register Ra. Single-bit fields can be specified by eliminating the latter operand, e.g. Rb\u0026lt;31:\u0026gt;. Unlike in global bitfield definitions, the colon cannot be eliminated, as it becomes too difficult to distinguish bitfield operators from template arguments. In addition, the bit index parameters must be either identifiers or integer constants; expressions are not allowed. The bit operator will apply either to the syntactic token on its left, or, if that token is a closing parenthesis, to the parenthesized expression.\nOperand type qualifiers The effective type of an instruction operand (e.g., a register) may be specified by appending a period and a type qualifier to the operand name. The list of type qualifiers is architecture-specific; the def operand_types statement in the ISA description is used to specify it. The specification is in the form of a Python dictionary which maps a type extension to type name. For example, the Alpha ISA definition is as follows:\ndef operand_types {{ 'sb' : 'int8_t', 'ub' : 'uint8_t', 'sw' : 'int16_t', 'uw' : 'uint16_t', 'sl' : 'int32_t', 'ul' : 'uint32_t', 'sq' : 'int64_t', 'uq' : 'uint64_t', 'sf' : 'float', 'df' : 'double' }};  Thus the Alpha 32-bit add instruction addl could be defined as:\nRc.sl = Ra.sl + Rb.sl;  The operations are performed using the types specified; the result will be converted from the specified type to the appropriate register value (in this case by sign-extending the 32-bit result to 64 bits, since Alpha integer registers are 64 bits in size).\nType qualifiers are allowed only on recognized instruction operands (see #Instruction operands).\nInstruction operands Most of the automation provided by the parser is based on its recognition of the operands used in the instruction definition code. Most relevant instruction characteristics can be inferred from the operands: floating-point vs. integer instructions can be recognized by the registers used, an instruction that reads from a memory location is a load, etc. In combination with the bitfield operands and type qualifiers described above, most instructions can be described in a single line of code. In addition, most of the differences between simulator CPU models lies in the operand access mechanisms; by generating the code for these accesses automatically, a single description suffices for a variety of situations.\nThe ISA description provides a list of recognized instruction operands and their characteristics via the def operands statement. This statement specifies a Python dictionary that maps operand strings to a five-element tuple. The elements of the tuple specify the operand as follows:\n the operand class, which must be one of the strings \u0026ldquo;IntReg\u0026rdquo;, \u0026ldquo;FloatReg\u0026rdquo;, \u0026ldquo;Mem\u0026rdquo;, \u0026ldquo;NPC\u0026rdquo;, or \u0026ldquo;ControlReg\u0026rdquo;, indicating an integer register, floating-point register, memory location, the next program counter (NPC), or a control register, respectively. the default type of the operand (an extension string defined in the def operand_types block), a specifier indicating how specific instances of the operand are decoded (e.g., a bitfield name), a string or triple of strings indicating the instruction flags that can be inferred when the operand is used, and a sort priority used to control the order of operands in disassembly.  For example, a simplified subset of the Alpha ISA operand traits map is as follows:\ndef operands {{ 'Ra': ('IntReg', 'uq', 'RA', 'IsInteger', 1), 'Rb': ('IntReg', 'uq', 'RB', 'IsInteger', 2), 'Rc': ('IntReg', 'uq', 'RC', 'IsInteger', 3), 'Fa': ('FloatReg', 'df', 'FA', 'IsFloating', 1), 'Fb': ('FloatReg', 'df', 'FB', 'IsFloating', 2), 'Fc': ('FloatReg', 'df', 'FC', 'IsFloating', 3), 'Mem': ('Mem', 'uq', None, ('IsMemRef', 'IsLoad', 'IsStore'), 4), 'NPC': ('NPC', 'uq', None, ( None, None, 'IsControl'), 4) }};  The operand named Ra is an integer register, default type uq (unsigned quadword), uses the RA bitfield from the instruction, implies the IsInteger instruction flag, and has a sort priority of 1 (placing it first in any list of operands).\nFor the instrucion flag element, a single string (such as 'IsInteger' implies an unconditionally inferred instruction flag. If the flag operand is a triple, the first element is unconditional, the second is inferred when the operand is a source, and the third when it is a destination. Thus the ('IsMemRef', 'IsLoad', 'IsStore') element for memory references indicates that any instruction with a memory operand is marked as a memory reference. In addition, if the memory operand is a source, the instruction is marked as a load, while if the operand is a destination, the instruction is marked a store. Similarly, the (None, None, 'IsControl') tuple for the NPC operand indicates that any instruction that writes to the NPC is a control instruction, but instructions which merely reference NPC as a source do not receive any default flags.\nNote that description code parsing uses regular expressions, which limits the ability of the parser to infer the nature of a partciular operand. In particular, destination operands are distinguished from source operands solely by testing whether the operand appears on the left-hand side of an assignment operator (=). Destination operands that are assigned to in a different fashion, e.g. by being passed by reference to other functions, must still appear on the left-hand side of an assignment to be properly recognized as destinations. The parser also does not recognize C compound assignments, e.g., +=. If an operand is both a source and a destination, it must appear on both the left- and right-hand sides of =.\nAnother limitation of regular-expression-based code parsing is that control flow in the code block is not recognized. Combined with the details of how register updates are performed in the CPU models, this means that destinations cannot be updated conditionally. If a particular register is recognized as a destination register, that register will always be updated at the end of the execute() method, and thus the code must assign a valid value to that register along each possible code path within the block.\nThe CodeBlock class An instruction format requests processing of a string containing instruction description code by passing the string to the CodeBlock constructor. The constructor performs all of the needed analysis and processing, storing the results in the returned object. Among the CodeBlock fields are:\n orig_code: the original code string. code: a processed string containing legal C++ code, derived from the original code by substituting in the bitfield operators and munging operand type qualifiers (s/\\./_/) to make valid C++ identifiers. constructor: code for the constructor of an instruction object, initializing various C++ object fields including the number of operands and the register indices of the operands. exec_decl: code to declare the C++ variables corresponding to the operands, for use in an execution emulation function. *_rd: code to read the actual operand values into the corresponding C++ variables for source operands. The first part of the name indicates the relevant CPU model (currently simple and dtld are supported). *_wb: code to write the C++ variable contents back to the appropriate register or memory location. Again, the first part of the name reflects the CPU model. *_mem_rd, *_nonmem_rd, *_mem_wb, *_nonmem_wb: as above, but with memory and non-memory operands segregated. flags: the set of instruction flags implied by the operands. op_class: a basic guess at the instruction\u0026rsquo;s operation class (see OpClass) based on the operand types alone.  The InstObjParams class Instances of the InstObjParams class encapsulate all of the parameters needed to substitute into a code template, to be used as the argument to a template\u0026rsquo;s subst() method (see Template definitions).\nclass InstObjParams(object): def __init___(self, parser, mem, class_name, base_class = '', snippets = {}, opt_args = []):  The first three constructor arguments populate the object\u0026rsquo;s mnemonic, class_name, and (optionally) base_class members. The fourth (optional) argument is a CodeBlock object; all of the members of the provided CodeBlock object are copied to the new object, making them accessible for template substitution. Any remaining arguments are interpreted as either additional instruction flags (appended to the flags list inherited from the CodeBlock argument, if any), or as an operation class (overriding any op_class from the CodeBlock).\n"
},
{
	"uri": "http://localhost/developer/isa-description-system/",
	"title": "ISA description system",
	"tags": [],
	"description": "",
	"content": "The purpose of the M5 ISA description system is to generate a decoder function that takes a binary machine instruction and returns a C++ object representing that instruction. The returned object encapsulates all of the information (data and code) needed by the M5 simulator related to that specific machine instruction. By making the object as specific as possible to the machine instruction, the decoding overhead is paid only once; throughout the rest of the simulator, the object makes the needed instruction characteristics accessible easily and with low overhead.\nBecause a typical commercial ISA has hundreds of instructions, specifying the properties of each individual instruction in detail is tedious and error prone. The ISA description (as specified by the programmer) must take advantage of the fact that large classes of instructions share common characteristics. However, real ISAs invariably have not only several different instruction classes, but also a number of oddball instructions that defy categorization. The goal of the M5 ISA description system is to allow a human-readable ISA description that captures commonalities across sets of instructions while providing the flexibility to specify instructions with arbitrary characteristics. For example, once the structure is in place to define integer ALU instructions, the specification of simple instructions in that class (e.g., add or subtract) should constitute a single, readable, non-redundant line of the description file.\nM5 uses a custom, domain-specific language to achieve this goal. The language\u0026rsquo;s syntax and features are specifically designed to make instruction definitions compact and readable while minimizing redundancy. The language\u0026rsquo;s flexibility arises because the final code generation is performed by user-provided functions written in a general-purpose programming language (Python).\nAn ISA description written in this custom language is processed by a parser to generate several C++ files containing class definitions and the decode function. This file is in turn compiled into the M5 simulator.\nThe documentation for the ISA description system is divided into three pages:\n Static instruction objects provides a basic familiarity with the structure of these C++ objects, which are the final output of the decoding process. The goal of the ISA description system is to generate definitions for these objects, so you\u0026rsquo;ll need to understand them before continuing. The M5 ISA description language covers the description language proper. Code parsing describes a library of Python classes and functions that automate the process of deducing an instruction\u0026rsquo;s characteristics from a brief description of its operation. Although this library is not strictly part of the description language, it is used extensively by the Python code that generates the final C++ objects.  "
},
{
	"uri": "http://localhost/developer/isa-parser/",
	"title": "ISA parser",
	"tags": [],
	"description": "",
	"content": " The M5 ISA description language is a custom language designed specifically for generating the class definitions and decoder function needed by M5. This section provides a practical, informal overview of the language itself. A formal grammar for the language is embedded in the \u0026ldquo;yacc\u0026rdquo; portion of the parser (look for the functions starting with p_ in isa_parser.py). A second major component of the parser processes C-like code specifications to extract instruction characteristics; this aspect is covered in the section Code parsing. At the highest level, an ISA description file is divided into two parts: a declarations section and a decode section. The decode section specifies the structure of the decoder and defines the specific instructions returned by the decoder. The declarations section defines the global information (classes, instruction formats, templates, etc.) required to support the decoder. Because the decode section is the focus of the description file, we will begin the discussion there.\nThe decode section The decode section of the description is a set of nested decode blocks. A decode block specifies a field of a machine instruction to decode and the result to be provided for particular values of that field. A decode block is similar to a C switch statement in both syntax and semantics. In fact, each decode block in the description file generates a switch statement in the resulting decode function. Let\u0026rsquo;s begin with a (slightly oversimplified) example:\ndecode OPCODE { 0: add({{ Rc = Ra + Rb; }}); 1: sub({{ Rc = Ra - Rb; }}); }  A decode block begins with the keyword decode followed by the name of the instruction field to decode. The latter must be defined in the declarations section of the file using a bitfield definition (see #Bitfield definitions). The remainder of the decode block is a list of statements enclosed in braces. The most common statement is an integer constant and a colon followed by an instruction definition. This statement corresponds to a \u0026lsquo;case\u0026rsquo; statement in a C switch (but note that the \u0026lsquo;case\u0026rsquo; keyword is omitted for brevity). A comma-separated list of integer constants may be used to allow a single decode statement to apply to any of a set of bitfield values.\nInstruction definitions are similar in syntax to C function calls, with the instruction mnemonic taking the place of the function name. The comma-separated arguments are used when processing the instruction definition. In the example above, the instruction definitions each take a single argument, a code literal. A code literal is operationally similar to a string constant, but is delimited by double braces ({{ and }}). Code literals may span multiple lines without escaping the end-of-line characters. No backslash escape processing is performed (e.g., \\t is taken literally, and does not produce a tab). The delimiters were chosen so that C-like code contained in a code literal would be formatted nicely by emacs C-mode.\nA decode statement may specify a nested decode block in place of an instruction definition. In this case, if the bitfield specified by the outer block matches the given value(s), the bitfield specified by the inner block is examined and an additional switch is performed.\nIt is also legal, as in C, to use the keyword default in place of an integer constant to define a default action. However, it is more common to use the decode-block default syntax discussed in the section #Decode block defaults below.\nSpecifying instruction formats When the ISA description file is processed, each instruction definition does in fact invoke a function call to generate the appropriate C++ code for the decode file. The function that is invoked is determined by the instruction format. The instruction format determines the number and type of the arguments given to the instruction definition, and how they are processed to generate the corresponding output. Note that the term \u0026ldquo;instruction format\u0026rdquo; as used in this context refers solely to one of these definition-processing functions, and does not necessarily map one-to-one to the machine instruction formats defined by the ISA. The one oversimplification in the previous example is that no instruction format was specified. As a result, the parser does not know how to process the instruction definitions.\nInstruction formats can be specified in two ways. An explicit format specification can be given before the mnemonic, separated by a double colon (::), as follows:\ndecode OPCODE { 0: Integer::add({{ Rc = Ra + Rb; }}); 1: Integer::sub({{ Rc = Ra - Rb; }}); }  In this example, both instruction definitions will be processed using the format Integer. A more common approach specifies the format for a set of definitions using a format block, as follows:\ndecode OPCODE { format Integer { 0: add({{ Rc = Ra + Rb; }}); 1: sub({{ Rc = Ra - Rb; }}); } }  In this example, the format \u0026ldquo;Integer\u0026rdquo; applies to all of the instruction definitions within the inner braces. The two examples are thus functionally equivalent. There are few restrictions on the use of format blocks. A format block may include only a subset of the statements in a decode block. Format blocks and explicit format specifications may be mixed freely, with the latter taking precedence. Format and decode blocks can be nested within each other arbitrarily. Note that a closing brace will always bind with the nearest format or decode block, making it syntactically impossible to generate format or decode blocks that do not nest fully inside the enclosing block.\nAt any point where an instruction definition occurs without an explicit format specification, the format associated with the innermost enclosing format block will be used. If a definition occurs with no explicit format and no enclosing format block, a runtime error will be raised.\nDecode block defaults Default cases for decode blocks can be specified by default: labels, as in C switch statements. However, it is common in ISA descriptions that unspecified cases correspond to unknown or illegal instruction encodings. To avoid the requirement of a default: case in every decode block, the language allows an alternate default syntax that specifies a default case for the current decode block and any nested decode block with no explicit default. This alternate default is specified by giving the default keyword and an instruction definition after the bitfield specification (prior to the opening brace). Specifying the outermost decode block as follows:\ndecode OPCODE default Unknown::unknown() { [...] }  is thus (nearly) equivalent to adding default: Unknown::unknown(); inside every decode block that does not otherwise specify a default case.\n-\n*Note:* The appropriate format definition (see [\\#Format definitions](#Format_definitions \u0026quot;wikilink\u0026quot;)) is invoked each time an instruction definition is encountered. Thus there is a semantic difference between having a single block-level default and a default within each nested block, which is that the former will invoke the format definition once, while the latter could result in multiple invocations of the format definition. If the format definition generates header, decoder, or exec output, then that output will be included multiple times in the corresponding files, which typically leads to multiple definition errors when the C++ gets compiled. If it is absolutely necessary to invoke the format definition for a single instruction multiple times, the format definition should be written to produce *only* decode-block output, and all needed header, decoder, and exec output should be produced once using `output` blocks (see [\\#Output blocks](#Output_blocks \u0026quot;wikilink\u0026quot;)).  Preprocessor directive handling The decode block may also contain C preprocessor directives. These directives are not processed by the parser; instead, they are passed through to the C++ output to be processed when the C++ decoder is compiled. The parser does not recognize any specific directives; any line with a # in the first column is treated as a preprocessor directive. The directives are copied to all of the output streams (the header, the decoder, and the execute files; see #Format definitions). The directives maintain their position relative to the code generated by the instruction definitions within the decode block. The net result is that, for example, #ifdef/#endif pairs that surround a set of instruction definitions will enclose both the declarations generated by those definitions and the corresponding case statements within the decode function. Thus #ifdef and similar constructs can be used to delineate instruction definitions that will be conditionally compiled into the simulator based on preprocessor symbols (e.g., FULL_SYSTEM). It should be emphasized that #ifdef does not affect the ISA description parser. In an #ifdef/#else/#endif construct, all of the instruction definitions in both parts of the conditional will be processed. Only during the subsequent C++ compilation of the decoder will one or the other set of definitions be selected.\nThe declaration section As mentioned above, the decode section of the ISA description (consisting of a single outer decode block) is preceded by the declarations section. The primary purpose of the declarations section is to define the instruction formats and other supporting elements that will be used in the decode block, as well as supporting C++ code that is passed almost verbatim to the generated output. This section describes the components that appear in the declaration section: #Format definitions, #Template definitions, #Output blocks, #Let blocks, #Bitfield definitions, #Operand and operand type definitions, and #Namespace declaration.\nFormat definitions An instruction format is basically a Python function that takes the arguments supplied by an instruction definition (found inside a decode block) and generates up to four pieces of C++ code. The pieces of C++ code are distinguished by where they appear in the generated output.\n The header output goes in the header file (decoder.hh) that is included in all the generated source files (decoder.cc and all the per-CPU-model execute .cc files). The header output typically contains the C++ class declaration(s) (if any) that correspond to the instruction. The decoder output goes before the decode function in the same source file (decoder.cc). This output typically contains definitions that do not need to be visible to the execute() methods: inline constructor definitions, non-inline method definitions (e.g., for disassembly), etc. The exec output contains per-CPU model definitions, i.e., the execute() methods for the instruction class. The decode block contains a statement or block of statements that go into the decode function (in the body of the corresponding case statement). These statements take control once the bit pattern specified by the decode block is recognized, and are responsible for returning an appropriate instruction object.  The syntax for defining an instruction format is as follows:\ndef format FormatName(arg1, arg2) {{ [code omitted] }};  In this example, the format is named \u0026ldquo;FormatName\u0026rdquo;. (By convention, instruction format names begin with a capital letter and use mixed case.) Instruction definitions using this format will be expected to provide two arguments (arg1 and arg2). The language also supports the Python variable-argument mechanism: if the final parameter begins with an asterisk (e.g., *rest), it receives a list of all the otherwise unbound arguments from the call site.\nNote that the next-to-last syntactic token in the format definition (prior to the semicolon) is simply a code literal (string constant), as described above. In this case, the text within the code literal is a Python code block. This Python code will be called at each instruction definition that uses the specified format.\nIn addition to the explicit arguments, the Python code is supplied with two additional parameters: name, which is bound to the instruction mnemonic, and Name, which is the mnemonic with the first letter capitalized (useful for forming C++ class names based on the mnemonic).\nThe format code block specifies the generated code by assigning strings to four special variables: header_output, decoder_output, exec_output, and decode_block. Assignment is optional; for any of these variables that does not receive a value, no code will be generated for the corresponding section. These strings may be generated by whatever method is convenient. In practice, nearly all instruction formats use the support functions provided by the ISA description parser to specialize code templates based on characteristics extracted automatically from C-like code snippets. Discussion of these features is deferred to the Code parsing page.\nAlthough the ISA description is completely independent of any specific simulator CPU model, some C++ code (particularly the exec output) must be specialized slightly for each model. This specialization is handled by automatic substitution of CPU-model-specific symbols. These symbols start with CPU_ and are treated specially by the parser. Currently there is only one model-specific symbol, CPU_exec_context, which evaluates to the model\u0026rsquo;s execution context class name. As with templates (see #Template definitions), references to CPU-specific symbols use Python key-based format strings; a reference to the CPU_exec_context symbol thus appears in a string as %(CPU_exec_context)s.\nIf a string assigned to header_output, decoder_output, or decode_block contains a CPU-specific symbol reference, the string is replicated once for each CPU model, and each instance has its CPU-specific symbols substituted according to that model. The resulting strings are then concatenated to form the final output. Strings assigned to exec_output are always replicated and subsituted once for each CPU model, regardless of whether they contain CPU-specific symbol references. The instances are not concatenated, but are tracked separately, and are placed in separate per-CPU-model files (e.g., simple_cpu_exec.cc).\nTemplate definitions As discussed in section Format definitions above, the purpose of an instruction format is to process the arguments of an instruction definition and generate several pieces of C++ code. These code pieces are usually generated by specializing a code template. The description language provides a simple syntax for defining these templates: the keywords def template, the template name, the template body (a code literal), and a semicolon. By convention, template names start with a capital letter, use mixed case, and end with \u0026ldquo;Declare\u0026rdquo; (for declaration (header output) templates), \u0026ldquo;Decode\u0026rdquo; (for decode-block templates), \u0026ldquo;Constructor\u0026rdquo; (for decoder output templates), or \u0026ldquo;Execute\u0026rdquo; (for exec output templates). For example, the simplest useful decode template is as follows:\ndef template BasicDecode {{ return new %(class_name)s(machInst); }};  An instruction format would specialize this template for a particular instruction by substituting the actual class name for %(class_name)s. (Template specialization relies on the Python string format operator %. The term %(class_name)s is an extension of the C %s format string indicating that the value of the symbol class_name should be substituted.) The resulting code would then cause the C++ decode function to create a new object of the specified class when the particular instruction was recognized.\nTemplates are represented in the parser as Python objects. A template is used to generate a string typically by calling the template object\u0026rsquo;s subst() method. This method takes a single argument that specifies the mapping of substitution symbols in the template (e.g., %(class_name)s) to specific values. If the argument is a dictionary, the dictionary itself specifies the mapping. Otherwise, the argument must be another Python object, and the object\u0026rsquo;s attributes are used as the mapping. In practice, the argument to subst() is nearly always an instance of the parser\u0026rsquo;s InstObjParams class; see Code parsing#The InstObjParams class. A template may also reference other templates (e.g., %(BasicDecode)s) in addition to symbols specified by the subst() argument; these will be interpolated into the result by subst() as well.\nTemplate references to CPU-model-specific symbols (see #Format definitions) are not expanded by subst(), but are passed through intact. This feature allows them to later be expanded appropriately according to whether the result is assigned to exec_output or another output section. However, when a template containing a CPU-model-specific symbol is referenced by another template, then the former template is replicated and expanded into a single string before interpolation, as with templates assigned to header_output or decoder_output. This policy guarantees that only templates directly containing CPU-model-specific symbols will be replicated, never templates that contain such symbols indirectly. This last feature is used to interpolate per-CPU declarations of the execute() method into the instruction class declaration template (see the BasicExecDeclare template in the Alpha ISA description).\nOutput blocks Output blocks allow the ISA description to include C++ code that is copied nearly verbatim to the output file. These blocks are useful for defining classes and local functions that are shared among multiple instruction objects. An output block has the following format:\noutput \u0026lt;destination\u0026gt; {{ [code omitted] }};  The  keyword must be one of header, decoder, or exec. The code within the code literal is treated as if it were assigned to the header_output decoder_output, or exec_output variable within an instruction format, respectively, including the special processing of CPU-model-specific symbols. The only additional processing performed on the code literal is substitution of bitfield operators, as used in instruction definitions (see Code parsing#Bitfield operators), and interpolation of references to templates.\nLet blocks Let blocks provide for global Python code. These blocks consist simply of the keyword let followed by a code literal (double-brace delimited string) and a semicolon. The code literal is executed immediately by the Python interpreter. The parser maintains the execution context across let blocks, so that variables and functions defined in one let block will be accessible in subsequent let blocks. This context is also used when executing instruction format definitions. The primary purpose of let blocks is to define shared Python data structures and functions for use in instruction formats. The parser exports a limited set of definitions into this execution context, including the set of defined templates (see #Template definitions), the InstObjParams and CodeBlock classes (see Code parsing), and the standard Python string and re (regular expression) modules.\nBitfield definitions A bitfield definition provides a name for a bitfield within a machine instruction. These names are typically used as the bitfield specifications in decode blocks. The names are also used within other C++ code in the decoder file, including instruction class definitions and decode code. The bitfield definition syntax is demonstrated in these examples:\ndef bitfield OPCODE \u0026lt;31:26\u0026gt;; def bitfield IMM \u0026lt;12\u0026gt;; def signed bitfield MEMDISP \u0026lt;15:0\u0026gt;;  The specified bit range is inclusive on both ends, and bit 0 is the least significant bit; thus the OPCODE bitfield in the example extracts the most significant six bits from a 32-bit instruction. A single index value extracts a one-bit field, IMM. The extracted value is zero-extended by default; with the additional signed keyword, as in the MEMDISP example, the extracted value will be sign extended. The implementation of bitfields is based on preprocessor macros and C++ template functions, so the size of the resulting value will depend on the context.\nTo fully understand where bitfield definitions can be used, we need to go under the hood a bit. A bitfield definition simply generates a C++ preprocessor macro that extracts the specified bitfield from the implicit variable machInst. The machine instruction parameter to the decode function is also called machInst; thus any use of a bitfield name that ends up inside the decode function (such as the argument of a decode block or the decode piece of an instruction format\u0026rsquo;s output) will implicitly reference the instruction currently being decoded. The binary machine instruction stored in the StaticInst object is also named machInst, so any use of a bitfield name in a member function of an instruction object will reference this stored value. This data member is initialized in the StaticInst constructor, so it is safe to use bitfield names even in the constructors of derived objects.\nOperand and operand type definitions These statements specify the operand types that can be used in the code blocks that express the functional operation of instructions. See Code parsing#Operand type qualifiers and Code parsing#Instruction operands.\nNamespace declaration The final component of the declaration section is the namespace declaration, consisting of the keyword namespace followed by an identifier and a semicolon. Exactly one namespace declaration must appear in the declarations section. The resulting C++ decode function, the declarations resulting from the instruction definitions in the decode block, and the contents of any declare statements occurring after then namespace declaration will be placed in a C++ namespace with the specified name. The contents of declare statements occurring before the namespace declaration will be outside the namespace.\nISA parser Formats operands decode tree let blocks microcode assembler microops macroops directives rom object Lots more stuff "
},
{
	"uri": "http://localhost/tutorials/ics-2018/",
	"title": "ISC-2018 Vector Architecture Exploration",
	"tags": [],
	"description": "",
	"content": " Vector Architecture Exploration with gem5 (Arm) Abstract The Arm Scalable Vector Extension (SVE) is a key enabling technology to accelerate HPC and machine learning workloads on future Arm-based processors. SVE does not set a specific vector length, which is microarchitecture-specific. This vector-length agnosticism increases design space complexity and exacerbates the importance of having flexible and accurate modeling tools.\ngem5 is an open-source full-system microarchitectural simulator that is widely used in academia and industry. Arm is a major contributor to gem5 and has developed and upstreamed many features and models. SVE support in gem5 is being finalized to be made publicly available to enable users to simulate multi-core architectures with SVE using Arm-provided timing models.\nThis tutorial covers the features of SVE, the trade-offs of designing a multi-core that uses vectors, and the publicly available tools to model the performance of such vector architectures, with an emphasis on gem5 with SVE support. In addition to gem5, the tutorial will also cover other analysis tools for SVE, such as the Arm Instruction Emulator, which will be made available to the participants through docker images to provide a quick start in these environments.\nTarget Audience The primary audience are computer architect engineers both in academia (e.g., graduate students) and in industry who want to learn about the Arm Scalable Vector Extension (SVE) and the Arm tools for SVE, or are planning to use gem5 for architecture research, especially if they plan to explore Arm vector architectures. The tutorial is also expected to be useful as a high-level introduction to gem5 and how it can be used for architecture research.\nPrerequisites: working knowledge of computer systems, vector architectures, C++ and Python is recommended.\nSchedule (tentative)    Topic Time     Introduction 10 min   The Arm Scalable Vector Extension 30 min   Vector Architecture Design and Tools 30 min   Introduction to gem5 15 min       Break 30 min   gem5 Basics 45 min   gem5 Advanced Features 45 min   SVE gem5 Simulation 30 min   Closing 5 min    Organizers Tutorial organized by Alex Rico and Jose Joao of Arm\n"
},
{
	"uri": "http://localhost/cpu-models/inorder/inst-schedules/",
	"title": "Instruction schedules &amp; pipeline definition",
	"tags": [],
	"description": "",
	"content": " Instruction Schedules \u0026amp; Pipeline Definitions Overview At the heart of the InOrderCPU model is the concept of Instruction Schedules (IS). Instruction schedules create the generic framework that allow for developer\u0026rsquo;s to make a custom pipeline. A pipeline definition can be seen as a collection of instruction schedules that govern what an instruction will do in any given stage and what stage that instruction will go to next.\nIn general, each instruction has a stage-by-stage list of tasks that need to be accomplished before moving on to the next stage. This list we refer to as the instruction\u0026rsquo;s schedule. Each list is composed of \u0026ldquo;ScheduleEntry\u0026rdquo;s that define a task for the instruction to do for a given pipeline stage.\nInstruction scheduling is then divided into a front-end schedule (e.g. Instruction Fetch and Decode) which is uniform for all the instructions, and a back-end schedule, which varies across the different instructions (e.g. a \u0026lsquo;addu\u0026rsquo; instruction and a \u0026lsquo;mult\u0026rsquo; instruction need to access different resources).\nThe combination of a front-end schedule and a back-end schedule make up the instruction schedule. Ideally, changing the pipeline can be as simple as editing how a certain class of instructions operate by editing the instruction schedule functions.\nRelevant source files:\n pipeline_traits.[hh,cc] resource.[hh,cc] resources/*.[hh,cc] resource_pool.[hh,cc] cpu.[hh,cc]  Schedule Entries Schedule Entries denote a particular task for an instruction to process in a pipeline stage.\nThe following code snippet shows the definition of a Schedule Entry:\nstruct ScheduleEntry { ScheduleEntry(int stage_num, int _priority, int res_num, int _cmd = 0, int _idx = -1) : stageNum(stage_num), resNum(res_num), cmd(_cmd), idx(_idx), priority(_priority) { } virtual ~ScheduleEntry(){} // Stage number to perform this service. int stageNum; // Resource ID to access int resNum; // See specific resource for meaning unsigned cmd; // See specific resource for meaning unsigned idx; // Some Resources May Need Priority? int priority; };  Instruction Schedules consist of lists of \u0026ldquo;ScheduleEntry\u0026rdquo;s specific to each instruction. Resource IDs are found in the pipeline_traits.hh and must match those that are allocated in resource_pool.cc. Each resource defines which commands are eligible to be processed for that particular resource.\nA pipeline stage processes an instruction by using the instruction schedule schedule to see which resource it needs to process next.\nFront-End Schedules Front End Schedules are composed of tasks that all instructions must perform. This typically consists of instruction fetch and decode. A new front end schedule is created every time a new instruction is created.\nFront-end Schedule Example  Front-end Schedule  The front-end schedule comprises of the IF and ID stages  IF  NPC is updated by the Fetch unit Instruction fetch from the I-Cache is initiated  ID  Instruction fetch is completed by I-Cache Instruction decode is performed by the Decode unit Branch prediction is performed by the BPred unit Target PC is updated by the Fetch unit     An example of this is in this code snippet from pipeline_traits.cc:\n InstStage *F = inst-\u0026gt;addStage(); InstStage *D = inst-\u0026gt;addStage(); // FETCH F-\u0026gt;needs(FetchSeq, FetchSeqUnit::AssignNextPC); F-\u0026gt;needs(ICache, CacheUnit::InitiateFetch); // DECODE D-\u0026gt;needs(ICache, CacheUnit::CompleteFetch); D-\u0026gt;needs(Decode, DecodeUnit::DecodeInst); D-\u0026gt;needs(BPred, BranchPredictor::PredictBranch); D-\u0026gt;needs(FetchSeq, FetchSeqUnit::UpdateTargetPC);   Note an InstStage object has been introduced to help create instruction schedules easier.  Back-end Schedule Back end schedules vary depending on the instruction type. Typically, this consists of the pipeline after (or including) the decode stage since we can identify whether an instruction is a load,store, branch, etc. at that point.\nBack-end Schedule Example  Back-end Schedule  The back-end schedule comprises of the ID, EX, MEM, and WB stages  ID  For non-store instructions, the source registers, if any, are read by the RF Manager For load instructions, address generation is performed by the AGEN unit and data read from the D-Cache is initiated The rest of the instructions are executed in the execution units  Single cycle operations are sent to the integer EXU Execution is initiated for the multicycle/pipelined operations   EX  Execution is finished for the multicycle/pipelined operations For load instructions, data read from the D-Cache is completed For store instructions, the following tasks are performed  The source registers are read by the RF manager Address generation is performed by the AGen unit Data write into the D-Cache is initiated   MEM  For store instructions, data write into the D-Cache is completed  WB  Destination registers are written into by the RF manager The instruction is graduated by the Grad unit     Typical code might for the Execute stage might be:\n if ( inst-\u0026gt;isNonSpeculative() ) { // skip execution of non speculative insts until later } else if ( inst-\u0026gt;isMemRef() ) { if ( inst-\u0026gt;isLoad() ) { X-\u0026gt;needs(AGEN, AGENUnit::GenerateAddr); } } else { X-\u0026gt;needs(ExecUnit, ExecutionUnit::ExecuteInst); }  "
},
{
	"uri": "http://localhost/deprecated/integrating-m5-gems/",
	"title": "Integrating m5 and gems",
	"tags": [],
	"description": "",
	"content": " {|style=\u0026ldquo;width:100%;text-align:center;white-space:nowrap;color:#000\u0026rdquo; |\nThe GEMS/M5 integration project  |}\nSprint We\u0026rsquo;re having a coding sprint on January 13, 2009. The sprint begins at 9AM PST/11AM CST. We will begin with a phone call on Nate\u0026rsquo;s conference line and use IRC throughout the day.\nGoal To get a \u0026ldquo;working\u0026rdquo; unified simulator by the end of the day.\nTasks  Unified build environment using scons \u0026ndash; Arka w/ Nate and Steve supervising Support system call emulation mode Support full system mode  atomic support, especially load locked/store conditional \u0026ndash; Derek pio support  Deal with lack of first-class data support in Ruby Configuration management  Option 1: Configuration checks between m5 and RubyConfig Option 2: M5 front-end directly modifying ruby parameters  Testing infrastructure base on m5 infrastructure  Which tests?  What run modes to support? A fast Ruby-less or Ruby-lite mode? Develop detailed list of future tasks  Participants  Nathan Binkert (All Day) Dan Gibson (All Day) David Wood (All Day, except 12:30-2pm CST) Derek Hower (All Day) Steve Reinhardt (All Day except 10-10:30 PST) Polina Dudnik (All Day) Brad Beckmann (All Day) Ali Saidi (All Day) Arkaprava(Arka) Basu ( All Day)  Communications IRC channel: irc.freenode.net channel: #m5dev IRC Client Recommendations:\n   Operating System Client     Mac OS X Colloquy   Windows mIrc   Linux/Unix graphical client   Linux/Unix text client    Repository ssh://m5sim.org//repo/gem5\nRuby-Side Short Term Tasks  ONGOING: References to Ruby\u0026rsquo;s configuration parameters directly via their global names should be changed to reference them through static calls to RubyConfig instead. This can be done in small or large chunks, as time allows.  Time est: Hours Difficulty: Trivial    ONGOING: Find and remove random unused transactional memory remnants. Grep for XACT, xact, etc.  Time est: (up to) Hours Difficulty: Trivial (Should be trivial at this stage)    Convert Ruby\u0026rsquo;s Profiler.* to use M5\u0026rsquo;s registered statistics interface.  Time est: Hours Difficulty: Moderate (requires learning)    Convert Ruby\u0026rsquo;s warmup functionality to operate under M5\u0026rsquo;s so-called \u0026lsquo;atomic\u0026rsquo; memory mode.  Time est: Hours Difficulty: Easy-Moderate    DONE Investigate compression for Ruby\u0026rsquo;s warmup traces. We removed gzstream because of an incompatible license, but if we ever want to use warmup traces, we will probably want them compressed, somehow.  Time est: Minutes-hours Difficulty: Easy-Moderate  COMMENT (ATTN: Derek) : We need to decide whether we need traces for Ruby now that we can use M5 atomic mode. If so and Derek still wants to keep compressing and decompressing traces, the current source of zlib available at http://www.zlib.net/ contains three contrib/iostream folders implementing C++ wrappers around zlib. The license should be appropriate.     Investigate the differences between Ruby\u0026rsquo;s DRAM model and M5\u0026rsquo;s DRAM model. Prepare a brief textual summary of how they differ (what functionality does one have and not the other).  Time est: Minutes-hours Difficulty: Easy    Integrate Ruby\u0026rsquo;s random tester into M5 test framework  Time est: Minutes-hours Difficulty: Easy-Moderate    Establish a connection from Ruby to M5\u0026rsquo;s physical memory or move memory totally into Ruby  Time est: Minutes-hours Difficulty: Easy-Moderate   Long Term Tasks  Ruby-side: Data in caches (tentative: Polina) - I don\u0026rsquo;t see a huge need for this, but if it is forced on us, it\u0026rsquo;ll be good to have a junior grad student (JGS) do it. Basically we have to revive the old DATA_BLOCK flag, which worked in the \u0026lsquo;research tree\u0026rsquo; when I joined the group. I\u0026rsquo;ve turned it on for my own reasons in the past, discovered it was broken, and put no effort into fixing it. Ruby-side: Python configuration adaptation - M5 uses a very different configuration system than Ruby, and Ruby\u0026rsquo;s was based heavily on the Simics CLI. I have a hack in place, so that it is possible to change some Ruby parameters at runtime (any at compile-time), but that should be temporary \u0026ndash; we should switch entirely to M5\u0026rsquo;s style. It will little more than a lot of grep\u0026rsquo;ping and such, but again its a good familiarization excercise. At the same time, we can prune some fat from the configuration parameters. Ruby-side: M5 fast/timing mode support - Timing mode is Ruby\u0026rsquo;s normal operation. Its possible that we can use \u0026lsquo;fast\u0026rsquo; mode to warm caches (as we currently do from gzipped traces). This will require some new coding here and there, as well as testing. - As a point of terminology, the \u0026lsquo;fast\u0026rsquo; mode is called \u0026lsquo;atomic\u0026rsquo; mode in M5 (since memory transactions complete atomically), not to be confused with support for processor-atomic memory operations discussed below. There\u0026rsquo;s also a third \u0026lsquo;functional\u0026rsquo; mode (see [Memory System#Access Types]). I\u0026rsquo;d think that some support for functional mode would be required for syscall emulation, though I don\u0026rsquo;t see where that is in Daniel\u0026rsquo;s tarball (RubyMemoryPort has recvTiming() but not recvFunctional()). Using atomic mode for warmup would be a nice addition but isn\u0026rsquo;t critical, presuming that you never needed it before (or did you do warmup via a different method?). \u0026ndash; Steve Ruby-side: Atomic support (Derek Hower) - This may be important for us in the long run. We\u0026rsquo;ll do some kind of horrible nasty hack in the sprint, but we\u0026rsquo;ll want something flexible, generic, and elegant in the long run. We\u0026rsquo;ll need a clever JGS to make that happen. This will actually be quite challenging to integrate into existing protocols, as we need something like an M-locked state to really get the timing right. At the same time, we might also implement true write merging (read-merge-write timing), as an option (the other option is subblocked caches with per-subblock ECC). - I think there are three different issues here: 1. Support for Alpha LL/SC (which the M5 code confusingly calls \u0026ldquo;locked\u0026rdquo; operations). 2. Support for \u0026ldquo;normal\u0026rdquo; atomic RMW ops (SPARC swaps, etc.). 3. Support for uncached RMW ops (e.g., bus locking; x86 only). Number 1 will require some hacking, though it can mostly be done outside the coherence protocol. (There are a few efficiency and potential livelock things you want to do at the coherence level, but I believe they\u0026rsquo;re optional for just getting things to work.) Number 2 should be trivial the way M5 does them, as the swap operation is sent to the cache and only requires exclusive block access (though Dan is correct that if you want to get the timing precise it\u0026rsquo;s a little trickier\u0026hellip; but if you really wanted to be realistic you wouldn\u0026rsquo;t be doing the operation at the cache anyway, I don\u0026rsquo;t think). Number 3 is a big pain but it\u0026rsquo;s not needed until we get further along with x86 FS mode (M5 doesn\u0026rsquo;t do it currently either); just wanted to raise that spectre to get people used to the idea. \u0026ndash; Steve Ruby-side: Timing of uncached accesses( Arka) - Basically, we need add an \u0026lsquo;isUncacheable\u0026rsquo; flag to network messages and modify SLICC to generate cache controllers that ignore messages with the isUncacheable flag set. That should effectively force the messages to traverse their normal miss path. As an optimization (depending on interconnect, etc.), we can add special routing capability to move straight to the memory controller and/or off-chip bridge. If we *add* some notion of an off-chip actor, that is. - Is modifying SLICC required? I would have thought that uncached accesses could be handled entirely in Ruby, but my understanding of the division between Ruby and SLICC is probably faulty. \u0026ndash; Steve - I think this can be done entirely in the Ruby sequencer. Unless there is a showstopper I\u0026rsquo;m not seeing now, modifying SLICC to handle this would be a little overkill. \u0026ndash;Derek Ruby-side: Fix Directory Memory (tentative: Polina) - DirectoryMemory.C implements \u0026lsquo;generic\u0026rsquo; directory data by allocating permanent directory state for all cache blocks in the physical memory. This is fine, except it is stored as an array of DirectoryEntry*. The size of that array is MAX_ADDRESS / CACHE_LINE_SIZE_BYTES. That, too, is fine, except when the physical memory space isn\u0026rsquo;t contigous. E.g. when addresses start at 0, run to 0x0ff, then resume at 0x100000 through 0x1000ff. It happens \u0026ndash; for instance when we simulate really large machines with huge complex backplanes. It /can/ happen in M5, too. Ruby also seems to leak memory because of this\u0026hellip; even though its not really a leak. The solution is to move the whole data structure from array-based to some kind of balanced tree. Its been on the list of things to do for a long long time.  NOTOC\n"
},
{
	"uri": "http://localhost/ruby/interconnection-network/",
	"title": "Interconnection network",
	"tags": [],
	"description": "",
	"content": " The various components of the interconnection network model inside gem5\u0026rsquo;s ruby memory system are described here.\nHow to invoke the network Simple Network:\n./build/ALPHA/gem5.debug \\ configs/example/ruby_random_test.py \\ --num-cpus=16 \\ --num-dirs=16 \\ --network=simple --topology=Mesh_XY \\ --mesh-rows=4  The default network is simple, and the default topology is crossbar.\nGarnet network:\n./build/ALPHA/gem5.debug \\ configs/example/ruby_random_test.py \\ --num-cpus=16 \\ --num-dirs=16 \\ --network=garnet2.0 \\ --topology=Mesh_XY \\ --mesh-rows=4  Topology The connection between the various controllers are specified via python files. All external links (between the controllers and routers) are bi-directional. All internal links (between routers) are uni-directional \u0026ndash; this allows a per-direction weight on each link to bias routing decisions.\n Related Files:  src/mem/ruby/network/topologies/Crossbar.py src/mem/ruby/network/topologies/CrossbarGarnet.py \u0026rsquo; src/mem/ruby/network/topologies/Mesh_XY.py\u0026lsquo; \u0026rsquo; src/mem/ruby/network/topologies/Mesh_westfirst.py\u0026lsquo; src/mem/ruby/network/topologies/MeshDirCorners_XY.py src/mem/ruby/network/topologies/Pt2Pt.py src/mem/ruby/network/Network.py src/mem/ruby/network/BasicLink.py src/mem/ruby/network/BasicRouter.py    Topology Descriptions:  Crossbar: Each controller (L1/L2/Directory) is connected to a simple switch. Each switch is connected to a central switch (modeling the crossbar). This can be invoked from command line by \u0026ndash;topology=Crossbar. CrossbarGarnet: Each controller (L1/L2/Directory) is connected to every other controller via one garnet router (which internally models the crossbar and allocator). This can be invoked from command line by \u0026ndash;topology=CrossbarGarnet. Mesh_*: This topology requires the number of directories to be equal to the number of cpus. The number of routers/switches is equal to the number of cpus in the system. Each router/switch is connected to one L1, one L2 (if present), and one Directory. The number of rows in the mesh has to be specified by \u0026ndash;mesh-rows. This parameter enables the creation of non-symmetrical meshes too.  Mesh_XY: Mesh with XY routing. All x-directional links are biased with a weight of 1, while all y-directional links are biased with a weight of 2. This forces all messages to use X-links first, before using Y-links. It can be invoked from command line by \u0026ndash;topology=Mesh_XY Mesh_westfirst: Mesh with west-first routing. All west-directional links are biased with a weight of 1, al other links are biased with a weight of 2. This forces all messages to use west-directional links first, before using other links. It can be invoked from command line by \u0026ndash;topology=Mesh_westfirst  MeshDirCorners_XY: This topology requires the number of directories to be equal to 4. number of routers/switches is equal to the number of cpus in the system. Each router/switch is connected to one L1, one L2 (if present). Each corner router/switch is connected to one Directory. It can be invoked from command line by \u0026ndash;topology=MeshDirCorners_XY. The number of rows in the mesh has to be specified by \u0026ndash;mesh-rows. The XY routing algorithm is used. Pt2Pt: Each controller (L1/L2/Directory) is connected to every other controller via a direct link. This can be invoked from command line by \u0026ndash;topology=Pt2Pt.   http://pwp.gatech.edu/ece-synergy/wp-content/uploads/sites/332/2016/10/topologies.jpg\nIn each topology, each link and each router can independently be passed a parameter that overrides the defaults (in BasicLink.py and BasicRouter.py):\n Link Parameters:  latency: latency of traversal within the link. weight: weight associated with this link. This parameter is used by the routing table while deciding routes, as explained next in Routing. bandwidth_factor: Only used by simple network to specify width of the link in bytes. This translates to a bandwidth multiplier (simple/SimpleLink.cc) and the individual link bandwidth becomes bandwidth multiplier x endpoint_bandwidth (specified in SimpleNetwork.py). In garnet, the bandwidth is specified by ni_flit_size in GarnetNetwork.py)    - Internal Link Parameters:  src_outport: String with name for output port from source router. dst_inport: String with name for input port at destination router.   These two parameters can be used by routers to implement custom routing algorithms in garnet2.0 (see Routing.\n Router Parameters:  latency: latency of each router. Only supported by garnet2.0.   Routing \u0026rdquo;\u0026lsquo;Table-based Routing (Default): \u0026ldquo;\u0026rsquo; Based on the topology, shortest path graph traversals are used to populate routing tables at each router/switch. This is done in src/mem/ruby/network/Topology.cc The default routing algorithm is table-based and tries to choose the route with minimum number of link traversals. Links can be given weights in the topology files to model different routing algorithms. For example, in Mesh_XY.py and MeshDirCorners_XY.py Y-direction links are given weights of 2, while X-direction links are given weights of 1, resulting in XY traversals. In Mesh_westfirst.py, the west-links are given weights of 1, and all other links are given weights of 2. In garnet2.0, the routing algorithm randomly chooses between links with equal weights. In simple network, it statically chooses between links with equal weights.\n\u0026rdquo;\u0026lsquo;Custom Routing algorithms: \u0026ldquo;\u0026rsquo; In garnet2.0, we provide additional support to implement custom (including adaptive) routing algorithms (See outportComputeXY() in src/mem/ruby/network/garnet2.0/RoutingUnit.cc). The src_outport and dst_inport fields of the links can be used to give custom names to each link (e.g., directions if a mesh), and these can be used inside garnet to implement any routing algorithm. A custom routing algorithm can be selected from the command line by setting \u0026ndash;routing-algorithm=2. See configs/network/Network.py and src/mem/ruby/network/garnet2.0/GarnetNetwork.py\nFlow-Control and Router Microarchitecture Ruby supports two network models, Simple and Garnet, which trade-off detailed modeling versus simulation speed respectively.\nSimple Network Details of the Simple Network are here.\nGarnet Details of the original (2009) Garnet network are here. This design is no longer supported in the codebase.\nGarnet2.0 Details of the new (2016) Garnet2.0 network are here.\nRunning the Network with Synthetic Traffic The interconnection networks can be run in a standalone manner and fed with synthetic traffic. We recommend doing this with garnet2.0.\nRunning Garnet Standalone with Synthetic Traffic\n"
},
{
	"uri": "http://localhost/docs/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " What is gem5? gem5 is a modular discrete event driven computer system simulator platform. That means that:\n gem5\u0026rsquo;s components can be rearranged, parameterized, extended or replaced easily to suit your needs. It simulates the passing of time as a series of discrete events. Its intended use is to simulate one or more computer systems in various ways. It\u0026rsquo;s more than just a simulator; it\u0026rsquo;s a simulator platform that lets you use as many of its premade components as you want to build up your own simulation system.  gem5 is written primarily in C++ and python and most components are provided under a BSD style license. It can simulate a complete system with devices and an operating system in full system mode (FS mode), or user space only programs where system services are provided directly by the simulator in syscall emulation mode (SE mode). There are varying levels of support for executing Alpha, ARM, MIPS, Power, SPARC, and 64 bit x86 binaries on CPU models including two simple single CPI models, an out of order model, and an in order pipelined model. A memory system can be flexibly built out of caches and crossbars. Recently the Ruby simulator has been integrated with gem5 to provide even more flexible memory system modeling.\nThere are many components and features not mentioned here, but from just this partial list it should be obvious that gem5 is a sophisticated and capable simulation platform. Even with all gem5 can do today, active development continues through the support of individuals and some companies, and new features are added and existing features improved on a regular basis.\nCapabilities out of the box gem5 is designed for use in computer architecture research, but if you\u0026rsquo;re trying to research something new and novel it probably won\u0026rsquo;t be able to evaluate your idea out of the box. If it could, that probably means someone has already evaluated a similar idea and published about it.\nTo get the most out of gem5, you\u0026rsquo;ll most likely need to add new capabilities specific to your project\u0026rsquo;s goals. gem5\u0026rsquo;s modular design should help you make modifications without having to understand every part of the simulator.\nAs you add the new features you need, please consider contributing your changes back to gem5. That way others can take advantage of your hard work, and gem5 can become an even better simulator.\nQuick Start If you are just getting started with gem5, you can follow the steps below or watch the following video to download, build, and run the code. Refer back to the documentation page for many more details.\n{{#ev:youtube|SW63HJ0nW90|400|center|A youtube video of the installation process on Ubuntu 12.04 64bit. Video resolution can be set to 1080}}\nGetting a copy gem5\u0026rsquo;s source code is managed using the Mercurial revision control system. More details on the repository structure and on Mercurial are on the repository page. Assuming you have Mercurial installed on your system, you can get your own copy of the source repository by typing:\nhg clone http://repo.gem5.org/gem5\nGetting Additional Tools and Files The additional tools (and other platform dependencies) required to build gem5 are discussed here.\nIf you want to run the full-system version (including the full-system regression tests), you will also need to download the full-system files (disk images and binaries). Kernels, disk images, and boot loaders for Alpha, ARM, and x86 are available on the Download page. SPARC disk images are available on the with the OpenSPARC Architecture tools.\nThe path to these files is determined in configs/common/SysPaths.py. There are a couple of default paths hard-coded into this script; you can place the system files at one of those paths, edit SysPaths.py to change those paths, or override the paths in that file by setting your M5_PATH environment variable. If this is not done correctly you will see an error like ImportError: Can't find a path to system files. when you first attempt to run the simulator in full-system mode.\nNote that the default path, /dist/m5/system, is designed for environments where you have root (sudo) access (to create /dist) and want the files in a place where they can be shared by multiple users. If both of these are true, you can follow this example to put the system files at the default location:\n% sudo mkdir -p /dist/m5/system % cd /dist/m5/system % sudo tar vxfj \u0026lt;path\u0026gt;/m5_system_2.0b3.tar.bz2 % sudo mv m5_system_2.0b3/* . ; sudo rmdir m5_system_2.0b3/ % sudo chgrp -R \u0026lt;grp\u0026gt; /dist # where \u0026lt;grp\u0026gt; is a group that contains all the m5 users  In most cases, it\u0026rsquo;s simplest to put the files wherever is convenient and then set M5_PATH to point to them.\nBuilding gem5 uses the scons build system which is based on python. To build the simulator binary, run scons from the top of the source directory with a target of the form build// where  is replaced with one of the predefined set of build parameters and  is replaced with one of the possible m5 binary names. The predefined set of parameters determine build-wide configuration settings that affect the behavior, composition, and capabilities of the binary being built. These parameters include the ISA to be supported, the CPU models to be compiled, and the coherence protocol Ruby should use. Several example config files are available in the directory build_opts and it is fairly easy to see what each parameter represents. We\u0026rsquo;ll talk about the build system in more detail later. Valid binary names are gem5.debug, gem5.opt, gem5.fast, and gem5.prof. These binaries all have different properties suggested by their extension. gem5.debug has optimization turned off to make debugging easier in tools like gdb, gem5.opt has optimizations turned on but debug output and asserts left in, gem5.fast removes those debugging tools, and gem5.prof is built to use with gprof. Normally you\u0026rsquo;ll want to use gem5.opt. To build the simulator in syscall emulation mode with ARM support, optimizations turned on, and debugging left in, you would run:\nscons build/ARM/gem5.opt\nIn your source tree, you\u0026rsquo;d then find a new build/ARM/ directory with the requested gem5.opt in it. For the rest of this chapter we\u0026rsquo;ll assume this is the binary you\u0026rsquo;re using.\nSee the Build System page for more details on building gem5 binaries.\nRunning Now that you\u0026rsquo;ve built gem5, it\u0026rsquo;s time to try running it. An gem5 command line is composed of four parts, the binary itself, options for gem5, a configuration script to run, and then finally options for the configuration script. Several example configuration scripts are provided in the “configs/example” directory and are generally pretty powerful. You are encouraged to make your own scripts, but these are a good starting point. The example script we\u0026rsquo;ll use is called se.py and sets up a basic SE mode simulation for us. We\u0026rsquo;ll tell it to run the hello world binary provided in the gem5 source tree.\nbuild/ARM/gem5.opt configs/example/se.py -c tests/test-progs/hello/bin/arm/linux/hello\nThis builds up a simulated system, tells it to run the binary found at the location specified, and kicks off the simulation. As the binary runs, its output is sent to the console by default and looks like this:\ngem5 Simulator System. http://gem5.org  gem5 is copyrighted software; use the --copyright option for details.\ngem5 compiled Jul 17 2011 19:16:28 gem5 started Jul 17 2011 19:18:16 gem5 executing on zizzer command line: ./build/ARM/m5.opt configs/example/se.py Global frequency set at 1000000000000 ticks per second 0: system.remote_gdb.listener: listening for remote gdb #0 on port 7000 **** REAL SIMULATION **** info: Entering event queue @ 0. Starting simulation... Hello world! hack: be nice to actually delete the event here Exiting @ tick 3188500 because target called exit()\nYou can see a lot of output from the simulator itself, but the line “Hello world!” came from the simulated program. Output files generated from the simulation are put in the m5out directory, including statistics in stats.txt.\nIn this example we didn\u0026rsquo;t provide any options to gem5 itself. If we had, they would have gone on the command line between gem5.opt and se.py. If you\u0026rsquo;d like to see what command line options are supported, you can pass the \u0026ndash;help option to either gem5 or the configuration script. Note that the two groups of options are different, so make sure you keep track of whether they go before or after the configuration script.\nSee the Running gem5 page for more details on running gem5 simulations.\nAsking for help gem5 has two main mailing lists where you can ask for help or advice. gem5-dev is for developers who are working on the main version of gem5. This is the version that\u0026rsquo;s distributed from the website and most likely what you\u0026rsquo;ll base your own work off of. gem5-users is a larger mailing list and is for people working on their own projects which are not, at least initially, going to be distributed as part of the official version of gem5. Most of the time gem5-users is the right mailing list to use. Most of the people on gem5-dev are also on gem5-users including all the main developers, and in addition many other members of the gem5 community will see your post. That helps you because they might be able to answer your question, and it also helps them because they\u0026rsquo;ll be able to see the answers people send you. To find more information about the mailing lists, to sign up, or to look through archived posts visit Mailing Lists.\nBefore reporting a problem on the mailing list, please read Reporting Problems\nWhat works gem5 combines several different ISAs, system modes (SE or FS), CPU models and memory models which all need to work together. Every combination of these may not be fully tested or completely work. The Status Matrix describes the current status of these combinations. Please send an e-mail to the mailing list if a supported combination no longer works, or if you find that a combination works that we\u0026rsquo;re unsure of.\n"
},
{
	"uri": "http://localhost/deprecated/legacy-arm-files/",
	"title": "Legacy ARM files",
	"tags": [],
	"description": "",
	"content": "This page contains full-system files that have been distributed with gem5 in the past, but are deprecated/legacy files. The files linked on the Download page should be used instead of these. They are here to simply document what has been available in the past.\n Tarballs of generic file systems are available from Linaro. Scroll down to the Developers and Community Builds section. Some work will be required to make these suitable for simulation, but they\u0026rsquo;re a reasonable starting point. ARMv8 Full-System Files \u0026ndash; Pre-compiled kernel and disk image for the 64 bit ARMv8 ISA. VExpress_EMM kernel w/PCI support and config \u0026ndash; Pre-compiled Linux 3.3 VExpress_EMM kernel that includes support for PCIe devices, a patch to add gem5 PCIe support to the revision of the vexpress kernel tree and a config file. This kernel is needed if you want to simulated more than 256MB of RAM or networking. Pass --kernel=/path/to/vmlinux-3.3-arm-vexpress-emm-pcie --machine-type=VExpress_EMM on the command line. You\u0026rsquo;ll still need the file systems below. This kernel supports a maximum of 2047MB (one MB less than 2GB) of memory. New Full System Files \u0026ndash; Pre-compiled Linux kernel, and file systems, and kernel config files. This includes both a cut-down linux and a full ubuntu linux. Old Full System Files \u0026ndash; Older pre-compiled Linux kernel, and file system. New users should use package above. This wil likely be removed soon.  "
},
{
	"uri": "http://localhost/deprecated/linux-kernel/",
	"title": "Linux kernel",
	"tags": [],
	"description": "",
	"content": " Depending on what ISA you\u0026rsquo;re using with gem5 the instructions for compiling a Linux kernel differ slightly. Ultimately, we\u0026rsquo;d like to have a single unified way to compile all these kernels, but it will take some work to happen. If you\u0026rsquo;d like to help, please let us know.\nAlpha Currently we have a patch queue for Alpha that can be applied on top of a linux mercurial repository. This patch queue adds various debugging, performance, introspection capabilities and increases the number of simulated processors the Alpha implementation in gem5 can support from 4 to 64. To compile a new kernel for Alpha follow the directions below:\n# First clone a copy of the linux repository hg clone git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux-2.6.git linux-2.6 cd linux-2.6\n# Go to the .hg directory cd .hg\n#Get a copy of the patches repository hg clone http://repo.gem5.org/linux-patches patches\n# Go back to the linux directory cd ..\n#Update the linux repository to a known working version hg update v2.6.27   # Select the patches for 2.6.27 hg qselect 2.6.27   # Apply the patches to the repository hg qpush -a\n# Copy the default config file to .config cp .config.m5 .config\n# Build the kernel make ARCH=alpha CROSS_COMPILE=/path/to/alpha/compiler/alpha-unknown-linux-gnu- vmlinux -j 4\nI few minutes later a kernel will be compiled. You can use a newer version of m5, however you\u0026rsquo;ll likely need to modify the config file and some of the patches slightly for it to work.\nARM See ARM Linux Kernel.\nx86 x86 doesn\u0026rsquo;t currently have a patch queue, but a generic kernel works well since the normal code already has functions that wait for interrupts in idle loops. Some of the debugging functionality, such as m5dprintk can be trivially added if you need to by just using the patch from the alpha linux-patches repository above.\n# To compile a kernel for x86, first get the kernel source. Here again we use mercurial because that is what we use for gem5 # However, you could get the code from kernel.org in a bzip format hg clone http://www.kernel.org/hg/linux-2.6\n# Change directory to the newly cloned directory cd linux-2.6\n# Update to the version you're interested in; 2.6.28.4 in this case because that is what the config file we've got is for hg update v2.6.28.4\n# Grab the config file on the download page: wget http://www.m5sim.org/dist/current/x86/config-x86.tar.bz2 tar jxvf config-x86.tar.bz2 cp configs/linux-2.6.28.4 .config\n# Compile the kernel, assuming you're building on an x86 host make vmlinux -j 4\n"
},
{
	"uri": "http://localhost/deprecated/linux-dist/",
	"title": "Linux-dist",
	"tags": [],
	"description": "",
	"content": " This documentation is relevant only for those desiring to use the full-system aspect of M5. When running in full-system, M5 reads a raw disk image as the hard disk. This tells you how to compile both the linux binary and the disk image for full-system simulation.\nThere is a general 3 step process for doing this:\n compile a cross-compiler capable of building alpha binaries. compile a kernel using this cross-compiler use linux-dist to build binaries for the M5 disk image, and create the image.  Compiling a Cross Compiler using Crosstool-NG Go to here, download crosstool-ng and follow the directions listed on the page. This is a new tool based on Dan Kegel\u0026rsquo;s cross tool and is more up to date.\nCompiling the Kernel First compile the kernel using the instructions at Compiling a Linux Kernel. Then, update m5/configs/common/FSConfig.py to use the new kernel. You basically need to change the part the points to the kernel binary to point to the new kernel (or just replace the binary file for the old kernel with the new kernel\u0026rsquo;s file).\nCompiling benchmarks for the image and creating the image with linux-dist \u0026rdquo;\u0026lsquo;Note that this process is quite deprecated and not well supported. We suggest that you get your disk images either from the downloads page, or if that doesn\u0026rsquo;t satisfy your needs, with a Gentoo stage 3 image. While we\u0026rsquo;re not saying the below will not work, it may require a lot of massaging on your part. \u0026ldquo;\u0026rsquo;\nLinux-dist is bootstrapped off of the Pengutronix PTX-dist tool for building disk images for embedded processors (i.e. need to be smaller). Be sure you have downloaded the linux-dist.tgz tarball from here. Untar it, and there is a standard compile/install procedure.\n Configure the package with ./configure --prefix=/where/you/want/the/installation make make install  You will notice that now, in the place where you have designated, there is now a bin/ and lib/ directory pertaining to linux-dist. Now, you will need to create your m5 image workspace.\n set your path to point to /where/you/want/the/installation/bin. wherever you want your workspace, type: ptxdist clone m5-alpha . This will create a workspace directory with everything you need to make an image. cd into that directory, and type: ptxdist menuconfig. The default ptxconfig ought to be sufficient for everything, but you do need to set one value. Find the Image Creation Options menu and ensure that the \u0026ldquo;Path to kernel src\u0026rdquo; value points to your linux directory that you got from us. type: ptxdist toolchain /path/to/your/toolchain/bin (e.g. /opt/crosstool/gcc-3.4.3-glibc-2.3.5/alpha-unknown-linux-gnu/bin). This ensures that in building all of the binaries you are using the appropriate toolchain. type: ptxdist menuconfig. Note that you will have to go to the Image Creation Options option and set the path to the kernel code that you will be using for your headers and iscsi benchmark module compilation. type: ptxdist go. This will compile everything you need to run existing M5 full-system benchmarks. type: ptxdist images. This actually creates the image for you and is somewhat interactive. You\u0026rsquo;ll need to tell it how big you want the image to be (currently 200MB is sufficient), and where you want to put it. Be sure you have sudo privileges.  This is all you need to create the image. To run benchmarks, you merely need to ensure that m5/configs/common/Benchmarks.py points to the image you just created, and m5/configs/common/FSConfig.py points to the vmlinux you created. To run these benchmarks, see Running M5 in Full-System Mode.\n"
},
{
	"uri": "http://localhost/ruby/mesi-two-level/",
	"title": "MESI two level",
	"tags": [],
	"description": "",
	"content": " Protocol Overview  This protocol models two-level cache hierarchy. The L1 cache is private to a core, while the L2 cache is shared among the cores. L1 Cache is split into Instruction and Data cache. Inclusion is maintained between the L1 and L2 cache. At high level the protocol has four stable states, M, E, S and I. A block in M state means the blocks is writable (i.e. has exclusive permission) and has been dirtied (i.e. its the only valid copy on-chip). E state represent a cache block with exclusive permission (i.e. writable) but is not written yet. S state means the cache block is only readable and possible multiple copies of it exists in multiple private cache and as well as in the shared cache. I means that the cache block is invalid. The on-chip cache coherence is maintained through Directory Coherence scheme, where the directory information is co-located with the corresponding cache blocks in the shared L2 cache. The protocol has four types of controllers \u0026ndash; L1 cache controller, L2 cache controller, Directory controller and DMA controller. L1 cache controller is responsible for managing L1 Instruction and L1 Data Cache. Number of instantiation of L1 cache controller is equal to the number of cores in the simulated system. L2 cache controller is responsible for managing the shared L2 cache and for maintaining coherence of on-chip data through directory coherence scheme. The Directory controller act as interface to the Memory Controller/Off-chip main memory and also responsible for coherence across multiple chips/and external coherence request from DMA controller. DMA controller is responsible for satisfying coherent DMA requests. One of the primary optimization in this protocol is that if a L1 Cache request a data block even for read permission, the L2 cache controller if finds that no other core has the block, it returns the cache block with exclusive permission. This is an optimization done in anticipation that a cache blocks read would be written by the same core soon and thus save an extra request with this optimization. This is exactly why E state exits (i.e. when a cache block is writable but not yet written). The protocol supports silent eviction of clean cache blocks from the private L1 caches. This means that cache blocks which have not been written to and has readable permission only can drop the cache block from the private L1 cache without informing the L2 cache. This optimization helps reducing write-back traffic to the L2 cache controller.  Related Files  src/mem/protocols  MESI_CMP_directory-L1cache.sm: L1 cache controller specification MESI_CMP_directory-L2cache.sm: L2 cache controller specification MESI_CMP_directory-dir.sm: directory controller specification MESI_CMP_directory-dma.sm: dma controller specification MESI_CMP_directory-msg.sm: coherence message type specifications. This defines different field of different type of messages that would be used by the given protocol MESI_CMP_directory.slicc: container file   Controller Description  L1 cache controller     States Invariants and Semantic/Purpose of the state     M The cache block is held in exclusive state by only one L1 cache. There are no sharers of this block. The data is potentially is the only valid copy in the system. The copy of the cache block is writable and as well as readable.   E The cache block is held with exclusive permission by exactly only one L1 cache. The difference with the M state is that the cache block is writable (and readable) but not yet written.   S The cache block is held in shared state by 1 or more L1 caches and/or by the L2 cache. The block is only readable. No cache can have the cache block with exclusive permission.   I / NP The cache block is invalid.   IS Transient state. This means that GETS (Read) request has been issued for the cache block and awaiting for response. The cache block is neither readable nor writable.   IM Transient state. This means that GETX (Write) request has been issued for the cache block and awaiting for response. The cache block is neither readable nor writable.   SM Transient state. This means the cache block was originally in S state and then UPGRADE (Write) request was issued to get exclusive permission for the blocks and awaiting response. The cache block is readable.   IS_I Transient state. This means that while in IS state the cache controller received Invalidation from the L2 Cache\u0026rsquo;s directory. This happens due to race condition due to write to the same cache block by other core, while the given core was trying to get the same cache blocks for reading. The cache block is neither readable nor writable..   M_I Transient state. This state indicates that the cache is trying to replace a cache block in M state from its cache and the write-back (PUTX) to the L2 cache\u0026rsquo;s directory has been issued but awaiting write-back acknowledgement.   SINK_WB_ACK Transient state. This state is reached when waiting for write-back acknowledgement from the L2 cache\u0026rsquo;s directory, the L1 cache received intervention (forwarded request from other cores). This indicates a race between the issued write-back to the directory and another request from the another cache has happened. This also indicates that the write-back has lost the race (i.e. before it reached the L2 cache\u0026rsquo;s directory, another core\u0026rsquo;s request has reached the L2). This state is essential to avoid possibility of complicated race condition that can happen if write-backs are silently dropped at the directory.         L2 cache controller  Recall that the on-chip directory is co-located with the corresponding cache blocks in the L2 Cache. Thus following states in the L2 cache block encodes the information about the status and permissions of the cache blocks in the L2 cache as well as the coherence status of the cache block that may be present in one or more private L1 caches. Beyond the coherence states there are also two more important fields per cache block that aids to make proper coherence actions. These fields are Sharers field, which can be thought of as a bit-vector indicating which of the private L1 caches potentially have the given cache block. The other important field is the Owner field, which is the identity of the private L1 cache in case the cache block is held with exclusive permission in a L1 cache.\n   States Invariants and Semantic/Purpose of the state     NP The cache blocks is not present in the on-chip cache hierarchy.   SS The cache block is present in potentially multiple private caches in only readable mode (i.e.in \u0026ldquo;S\u0026rdquo; state in private caches). Corresponding \u0026ldquo;Sharers\u0026rdquo; vector with the block should give the identity of the private caches which possibly have the cache block in its cache. The cache block in the L2 cache is valid and readable.   M The cache block is present ONLY in the L2 cache and has exclusive permission. L1 Cache\u0026rsquo;s read/write requests (GETS/GETX) can be satisfied directly from the L2 cache.   MT The cache block is in ONE of the private L1 caches with exclusive permission. The data in the L2 cache is potentially stale. The identity of the L1 cache which has the block can be found in the \u0026ldquo;Owner\u0026rdquo; field associated with the cache block. Any request for read/write (GETS/GETX) from other cores/private L1 caches need to be forwarded to the owner of the cache block. L2 can not service requests itself.   M_I Its a transient state. This state indicates that the cache is trying to replace the cache block from its cache and the write-back (PUTX/PUTS) to the Directory controller (which act as interface to Main memory) has been issued but awaiting write-back acknowledgement. The data is neither readable nor writable.   MT_I Its a transient state. This state indicates that the cache is trying to replace a cache block in MT state from its cache. Invalidation to the current owner (private L1 cache) of the cache block has been issued and awaiting write-back from the Owner L1 cache. Note that the this Invalidation (called back-invalidation) is instrumental in making sure that the inclusion is maintained between L1 and L2 caches. The data is neither readable nor writable.   MCT_I Its a transient state.This state is same as MT_I, except that it is known that the data in the L2 cache is in clean state. The data is neither readable nor writable.   I_I Its a transient state. The L2 cache is trying to replace a cache block in the SS state and the cache block in the L2 is in clean state. Invalidations has been sent to all potential sharers (L1 caches) of the cache block. The L2 cache\u0026rsquo;s directory is waiting for all the required Acknowledgements to arrive from the L1 caches. Note that the this Invalidation (called back-invalidation) is instrumental in making sure that the inclusion is maintained between L1 and L2 caches. The data is neither readable nor writable.   S_I Its a transient state.Same as I_I, except the data in L2 cache for the cache block is dirty. This means unlike in the case of I_I, the data needs to be sent to the Main memory. The cache block is neither readable nor writable..   ISS Its a transient state. L2 has received a GETS (read) request from one of the private L1 caches, for a cache block that it not present in the on-chip caches. A read request has been sent to the Main Memory (Directory controller) and waiting for the response from the memory. This state is reached only when the request is for data cache block (not instruction cache block). The purpose of this state is that if it is found that only one L1 cache has requested the cache block then the block is returned to the requester with exclusive permission (although it was requested for reading permission). The cache block is neither readable nor writable.   IS Its a transient state. The state is similar to ISS, except the fact that if the requested cache block is Instruction cache block or more than one core request the same cache block while waiting for the response from the memory, this state is reached instead of ISS. Once the requested cache block arrives from the Main Memory, the block is sent to the requester(s) with read-only permission. The cache block is neither readable nor writable at this state.   IM Its a transient state. This state is reached when a L1 GETX (write) request is received by the L2 cache for a cache blocks that is not present in the on-chip cache hierarchy. The request for the cache block in exclusive mode has been issued to the main memory but response is yet to arrive.The cache block is neither readable nor writable at this state.   SS_MB Its a transient state. In general any state whose name ends with \u0026ldquo;B\u0026rdquo; (like this one) also means that it is a blocking coherence state. This means the directory awaiting for some response from the private L1 cache ans until it receives the desired response any other request is not entertained (i.e. request are effectively serialized). This particular state is reached when a L1 cache requests a cache block with exclusive permission (i.e. GETX or UPGRADE) and the coherence state of the cache blocks was in SS state. This means that the requested cache blocks potentially has readable copies in the private L1 caches. Thus before giving the exclusive permission to the requester, all the readable copies in the L1 caches need to be invalidated. This state indicate that the required invalidations has been sent to the potential sharers (L1 caches) and the requester has been informed about the required number of Invalidation Acknowledgement it needs before it can have the exclusive permission for the cache block. Once the requester L1 cache gets the required number of Invalidation Acknowledgement it informs the director about this by UNBLOCK message which allows the directory to move out of this blocking coherence state and thereafter it can resume entertaining other request for the given cache block. The cache block is neither readable nor writable at this state.   MT_MB Its a transient state and also a blocking state. This state is reached when L2 cache\u0026rsquo;s directory has sent out a cache block with exclusive permission to a requester L1 cache but yet to receive UNBLOCK from the requester L1 cache acknowledging the receipt of exclusive permission. The cache block is neither readable nor writable at this state.   MT_IIB Its a transient state and also a blocking state. This state is reached when a read request (GETS) request is received for a cache blocks which is currently held with exclusive permission in another private L1 cache (i.e. directory state is MT). On such requests the L2 cache\u0026rsquo;s directory forwards the request to the current owner L1 cache and transitions to this state. Two events need to happen before this cache block can be unblocked (and thus start entertaining further request for this cache block). The current owner cache block need to send a write-back to the L2 cache to update the L2\u0026rsquo;s copy with latest value. The requester L1 cache also needs to send UNBLOCK to the L2 cache indicating that it has got the requested cache block with desired coherence permissions. The cache block is neither readable nor writable at this state in the L2 cache.   MT_IB Its a transient state and also a blocking state. This state is reached when at MT_IIB state the L2 cache controller receives the UNBLOCK from the requester L1 cache but yet to receive the write-back from the previous owner L1 cache of the block. The cache block is neither readable nor writable at this state in the L2 cache.   MT_SB Its a transient state and also a blocking state. This state is reached when at MT_IIB state the L2 cache controller receives write-back from the previous owner L1 cache for the blocks, while yet to receive the UNBLOCK from the current requester for the cache block. The cache block is neither readable nor writable at this state in the L2 cache.    "
},
{
	"uri": "http://localhost/ruby/mi-example/",
	"title": "MI example",
	"tags": [],
	"description": "",
	"content": " Protocol Overview  This is a simple cache coherence protocol that is used to illustrate protocol specification using SLICC. This protocol assumes a 1-level cache hierarchy. The cache is private to each node. The caches are kept coherent by a directory controller. Since the hierarchy is only 1-level, there is no inclusion/exclusion requirement. This protocol does not differentiate between loads and stores. This protocol cannot implement the semantics of LL/SC instructions, because external GETS requests that hit a block within a LL/SC sequence steal exclusive permissions, thus causing the SC instruction to fail.  Related Files  src/mem/protocols  MI_example-cache.sm: cache controller specification MI_example-dir.sm: directory controller specification MI_example-dma.sm: dma controller specification MI_example-msg.sm: message type specification MI_example.slicc: container file   Stable States and Invariants    States Invariants     M The cache block has been accessed (read/written) by this node. No other node holds a copy of the cache block   I The cache block at this node is invalid    The notation used in the controller FSM diagrams is described here.\nCache controller  Requests, Responses, Triggers:  Load, Instruction fetch, Store from the core Replacement from self Data from the directory controller Forwarded request (intervention) from the directory controller Writeback acknowledgement from the directory controller Invalidations from directory controller (on dma activity)    Main Operation:  On a load/Instruction fetch/Store request from the core:  it checks whether the corresponding block is present in the M state. If so, it returns a hit otherwise, if in I state, it initiates a GETX request from the directory controller     - On a replacement trigger from self: - it evicts the block, issues a writeback request to the directory controller - it waits for acknowledgement from the directory controller (to prevent races)   - On a forwarded request from the directory controller: - This means that the block was in M state at this node when the request was generated by some other node - It sends the block directly to the requesting node (cache-to-cache transfer) - It evicts the block from this node   - Invalidations are similar to replacements  Directory controller  Requests, Responses, Triggers:  GETX from the cores, Forwarded GETX to the cores Data from memory, Data to the cores Writeback requests from the cores, Writeback acknowledgements to the cores DMA read, write requests from the DMA controllers    Main Operation:  The directory maintains track of which core has a block in the M state. It designates this core as owner of the block. On a GETX request from a core:  If the block is not present, a memory fetch request is initiated If the block is already present, then it means the request is generated from some other core  In this case, a forwarded request is sent to the original owner Ownership of the block is transferred to the requestor   On a writeback request from a core:  If the core is owner, the data is written to memory and acknowledgement is sent back to the core If the core is not owner, a NACK is sent back  This can happen in a race condition The core evicted the block while a forwarded request some other core was on the way and the directory has already changed ownership for the core The evicting core holds the data till the forwarded request arrives   On DMA accesses (read/write)  Invalidation is sent to the owner node (if any). Otherwise data is fetched from memory. This ensures that the most recent data is available.    Other features  - MI protocols don\u0026rsquo;t support LL/SC semantics. A load from a remote core will invalidate the cache block.  This protocol has no timeout mechanisms.   "
},
{
	"uri": "http://localhost/ruby/moesi-cmp-directory/",
	"title": "MOESI CMP directory",
	"tags": [],
	"description": "",
	"content": " Protocol Overview  TODO: cache hierarchy   In contrast with the MESI protocol, the MOESI protocol introduces an additional Owned state. The MOESI protocol also includes many coalescing optimizations not available in the MESI protocol.  Related Files  src/mem/protocols  MOESI_CMP_directory-L1cache.sm: L1 cache controller specification MOESI_CMP_directory-L2cache.sm: L2 cache controller specification MOESI_CMP_directory-dir.sm: directory controller specification MOESI_CMP_directory-dma.sm: dma controller specification MOESI_CMP_directory-msg.sm: message type specification MOESI_CMP_directory.slicc: container file   L1 Cache Controller  Stable States and Invariants     States Invariants     MM The cache block is held exclusively by this node and is potentially modified (similar to conventional \u0026ldquo;M\u0026rdquo; state).   MM_W The cache block is held exclusively by this node and is potentially modified (similar to conventional \u0026ldquo;M\u0026rdquo; state). Replacements and DMA accesses are not allowed in this state. The block automatically transitions to MM state after a timeout.   O The cache block is owned by this node. It has not been modified by this node. No other node holds this block in exclusive mode, but sharers potentially exist.   M The cache block is held in exclusive mode, but not written to (similar to conventional \u0026ldquo;E\u0026rdquo; state). No other node holds a copy of this block. Stores are not allowed in this state.   M_W The cache block is held in exclusive mode, but not written to (similar to conventional \u0026ldquo;E\u0026rdquo; state). No other node holds a copy of this block. Only loads and stores are allowed. Silent upgrade happens to MM_W state on store. Replacements and DMA accesses are not allowed in this state. The block automatically transitions to M state after a timeout.   S The cache block is held in shared state by 1 or more nodes. Stores are not allowed in this state.   I The cache block is invalid.     FSM Abstraction  The notation used in the controller FSM diagrams is described here.\n - Optimizations     States Description     SM A GETX has been issued to get exclusive permissions for an impending store to the cache block, but an old copy of the block is still present. Stores and Replacements are not allowed in this state.   OM A GETX has been issued to get exclusive permissions for an impending store to the cache block, the data has been received, but all expected acknowledgments have not yet arrived. Stores and Replacements are not allowed in this state.    The notation used in the controller FSM diagrams is described here.\nL2 Cache Controller  Stable States and Invariants     Intra-chip Inclusion Inter-chip Exclusion States Description     Not in any L1 or L2 at this chip May be present at other chips NP/I The cache block at this chip is invalid.   Not in L2, but in 1 or more L1s at this chip May be present at other chips ILS The cache block is not present at L2 on this chip. It is shared locally by L1 nodes in this chip.   ILO The cache block is not present at L2 on this chip. Some L1 node in this chip is an owner of this cache block.     ILOS The cache block is not present at L2 on this chip. Some L1 node in this chip is an owner of this cache block. There are also L1 sharers of this cache block in this chip.     Not present at any other chip ILX The cache block is not present at L2 on this chip. It is held in exclusive mode by some L1 node in this chip.    ILOX The cache block is not present at L2 on this chip. It is held exclusively by this chip and some L1 node in this chip is an owner of the block.     ILOSX The cache block is not present at L2 on this chip. It is held exclusively by this chip. Some L1 node in this chip is an owner of the block. There are also L1 sharers of this cache block in this chip.     In L2, but not in any L1 at this chip May be present at other chips S The cache block is not present at L1 on this chip. It is held in shared mode at L2 on this chip and is also potentially shared across chips.   O The cache block is not present at L1 on this chip. It is held in owned mode at L2 on this chip. It is also potentially shared across chips.     Not present at any other chip M The cache block is not present at L1 on this chip. It is present at L2 on this chip and is potentially modified.    Both in L2, and 1 or more L1s at this chip May be present at other chips SLS The cache block is present at L2 in shared mode on this chip. There exists local L1 sharers of the block on this chip. It is also potentially shared across chips.   OLS The cache block is present at L2 in owned mode on this chip. There exists local L1 sharers of the block on this chip. It is also potentially shared across chips.     Not present at any other chip OLSX The cache block is present at L2 in owned mode on this chip. There exists local L1 sharers of the block on this chip. It is held exclusively by this chip.      FSM Abstraction  The controller is described in 2 parts. The first picture shows transitions between all \u0026ldquo;intra-chip inclusion\u0026rdquo; categories and within categories 1, 3, 4. Transitions within category 2 (Not in L2, but in 1 or more L1s at this chip) are shown in the second picture.\nThe notation used in the controller FSM diagrams is described here. Transitions involving other chips are annotated in brown.\nThe second picture below expands the central hexagonal portion of the above picture to show transitions within category 2 (Not in L2, but in 1 or more L1s at this chip).\nThe notation used in the controller FSM diagrams is described here. Transitions involving other chips are annotated in brown.\nDirectory Controller  Stable States and Invariants     States Invariants     M The cache block is held in exclusive state by only 1 node (which is also the owner). There are no sharers of this block. The data is potentially different from that in memory.   O The cache block is owned by exactly 1 node. There may be sharers of this block. The data is potentially different from that in memory.   S The cache block is held in shared state by 1 or more nodes. No node has ownership of the block. The data is consistent with that in memory (Check).   I The cache block is invalid.     FSM Abstraction  The notation used in the controller FSM diagrams is described here.\nOther features  Timeouts:  Rathijit will do it\n"
},
{
	"uri": "http://localhost/ruby/moesi-cmp-token/",
	"title": "MOESI CMP token",
	"tags": [],
	"description": "",
	"content": " Protocol Overview  This protocol also models a 2-level cache hierarchy.   It maintains coherence permission by explicitly exchanging and counting tokens.   A fix number of token are assigned to each cache block in the beginning, the number of token remains unchanged.   To write a block, the processor must have all the token for that block. For reading at least one token is required.   The protocol also has a persistent message support to avoid starvation.  Related Files  src/mem/protocols  MOESI_CMP_token-L1cache.sm: L1 cache controller specification MOESI_CMP_token-L2cache.sm: L2 cache controller specification MOESI_CMP_token-dir.sm: directory controller specification MOESI_CMP_token-dma.sm: dma controller specification MOESI_CMP_token-msg.sm: message type specification MOESI_CMP_token.slicc: container file   Controller Description  L1 Cache     States Invariants     MM The cache block is held exclusively by this node and is potentially modified (similar to conventional \u0026ldquo;M\u0026rdquo; state).   MM_W The cache block is held exclusively by this node and is potentially modified (similar to conventional \u0026ldquo;M\u0026rdquo; state). Replacements and DMA accesses are not allowed in this state. The block automatically transitions to MM state after a timeout.   O The cache block is owned by this node. It has not been modified by this node. No other node holds this block in exclusive mode, but sharers potentially exist.   M The cache block is held in exclusive mode, but not written to (similar to conventional \u0026ldquo;E\u0026rdquo; state). No other node holds a copy of this block. Stores are not allowed in this state.   M_W The cache block is held in exclusive mode, but not written to (similar to conventional \u0026ldquo;E\u0026rdquo; state). No other node holds a copy of this block. Only loads and stores are allowed. Silent upgrade happens to MM_W state on store. Replacements and DMA accesses are not allowed in this state. The block automatically transitions to M state after a timeout.   S The cache block is held in shared state by 1 or more nodes. Stores are not allowed in this state.   I The cache block is invalid.     L2 cache     States Invariants     NP The cache block is held exclusively by this node and is potentially locally modified (similar to conventional \u0026ldquo;M\u0026rdquo; state).   O The cache block is owned by this node. It has not been modified by this node. No other node holds this block in exclusive mode, but sharers potentially exist.   M The cache block is held in exclusive mode, but not written to (similar to conventional \u0026ldquo;E\u0026rdquo; state). No other node holds a copy of this block. Stores are not allowed in this state.   S The cache line holds the most recent, correct copy of the data. Other processors in the system may hold copies of the data in the shared state, as well. The cache line can be read, but not written in this state.   I The cache line is invalid and does not hold a valid copy of the data.     Directory controller     States Invariants     O Owner .   NO Not Owner.   L Locked.    "
},
{
	"uri": "http://localhost/ruby/moesi-hammer/",
	"title": "MOESI hammer",
	"tags": [],
	"description": "",
	"content": " This is an implementation of AMD\u0026rsquo;s Hammer protocol, which is used in AMD\u0026rsquo;s Hammer chip (also know as the Opteron or Athlon 64). The protocol implements both the original a HyperTransport protocol, as well as the more recent ProbeFilter protocol. The protocol also includes a full-bit directory mode.\nRelated Files  src/mem/protocols  MOESI_hammer-cache.sm: cache controller specification MOESI_hammer-dir.sm: directory controller specification MOESI_hammer-dma.sm: dma controller specification MOESI_hammer-msg.sm: message type specification MOESI_hammer.slicc: container file   Cache Hierarchy This protocol implements a 2-level private cache hierarchy. It assigns separate Instruction and Data L1 caches, and a unified L2 cache to each core. These caches are private to each core and are controlled with one shared cache controller. This protocol enforce exclusion between L1 and L2 caches.\nStable States and Invariants    States Invariants     MM The cache block is held exclusively by this node and is potentially locally modified (similar to conventional \u0026ldquo;M\u0026rdquo; state).   O The cache block is owned by this node. It has not been modified by this node. No other node holds this block in exclusive mode, but sharers potentially exist.   M The cache block is held in exclusive mode, but not written to (similar to conventional \u0026ldquo;E\u0026rdquo; state). No other node holds a copy of this block. Stores are not allowed in this state.   S The cache line holds the most recent, correct copy of the data. Other processors in the system may hold copies of the data in the shared state, as well. The cache line can be read, but not written in this state.   I The cache line is invalid and does not hold a valid copy of the data.    Cache controller The notation used in the controller FSM diagrams is described here.\nMOESI_hammer supports cache flushing. To flush a cache line, the cache controller first issues a GETF request to the directory to block the line until the flushing is completed. It then issues a PUTF and writes back the cache line.\nDirectory controller MOESI_hammer memory module, unlike a typical directory protocol, does not contain any directory state and instead broadcasts requests to all the processors in the system. In parallel, it fetches the data from the DRAM and forward the response to the requesters.\nprobe filter: TODO\n Stable States and Invariants     States Invariants     NX Not Owner, probe filter entry exists, block in O at Owner.   NO Not Owner, probe filter entry exists, block in E/M at Owner.   S Data clean, probe filter entry exists pointing to the current owner.   O Data clean, probe filter entry exists.   E Exclusive Owner, no probe filter entry.     Controller  The notation used in the controller FSM diagrams is described here.\n"
},
{
	"uri": "http://localhost/docs/memory-system/",
	"title": "Memory system",
	"tags": [],
	"description": "",
	"content": " gem5\u0026rsquo;s memory system was designed to enable:\n Modularity and compartmentalisation through standard interfaces. Suitable interfaces for loosely-timed, approximately-timed and untimed transaction-level modelling. Flexibility to allow other memory interconnects besides a crossbar. A comprehensive set of building blocks, ranging from caches, crossbars, to full-blown DRAM controllers.  Ports system MemObjects All objects within a memory system inherit from MemObject. This class adds the pure virtual functions getMasterPort(const std::string \u0026amp;name) and getSlavePort(const std::string \u0026amp;name) which returns a port corresponding to the given name. This interface is used to connect memory objects together.\nPorts Ports are used to interface memory objects to each other. They will always come in pairs and we refer to the other port object as the peer. A master port always connects to a slave port, with the master initiating requests, and the slave providing responses. Every memory object has to have at least one port to be useful.\nThere are two groups of functions in the port object. The send* functions are called on the port by the object that owns that port. For example to send a request packet in the memory system a CPU would call myPort-\u0026gt;sendTimingReq(pkt) to send a packet. Each send function has a corresponding recv function that is called on the ports peer. So the implementation of the sendTimingReq() call above would simply be peer-\u0026gt;recvTimingReq(pkt). Using this method we only have one virtual function call penalty but keep generic ports that can connect together any memory system objects.\nConnections In Python, Ports are first-class attributes of simulation objects, much like Params. Two objects can specify that their ports should be connected using the assignment operator. Unlike a normal variable or parameter assignment, port connections are symmetric: A.port1 = B.port2 has the same meaning as B.port2 = A.port1.\nObjects such as busses that have a potentially unlimited number of ports use \u0026ldquo;vector ports\u0026rdquo;. An assignment to a vector port appends the peer to a list of connections rather than overwriting a previous connection.\nPort proxies There are three types of port proxies that wrap the port interface and are used for initialisation and introspection.\n PortProxy provides easy to use methods for writing and reading physical addresses. It is only meant to load data into memory and update constants before the simulation begins. SETranslatingPortProxy and FSTranslatingPortProxy provide the same methods as PortProxy, but the addresses passed to them are virtual addresses, and a translation is done to get the physical address.  Packets A Packet is used to encapsulate a transfer between two objects in the memory system (e.g., the L1 and L2 cache). This is in contrast to a Request where a single Request travels all the way from the requester to the ultimate destination and back, possibly being conveyed by several different Packets along the way.\nRead access to many packet fields is provided via accessor methods which verify that the data in the field being read is valid.\nA packet contains the following all of which are accessed by accessors to be certain the data is valid:\n The address. This is the address that will be used to route the packet to its target (if the destination is not explicitly set) and to process the packet at the target. It is typically derived from the request object\u0026rsquo;s physical address, but may be derived from the virtual address in some situations (e.g., for accessing a fully virtual cache before address translation has been performed). It may not be identical to the original request address: for example, on a cache miss, the packet address may be the address of the block to fetch and not the request address. The size. Again, this size may not be the same as that of the original request, as in the cache miss scenario. A pointer to the data being manipulated.  Set by dataStatic() and dataDynamic(), which controls if the data associated with the packet is freed when the packet is. Allocated if not set by one of the above methods allocate() and the data is freed when the packet is destroyed. (Always safe to call). A pointer can be retrived by calling getPtr() or getConstPtr() get() and set() can be used to manipulate the data in the packet. The get() method does a guest-to-host endian conversion and the set method does a host-to-guest endian conversion.  A list of Packet Command Attributes associated with the packet A SenderState pointer which is a virtual base opaque structure used to hold state associated with the packet but specific to the sending device (e.g., an MSHR). A pointer to this state is returned in the packet\u0026rsquo;s response so that the sender can quickly look up the state needed to process it. A specific subclass would be derived from this to carry state specific to a particular sending device. A pointer to the request.  Requests A request object encapsulates the original request issued by a CPU or I/O device. The parameters of this request are persistent throughout the transaction, so a request object\u0026rsquo;s fields are intended to be written at most once for a given request. There are a handful of constructors and update methods that allow subsets of the object\u0026rsquo;s fields to be written at different times (or not at all). Read access to all request fields is provided via accessor methods which verify that the data in the field being read is valid.\nThe fields in the request object are typically not available to devices in a real system, so they should normally be used only for statistics or debugging and not as architectural values.\nRequest object fields include:\n Virtual address. This field may be invalid if the request was issued directly on a physical address (e.g., by a DMA I/O device). Physical address. Data size. Time the request was created. The ID of the CPU/thread that caused this request. May be invalid if the request was not issued by a CPU (e.g., a device access or a cache writeback). The PC that caused this request. Also may be invalid if the request was not issued by a CPU.  Atomic/Timing/Functional accesses There are three types of accesses supported by the ports.\n Timing - Timing accesses are the most detailed access. They reflect our best effort for realistic timing and include the modeling of queuing delay and resource contention. Once a timing request is successfully sent at some point in the future the device that sent the request will get a response. Timing and Atomic accesses can not coexist in the memory system. This is similar to the TLM nb_transport interface. Atomic - Atomic accesses are a faster than detailed access. They are used for fast forwarding and warming up caches and return an approximate time to complete the request without any resource contention or queuing delay. When an atomic access is sent the response is provided when the function returns. Atomic and timing accesses can not coexist in the memory system. This is similar to the TLM b_transport interface (without any blocking). Functional - Like atomic accesses functional accesses happen instantaneously, but unlike atomic accesses they can coexist in the memory system with atomic or timing accesses. Functional accesses are used for things such as loading binaries, examining/changing variables in the simulated system, and allowing a remote debugger to be attached to the simulator. The important note is when a functional access is received by a device, if it contains a queue of packets all the packets must be searched for requests or responses that the functional access is effecting and they must be updated as appropriate. The Packet::checkFunctionl() is responsible for this.  Timing Flow control Timing requests simulate a real memory system, so unlike functional and atomic accesses their response is not instantaneous. Because the timing requests are not instantaneous, flow control is needed. When a timing packet is sent via sendTiming() the packet may or may not be accepted, which is signaled by returning true or false. If false is returned the object should not attempt to sent anymore packets until it receives a recvRetry() call. At this time it should again try to call sendTiming(); however the packet may again be rejected. Note: The original packet does not need to be resent, a higher priority packet can be sent instead.\nResponse and Snoop ranges Ranges in the memory system are handled by having all slave ports provide an implementation for getAddrRanges This method returns anAddrRangeList with addresses it responds to. When these ranges change (e.g. from PCI configuration taking place) the device should call sendRangeChange on its port so that the new ranges are propagated to the entire hierarchy. This is precisely what happens during init(); all memory objects call sendRangeChange(), and a flurry of range updates occur until everyones ranges have been propagated to all busses in the system.\nTracing and traffic generation The memory system has a number of components that facilitate detailed tracing and traffic analysis, as well as traffic generation and trace playback.\nTraffic analysis and memory trace capture The CommMonitor provides a range of monitoring capabilities, such as histograms for bandwidth, latency, outstanding transactions and burst size, address heat maps, etc. It also provides a well-defined probe interface to analyze packets as they are transferred through the memory system. Currently, gem5 comes with probes to trace memory accesses and compute stack distance histograms. The monitor can be placed anywhere in the memory system by connecting it as a form of extension cord between two existing modules. For example l2cache.mem_side = membus.slave would turn into l2cache.mem_side = l2mon.slave and l2mon.master = membus.slave, assuming a monitor is already instantiated. Check config.json or config.dot.pdf to ensure the monitors are instantiated as expected.\nTo trace memory accesses, a CommMonitor needs to be attached to the memory system and a MemTraceProbe needs to be attached to the CommMonitor. To enable tracing in the probe, specify a trace_file = 'mytrace.trc' parameter. By appending \u0026lsquo;.gz\u0026rsquo; the trace will also be gzipped. For example: monitor.trace = MemTraceProbe(trace_file=\u0026quot;my_trace.trc.gz\u0026quot;)\nThe output trace is using Google\u0026rsquo;s protobuf to get a compact trace with a high-speed encoding/decoding. The trace can be dumped in human-readable format by using the utility script util/decode_packet_trace.py. The trace contains a time stamp, the command and flags of the request, the physical address, the size, and a generic ID. The traffic generator in gem5 also supports replaying these traces.\nTraffic generation and trace playback The TrafficGen module provides a generic framework for synthetic traffic generation and trace playback. Each traffic generator is independent, and has a single master port, through which requests are issued, and responses received. The traffic generator does not care about the data transported, and focuses solely on achieving a specific spatial (address) and temporal (inter-transaction time) distribution.\nThe specific behaviour of the traffic generator is controlled by a text-based configuration file. For an example of such a file, see tests/quick/se/70.tgen/tgen-simple-mem.cfg. In essence the traffic generator is a probabilistic state-transition diagram, where each stats is a specific generator behaviour. The states, such as LINEAR, RANDOM, TRACE and DRAM can be combined into an elaborate graph, and each state also has a large range of configuration options.\nPacket allocation protocol The protocol for allocation and deallocation of Packet objects varies depending on the access type. (We\u0026rsquo;re talking about low-level C++ new/delete issues here, not anything related to the coherence protocol.)\n Atomic and Functional : The Packet object is owned by the requester. The responder must overwrite the request packet with the response (typically using the Packet::makeResponse() method). There is no provision for having multiple responders to a single request. Since the response is always generated before sendAtomic() or sendFunctional() returns, the requester can allocate the Packet object statically or on the stack.   Timing : Timing transactions are composed of two one-way messages, a request and a response. In both cases, the Packet object must be dynamically allocated by the sender. Deallocation is the responsibility of the receiver (or, for broadcast coherence packets, the target device, typically memory). In the case where the receiver of a request is generating a response, it may choose to reuse the request packet for its response to save the overhead of calling delete and then new (and gain the convenience of using makeResponse()). However, this optimization is optional, and the requester must not rely on receiving the same Packet object back in response to a request. Note that when the responder is not the target device (as in a cache-to-cache transfer), then the target device will still delete the request packet, and thus the responding cache must allocate a new Packet object for its response. Also, because the target device may delete the request packet immediately on delivery, any other memory device wishing to reference a broadcast packet past point where the packet is delivered must make a copy of that packet, as the pointer to the packet that is delivered cannot be relied upon to stay valid.  Two memory system models: Classic and Ruby The gem5 simulator includes two different memory system models, Classic and Ruby, that incorporate the above mentioned general memory system components. As the name suggests, the Classic memory system model is inherited from the previous M5 simulator, while the Ruby memory system model is based on the GEMS memory system model of the same name. Each model is complementary and both have unique advantages and disadvantages. This sub-section introduces each model and compares their functionality. Then the next two respective top-level sections describe each model in detail.\nClassic memory system The Classic memory system model provides gem5 a fast, flexible and easily configurable memory system at the cost of detail in the coherency interactions. All objects within the Classic model inherit from MemObject and connect together using ports. In particular, ports support direct point-to-point connections between two MemObjects and buses/crossbars connect two or more MemObjects together. Cache coherence is maintained using an abstract MOESI snooping protocol where state-transitions due to snoops occur instantaneously. Using this methodology, the Classic memory system model has the following advantages and disadvantages:\n Advantages   Fast Forwarding - The Classic model supports atomic accesses, which as stated above, are faster than detailed accesses. This mode of operation is especially advantageous when one needs to fast forward to interesting parts of the execution. It is also the mode used when running gem5 in KVM mode. Speed - Not only does the Classic model support fast atomic accesses, but its timing accesses are relatively fast as compared to Ruby. Ease of Configuration - By simply modifying the python configuration, the Classic model allows one to create an arbitrary memory hierarchy. The Classic\u0026rsquo;s abstract cache coherence protocol automatically extends to any memory hierarchy as long as it is composed of caches, crossbars, and bridges.   Disadvantages   Cache Coherence Flexibility - While the Classic model allows one to create arbitrary systems composed of caches, crossbars, and cpus, the Classic model is restricted to its abstract MOESI snooping protocol. Modifying the protocol requires significant effort. Cache Coherence Fidelity - By not modeling transient states, the Classic model does not model protocol contention as detailed as the Ruby model.  Ruby memory system In contrast to the Classic model, the Ruby memory system model sacrifices simulation speed to provide gem5 a flexible infrastructure capable of accurately simulating a wide variety of memory systems. In particular, Ruby supports a domain specific language called SLICC (Specification Language for Implementing Cache Coherence) where one can define many different types of cache coherence protocols. Essentially SLICC defines the cache, memory, and dma controllers as individual per-memory-block state machines that together form the overall protocol. By defining the controller logic in a higher level language, SLICC allows different protocols to incorporate the same underlining state transition mechanisms with minimal programmer effort.\nUnlike the Classic model, Ruby does not connect all objects together using ports. Instead, ports only connect cpus and devices to the memory system via the RubyPort object. Then within the Ruby memory system, all objects are connected to each other via MessageBuffers. MessageBuffers are similar to ports in that they provide objects a standard communication interface. However, MessageBuffers include a queue that stores messages while ports do not. Messages cannot be enqueued and dequeued at the same simulated cycle and thus communication across a message buffer is not instantaneous. The result is MessageBuffers only support timing accesses and cannot support the instantaneous atomic and functional accesses.\nTo summarize, the Ruby memory system model has the following advantages and disadvantages:\n Advantages   Cache Coherence Flexibility - Utilizing SLICC, Ruby can implement a wide variety of cache coherence protocols, from directory to snooping protocols and several points in between. Fidelity - Ruby accurately models both cache coherence and network related features in the memory system. In particular, SLICC\u0026rsquo;s message trigger event methodology accurately models transient state timing. Also the Garnet network model integrated in Ruby accurately models network contention and flow control.   Disadvantages   Fast Forwarding - Ruby does not support atomic accesses and thus does not have reasonable fast forwarding capability. Speed - As compared to the Classic model, Ruby is relatively slow. This is especially true when using the Garnet network model. Ease of Configuration - While SLICC is a powerful tool to model a wide variety of cache coherence protocols, the resulting protocols are optimized for a specific cache hierarchy configuration. As such, it is difficult to simply extend protocols to another level of cache.  "
},
{
	"uri": "http://localhost/deprecated/mercurial-queues/",
	"title": "Mercurial Queues",
	"tags": [],
	"description": "",
	"content": " Repository Management Problem gem5 users typically opt to freeze their repository at a particular changeset when starting a new research project. This approach has several downsides:\n It discourages users from contributing back any useful changes they may develop. If a useful change is added upstream, it\u0026rsquo;s a long, tedious process to update.  If a user chooses to keep their local repository up-to-date with the source tree they typically use named branches and merge any upstream changes into their branches. This approach also has its downsides:\n If any local change needs to be updated, it requires a separate commit. If you have several small, unrelated changes, separate branches must be maintained. Upstream changes must be merged into the local branches.  A tool that overcomes these problems is the mercurial queue extension.\nMercurial Queues The mercurial queue extension is a powerful tool that allows you to:\n Manage small changes easily as a set of well-defined patches. Edit previous patches without having a new commit. Keep your local changes cleanly separated from upstream changes. Prevent changes from being recorded in the project history until they are ready.  This guide will give a brief overview of the basic functionality of mercurial queues, which should be enough information to enable you to effectively manage your local changes and allow you to contribute them to the reviewboard if you choose to do so. However, there are many more advanced uses of mercurial queues that may be beneficial. See also the MQ tutorial on the Mercurial wiki, the Mozilla developer page on MQ, and the chapters on \u0026ldquo;Managing change with Mercurial Queues\u0026rdquo; and \u0026ldquo;Advanced uses of Mercurial Queues\u0026rdquo; from the Mercurial book.\nBasic MQ commands Help Command  hg help mq \u0026mdash; Gives a list of mercurial queue commands and a brief description of each.  Creating and handling patches  hg qnew change1.patch -m \u0026quot;commit message\u0026quot; \u0026mdash; Create a new patch named \u0026ldquo;change1.patch\u0026rdquo; with a commit message. hg qpop \u0026mdash; Pop topmost patch off the queue. hg qpush \u0026mdash; Push next patch in the series onto the queue. hg qrefresh \u0026mdash; Add any local changes to the topmost patch. hg qfinish \u0026mdash; Remove patch from the queue and make a permanent part of the repo history.  Checking the status of patches in the queue  hg qapplied \u0026mdash; List all applied patches in the queue. hg qseries \u0026mdash; List all patches in the current series (this includes even patches that aren\u0026rsquo;t applied). hg qdiff \u0026mdash; Display the diff for the applied patch at the top of the queue ht qtop \u0026mdash; List the patch at the top of the queue.  Adding patches from other queues  hg qimport -e pre_existing.patch \u0026mdash; Adds a pre-existing patch called \u0026ldquo;pre_existing.patch\u0026rdquo; to the local queue.  Advanced mercurial queue usage Here will give a list of some of the advanced uses of mercurial queues, and provide pointers to more in-depth information about them.\n Queue Guards - Guards will allow you to manage patches by placing \u0026ldquo;guards\u0026rdquo; on them, i.e., you may perform actions on a specific set of patches based on the guard(s) placed on them. See this guide for more information.   Multiple Queues - You can maintain multiple patch queues, this could be useful for grouping sets of related patches together, while keeping them separate from other queues. More info can be found here.   Versioning Your Patch Queues - You can even maintain the changes to your patches in your repository. This allows you to keep track of the change within your patches, use hg push/pull to share the patches with others, etc. More info about this feature is here.  Example Mercurial Queue Use Enable the MQ extension To enable the mercurial queue extension, simply add the following to your .hgrc file:\n[extensions] hgext.mq =\nSimple workflow with MQs Here is a simple example outlining basic MQ usage:\n# clone a clean copy of gem5 hg clone http://repo.gem5.org/gem5 # initialize a new mercurial queue cd ./gem5 hg init --mq # make some local changes and turn them into a patch hg qnew change1.patch -m \u0026quot;cpu: made some changes to the cpu model\u0026quot; # we have some more changes that we want to turn into a separate patch hg qnew change2.patch -m \u0026quot;cache: made some changes to the cache\u0026quot; # now you want to make some more changes and include them in change1 # make sure change1 is at the top of the queue hg qtop \u0026gt;\u0026gt;\u0026gt; change2.patch # it's not, so we have to pop change2 off the queue hg qpop hg qtop \u0026gt;\u0026gt;\u0026gt; change1.patch # now it's the top patch. make the necessary changes and update hg qrefresh # re-apply change2 hg qpush # let's check that all of our patches are applied hg qapplied \u0026gt;\u0026gt;\u0026gt; change1.patch \u0026gt;\u0026gt;\u0026gt; change2.patch  Rebase Extension The rebase extension is a useful tool that allows you too keep your local changes \u0026ldquo;detached\u0026rdquo; from the mainstream repository while still keeping them compatible with it. This extension will essentially reapply your local changes, i.e., the changes in your patch queue, on top of the up stream changes. More info about the rebase extension, and its advanced uses, can be found here.\nExample use of the rebase extension To enable the rebase extension, simply add the following to your .hgrc file:\n[extensions] rebase =\nSuppose you have some patches applied in your local patch queue, then you do a pull request from the upstream repo:\nhg pull -u\nNow, simply rebase your local changes:\nhg rebase\n"
},
{
	"uri": "http://localhost/developer/microcode-assembler/",
	"title": "Micro-code assembler",
	"tags": [],
	"description": "",
	"content": " Syntax The assembler is responsible for taking listings similar to traditional assembly and processing it into a set of python objects. The listing is broken into sections which represent macroops or sections of a microcode ROM. In each section, there can be assembler directives and microops. Microops can be preceeded with labels which may be local, or in the case of the ROM, global. The distinction is to allow local labels to be duplicated between sections, but global labels to globally mark a position to, for instance, branch into the ROM or amongst sections of the ROM.\n local labels aren't really local to ROM sections at the moment --Gblack 19:52, 5 June 2007 (EDT)\nMicro assembly syntax allows directives which provide control over the assembler (indirectly), and actual microop instantiations. The directives are names preceeded by a \u0026ldquo;.\u0026rdquo; and followed by their arguments. The arguments should follow legal python syntax and start immediatelly after the white space trailing the directive name until an unescaped newline or semicolon, the two line terminators. The following are some examples:\n.example \u0026quot;some text %s\u0026quot; % \u0026quot;other text\u0026quot;, 5*4 .harder_example python_variable \\ arguments on the next line .example_with_semicolon argument; .example_with_semicolon2 argument\\; ;  Microops are defined similarly, but without the preceeding \u0026ldquo;.\u0026rdquo; and with optional labels in front of them. Labels are names followed by a colon. There may be more than one label for a particular microop. Lables which are intended to be global should be preceeded by the keyword \u0026ldquo;extern\u0026rdquo;. The following is an\nexample: extern my_macroop: top_of_loop: add destReg, sourceReg, 4*bytes_in_word branch less_than, top_of_loop  To define a macroop, there are two options. These options correspond to whether the macroop will be generated combinationally by the decoder, or if the macroop will refer to code in the ROM. In the combinational case, The syntax is:\ndef macroop macroop_name { [microops and directives] };  A macroop object will be created and given the name macroop_name, and then populated using the microops and directives in it\u0026rsquo;s body.\nIn the ROM based case, the syntax is simpler and looks like the following:\ndef macroop macroop_name (target);  A macroop object will be created and given the name macroop_name, but instead of being filled with microops, it will only be told what it\u0026rsquo;s target is which should be a label.\nTo define a section of the microcode ROM, use syntax like:\ndef rom { [microops and directives] };  A new rom object will not be created. Instead, the microops and directives defined in the body will be used to extend the contents of an existing ROM object.\nComments are ignored by the assembler and have two forms of syntax. Single line comments begin with a # and continue to the end of the line. Multiline comments begin with a /* and extend to the following */.\n#This is a single line comment label: some_microop #Comments can be here too /* * A * multiline * comment */  ISA Parser Interface In order to allow customizing the assembler to work with different ISAs, a lot of the work of the assembler is actually performed by python objects and classes passed in with the listing. There is both a combinational and ROM based macroop class which are instantiated by the assembler to hold the macroops it recognizes in the listing, and in the combinational case, to support any assembler directives. Instances of this class must provide a dict which maps from directive names to their implementations. ROM macroops are instantiated and set to their target. A ROM object is passed into the assembler and is filled with microops which are defined in ROM sections.\nEach of the microops the assembler should recognize need to have a corresponding python class. The arguments of the microop are passed to the constructor of the python class, and the object created is what is stored in the macroops. A dict of the mappings between mnemonics and these classes is also passed into the assembler. After each microop is created, it\u0026rsquo;s added into its container using the container\u0026rsquo;s add_microop method.\nPreprocessor The microcode assembler could be outfitted with a preprocessor, if one is needed. Because the assembler is instantiated from python and just passed a string containing the listing, it does not need to be aware of the preprocessor. Simple process the input string in whatever way fits the need, and then send the resulting output to the assembler. A common preprocessor may be developed which could follow nasm style syntax. gas, the gnu assembler, doesn\u0026rsquo;t use a preprocessor and instead relies on C\u0026rsquo;s.\n"
},
{
	"uri": "http://localhost/benchmarks/moby/",
	"title": "Moby",
	"tags": [],
	"description": "",
	"content": " AsimBench is renamed as Moby!\n This page describes all the necessary files and modifications to run Android and Moby on gem5 using the ARM ISA. BBench-gem5 introduces more detailed information about how to set Android running on gem5, and how to build Android File System and Kernel.\nAndroid Full-System Files These files contain everything you need to get Android, and Moby, up and running on gem5.\nAll these files can be downloaded from here.\n vmlinux.smp.ics.arm.asimbench.2.6.35 \u0026ndash; Pre-compiled Android kernel. Compared to the 2.6.35 version Android kernel, this kernel only modifies the screen size of simulated mobile device, which is specified to 800 x 480.   ARMv7a-ICS-Android.SMP.Asimbench.tar.gz \u0026ndash; Disk image with a pre-compiled ICS file-system. This disk image contains all the files for all the applications included in AsimBench. The related files are mainly stored in the directories data/app, data/data of the image.   sdcard-1g.tar.gz \u0026ndash; Disk image with applications\u0026rsquo; data. This disk image simulates the SDcard partition of mobile devices.   {k9mail, adobe, sinaweibo, \u0026hellip;}.rcS \u0026ndash; Execution scripts for each application in gem5.  Running Moby on Android with gem5  Both the disk image and the SDcard image should be loaded by gem5, and the SDcard image is automatically mounted under the directory /mnt/sdcard of Android file system by default.   Here is an example of loading two disk images in gem5. In the file config/common/FSConfig.py  def make ArmSystem(...)  ...  self.cf0 = CowIdeDisk(driveID='master')  self.cf2 = CowIdeDisk(driveID='master')  self.cf0.childImage(mdesc.disk())  self.cf2.childImage(disk(\u0026quot;sdcard-1g.img\u0026quot;))  # default to an IDE controller rather than a CF one  # assuming we've got one  try:  self.realview.ide.disks = [self.cf0, self.cf2]  except:  self.realview.cf_ctrl.disks = [self.cf0, self.cf2]  ...\nPublications If you use Moby (or AsimBench) in your work please cite our paper which will be appeared in ISPASS\u0026rsquo;2014.\nYongbing Huang, Zhongbin Zha, Mingyu Chen, Lixin Zhang. Moby: A Mobile Benchmark Suite for Architectural Simulators. IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), Monterey, CA, March 2014.\n"
},
{
	"uri": "http://localhost/ruby/modular-coherence/",
	"title": "Modular coherence",
	"tags": [],
	"description": "",
	"content": " Introduction We need a way to add/extend/replace cache coherence protocols in a modular fashion. The original cache module had coherence factored out into a separate object, but the interface was totally inadequate for doing anything interesting. The redesigned memory system from 2.0 integrated coherence back in to the cache module as a temporary step to eliminate the useless bloat caused by the original separation. It\u0026rsquo;s now time to go back and re-introduce this modularity, but to do it right this time.\nWe are hugely inspired by SLICC, the coherence protocol description language/compiler that\u0026rsquo;s part of the GEMS simulator from Wisconsin. Following their example, we plan on developing a similar domain-specific language (DSL) and compiler for coherence protocols. SLICC is fairly closely tied to the GEMS framework, so it\u0026rsquo;s not possible to adopt it directly for M5. In addition, there are a number of aspects of SLICC that we would like to change, partly based on personal bias and partly based on areas where we think we can improve on their design. To try and come up with the best solution we can, we will approach this design as a clean sheet, and not automatically adopt any part of SLICC by default. At the same time, SLICC is a terrific working example, so many of our design decisions will probably end up being \u0026ldquo;let\u0026rsquo;s do this like SLICC, except\u0026hellip;\u0026rdquo;. Correspondingly, most of the initial design requirements and preferences listed below are \u0026ldquo;things we should do differently than SLICC\u0026rdquo;, with the implicit assumption that most of the issues not listed below will be handled similarly.\nDesign Requirements Protocol inheritance This is Brad\u0026rsquo;s #1 request for improving on SLICC. Related protocols should be able to share common elements. New protocols that are minor modifications or extensions of existing protocols should be describable by inheriting from the existing protocol and describing only the differences.\nImplicit hit path The coherence protocol should not be involved on cache hits. Requiring a call into the protocol object on hits will drastically reduce our opportunities for performance optimization of this critical path in the simulator. As is often the case, simulation reflects reality, and no sane coherence protocol would intrude on the hit path of a real implementation either. The Tag object is currently consulted on hits (to enable replacement algorithm bookkeeping), so if someone desperately needs to collect information on hits they can implement a new Tag object to do that.\nThe implicit cache hit path will use common, protocol-independent access permission bits to determine if a block is readable and/or writable and determine whether an access is a hit accordingly. The hit path will set a protocol-independent dirty bit on writes. This bit can be incorporated into the protocol state, e.g., to differentiate clean exclusive vs. modified states.\nSupport internal cache timing Though we currently don\u0026rsquo;t model any internal cache timing, we probably should eventually, and we want to make sure the language is up to the task. For example, having a getState() function that we expect to return immediately means that we can\u0026rsquo;t explicitly model the tag array access latency. It might be better to assume that the tag lookup happens explicitly when a message is first processed and then assume that the controller (and thus the coherence code) is handed both the message and the tag state right from the start.\nAutomatic and Powerful Protocol Debugging Support Protocol designers will likely spend the majority of their time debugging protocols. The protocol support should make debugging as easy as possible. In particular, concise and informative auto generated DPRINTFs are a high-priority requirement. The key is outputting debug traces with enough information to completely follow all state transitions while allowing a single (\u0026lt; 1 GB) trace to span millions of cycles. Furthermore, the protocols must detect invalid transition errors as soon as they occur.\nVirtual Channels To encounter and solve protocol deadlock issues the coherence protocols must support virtual channels. Therefore the network connecting different memory components must support virtual channels as well.\nDesign Preferences Balance Explicit State Machines vs. Attributes Ideally, we would like to find a balance of using an explicit state-machine approach versus attributes. Currently, SLICC and M5 highlight the extremes of each approach. The current M5 coherence protocol uses memory command attributes to allow clean code; see the MemCmd class in src/mem/packet.{hh,cc}. For example, all requests requiring an exclusive copy of a block (Write, ReadEx, Upgrade, StoreCond, Swap) have the NeedsExclusive attribute set, so the handler simply uses \u0026ldquo;if (req-\u0026gt;needsExclusive())\u0026rdquo; to determine whether to check for exclusivity and to request an exclusive copy if needed. While attributes work well for simple and abstract protocols, it is likely an attribute-only approach is insufficient to describe a complex and more realistic protocol.\nIn contrast, SLICC uses different explicit events and states to signify all easily visible protocol differences. The advantage of this approach is all differences in events and states can be easily tracked in the protocol. Therefore many errors in complicated protocols can be found with easily traceable invalid transition errors, rather than hard to isolate incorrect attribute handling errors. The disadvantage is the language specification files are quite long and verbose, and are susceptible to cut-and-paste errors. SLICC does support attributes as well, but they are only utilized within actions.\nOverall, the best solution may be to support for both models, using an explicit state-machine model for the general coherence controller structure, but providing explicit support for attributes as well so that minor variations along common paths can be implemented via small if-conditions on a common state-machine transition.\nEncode Protocol-Independent State in Protocol-Specific State Ideally, we would like the encode the cache state to combine both the protocol-independent state (access attributes, etc.) and protocol-specific state. For example, M5 currently supports CacheBlkStatusBits in src/mem/cache/blk.hh which for the most part are protocol independent (except the dirty bit). The block state model needs to be expanded to allow protocol-specific state, but we should consider continuing to use encoding tricks to avoid redundantly storing both the block access bits and the full coherence state separately. This encoding means that setting the protocol-level cache state implicitly changes the protocol-independent access permissions, and vice versa (e.g., setting the dirty bit implicitly modifies the protocol-level state appropriately). Therefore, with protocol-independent access permissions encoded in the state, the cache access path can determine hits and misses without invoking the cache controller logic and without requiring separate logic to set permissions.\nIn contrast, SLICC doesn\u0026rsquo;t directly associate semantics with requests or states, and you end up with code like this:\n // Set permission  if ((state == State:I) || (state == State:MI_A) || (state == State:II_A)) {  changePermission(addr, AccessPermission:Invalid);  } else if (state == State:S || state == State:O) {  changePermission(addr, AccessPermission:Read_Only);  } else if (state == State:M) {  changePermission(addr, AccessPermission:Read_Write);  } else {  changePermission(addr, AccessPermission:Busy);  }\nSupport Hardware Prefetching without Requiring Specific Protocol Support Another advantage with encoding protocol-independent state within the cache state, is hardware prefetching can possibly be supported in all designs without requiring specific protocol support. We should be careful to ensure hardware prefetches don\u0026rsquo;t impede demand requests. Hopefully, we can enforce this policy without impacting the protocol specific logic. In contrast, GEMS requires each protocol to explicitly handle hardware prefetches separately.\nNo new imperative language Let\u0026rsquo;s not invent a new language for imperative computation. I admit that the \u0026ldquo;treat C++ code blocks as strings\u0026rdquo; approach of the ISA description language is a bit cheesy, but for the most part it works. The main drawback is that you can\u0026rsquo;t parse the code to analyze its behavior (though the ISA parser extracts a lot of information in an imperfect way using regexps; see Code parsing), but I think we\u0026rsquo;ll have even less need of that here than the ISA parser does.\nEven if we do feel the need to fully parse imperative computations, or to restrict what people can do in them, I\u0026rsquo;d rather see us do a subset of C++ than invent something new. (That might actually be useful if we can reuse the C++-subset parser for the ISA description language.) Unfortunately, this is easier said than done; one big challenge is getting the context set up right so the parser can figure out which symbols are types, preprocessor macros are expanded, etc.\nNote that some of the SLICC design notes talk about wanting to make the language synthesizable to hardware, which is a reasonable motivation for them to have a limited imperative language. We don\u0026rsquo;t have that goal, so there\u0026rsquo;s much less reason to be restrictive.\nDesign Challenges Interfacing generic with protocol-specific bits Though it\u0026rsquo;s reasonable to specialize caches and interconnects based on the coherence protocol, we don\u0026rsquo;t want to recompile every object that connects to the memory system (CPUs, devices, etc.) for the protocol too. Yet these devices do need to send out and respond to basic memory requests (read, write, instruction fetch, etc.). One possible solution is to leverage inheritance, and have a base coherence protocol that defines the set of messages which all CPUs and devices can use. This base protocol will probably be abstract (if only by having all the required functions call fatal()). Then generic memory system clients can depend only on the base protocol, and as long as the derived protocols extend it in a compatible fashion, everything will work.\nNote that this solution will constrain how we implement inheritance: we can\u0026rsquo;t do it entirely in the DSL and have it spit out flattened C++, as we\u0026rsquo;ll need this base protocol to be separate in C++ so that the compilation of CPU and device models can be independent of the specific protocol.\nSystem Configuration Issues (Steve I\u0026rsquo;m not sure what public information you want to discuss here.)\nFixed vs. flexible aspects of cache organization At some level, all the coherence protocols have to fit into a common framework; we don\u0026rsquo;t want each protocol having to define from scratch what it means to be a cache. Figuring out where the line is between the fixed components of the common framework and the pieces that need to be felxibly defined by the coherence protocol is a challenge, particularly in these areas:\nMSHR Model From brief discussions with Brad, it sounds like caches in SLICC protocols have limited ability to buffer requests to busy blocks. Multiple read requests may get buffered/merged by building a bitmask of requesters, but otherwise requests are either NACKed or recycled to the top of the input queue. In contrast, M5 has an MSHR model in which each MSHR has a finite number of \u0026ldquo;targets\u0026rdquo;, each of which can buffer a request to a busy block. This model encompasses both the handling of overlapping requests from an out-of-order CPU in an L1 cache, handling requests from multiple CPUs/L1s in an L2 cache, and so on. Depending on the system configuration, it is expected that these MSHRs can be sized to handle the worst-case number of outstanding accesses from upstream CPUs/caches.\nThe MSHR model seems to provide a nice consistent framework for handling multiple requests; once a response arrives, the buffered target requests are replayed almost as if they had just been received, that is, in effect the same message is re-processed against the new post-response cache state as if that state had been there when the request was first received. While this model seems like it should work for a variety of protocols, do we want to hard-code it as part of the framework? If not, how do we expose this kind of structure in the protocol description language in such a way that its behavior can be modified?\nOne possible shortcoming of the MSHR model is that it would need to be enhanced to take advantage of networks \u0026amp; protocols that support broadcast or multicast responses.\nCache Controller to Cache Object Ratio There are a few cases where SLICC uses a single controller to manage multiple caches, including separate Harvard-style L1 I and L1 D caches, as well as exclusive L1/L2 caches. It\u0026rsquo;s unclear if this is really desirable or is an artifact of SLICC not easily supporting a more decoupled approach. Conceptually it seems both more straightforward and more realistic to restrict a single controller to managing a single cache, but we don\u0026rsquo;t want to make our model less flexible than SLICC unless we\u0026rsquo;re really sure that\u0026rsquo;s the right thing to do.\nBasic Design There\u0026rsquo;s not much here yet, unfortunately.\nSimulator implementation The coherence protocol implementation (which will eventually be auto-generated from the DSL description) should consist of one or more C++ objects. The cache and interconnect models will be modified to take a coherence protocol object as a template parameter. The protocol object(s) will define types used to define the allowable commands, define the protocol-specific state included in Packet and CacheBlk classes, etc.\n Should the C++ protocol object(s) form an inheritance hierarchy that mirrors the inheritance relationship of the protocols in the DSL?  Domain-specific language We should write the parser in PLY and the compiler in Python because we know them, and to avoid introducing any new tool dependencies.\nDevelopment Plan  Separate the current coherence protocol from the cache module into a separate policy object (or objects). This will give us an initial interface definition and an idea of what kind of code the description language will need to generate. Implement a few different protocols using this interface, expanding and modifying it as needed. Ideally these protocols will be significantly different from the base protocol, e.g., directory based, point-to-point, etc. This will help us flesh out the interface, and get started on implementing features that we\u0026rsquo;ll need for more advanced protocols (such as virtual channels) that will be reflected in the description language but are largely orthogonal to it. Some candidates would be Geoff Blake\u0026rsquo;s LogTM implementation or a manual translation of an existing SLICC protocol from the GEMS distribution. A side benefit will be that those of us who need a particular protocol can implement it at this stage and not have the description language be on their critical path. There is the down side that there will be some extra effort to reimplement these protocols in the description language, but if our language is as concise as it should be then this effort should be minimal. Design and implement the description language and associated compiler. This work can overlap with the previous step somewhat, but we should avoid getting too far ahead of ourselves in case the previous step uncovers some features that require significant modifications to the language structure.  --Brad 13:41, 18 July 2008 (EDT)\n"
},
{
	"uri": "http://localhost/docs/multi-programmed-workloads/",
	"title": "Multi-programmed workloads",
	"tags": [],
	"description": "",
	"content": " Running In SE mode, simply create a system with multiple CPUs and assign a different workload object to each CPU\u0026rsquo;s workload parameter. If you\u0026rsquo;re using the O3 model, you can also assign a vector of workload objects to one CPU, in which case the CPU will run all of the workloads concurrently in SMT mode. Note that SE mode has no thread scheduling; if you need a scheduler, run in FS mode and use the fine scheduler built into the Linux kernel.\nTerminating There are several options when deciding how to stop your workloads:\n Terminate as soon as any thread reaches a particular maximum number of instructions. This is equivalent to max_insts_any_thread. The problem here is that multithreaded programs are non-determinstic, and there is no way to determine how many instructions the other threads will have executed. The amount of work done per instruction could change as well if more or less time is spent waiting for other threads. The benefit of this approach is that all threads are running fully until the simulation terminates. Terminate once all threads have reached a maximum number of instructions. This is equivalent to max_insts_all_threads. With this option you can be sure all threads do at least a certain amount of work, but threads that reach the maximum continue executing and there\u0026rsquo;s no way to know how much extra work these other threads will do. Terminate each thread individually after it executes a particular number of instructions. This option is not currently supported. All threads will do the same amount of work which avoids some of the problems mentioned above, but now the threads may not all be running for the entire simulation. After some threads have finished, the remaining threads will have less competition for resources, and the compute resources will have less work to do. Terminate each thread individually after it reached a particular, per thread number of instructions. This is basically the same as the previous option but allows tuning the number of instructions executed per thread. It is also not currently supported.  Merging Single Workload Checkpoints "
},
{
	"uri": "http://localhost/docs/nic-devices/",
	"title": "NIC devices",
	"tags": [],
	"description": "",
	"content": " The gem5 simulator has two different Network Interface Cards (NICs) devices that can be used to connect together two simulation instances over a simulated ethernet link.\nGetting an list of packets on the ethernet link You can get a list of the packet on the ethernet link by creating a Etherdump object, setting it\u0026rsquo;s file parameter, and setting the dump parameter on the EtherLink to it. This is easily accomplished with our fs.py example configuration by adding the command line option \u0026ndash;etherdump=. The resulting file will be named  and be in a standard pcap format. This file can be read with wireshark or anything else that understands the pcap format.\n"
},
{
	"uri": "http://localhost/deprecated/nates-wish-list/",
	"title": "Nate&#39;s wishlest",
	"tags": [],
	"description": "",
	"content": " Framework  Parallelize M5  Converting everything to support the new Params mechanism is a prerequisite Make various global structures thread safe (e.g. Instruction cache, Statistics, etc.)  Unify full system and syscall emulation modes into a single build Unify all ISAs into a single build  This requires fixes to the way endianness is handled in the memory system. Add a NOISA option that doesn\u0026rsquo;t have CPUs till this happens.  Modularize the build, allowing people to pick and choose which SimObjects get compiled in. Make Python/C++ I/O systems work much better together. i.e. C++ can cprintf to python file objects, or Python can print to C++ iostreams. Handle reading and writing of gzipped files Make switching between CPU models way easier Get simulations with \u0026gt; 100 cores working  CPU Models  Support something resembling direct execution using KVM. Have a very fast model that dynamically compiles like PIN (or even use PIN). Make a configurable in-order model Make a very fast functional CPU model that doesn\u0026rsquo;t have any stats or anything like that and is as fast as possible. Clean up wakeup/sleeping/interrupts so the CPU models are more consistent and the interface isn\u0026rsquo;t so horrendous.  Monitoring  Listen on a socket with cherrypy and allow people to browse and query the status of a simulation.  Configuration  Clean up SimObject.py significantly Clean up configs/* and make it much more modular and library like Make a top level DictParam that is like a VectorParam  Memory System  Clean up interrupt handling and route interrupts through the memory system as packets. Implement something like SLICC for M5. Implement the ARM instruction. Add support to flush caches.  Disk Models  Support something like qemu\u0026rsquo;s qcow2 disk image  This is actually bsd code, so we can steal it Figure out how to mount it natively on linux so we don\u0026rsquo;t have to convert back and forth to add stuff  Add host Linux file system support so the guest can easily access files on the host.  Maybe support the host side of vmhgfs since the guest side already exists and is GPL.   Disk Images  Just clean this stuff up Figure out a replacement for ptxdist.  Currently think Gentoo is probably the best option.   Statistics  Make all stats types actually work. Move lots of stats processing to python. Keep collection in C++, but all of the printing stuff should go to python. Add a sqlite backend. Clean up and better integrate the graphing stuff.  Ethernet  Add Ethernet overheads (preamble, trailer, inter frame gap) to EtherLink model. Build and Ethernet switch model. Get EtherTap working.  Keep the stuff that must run as root separate.   "
},
{
	"uri": "http://localhost/ruby/network-test/",
	"title": "Network test",
	"tags": [],
	"description": "",
	"content": " This is a dummy cache coherence protocol that is used to operate the ruby network tester. The details about running the network tester can be found here.\nRelated Files  src/mem/protocols  Network_test-cache.sm: cache controller specification Network_test-dir.sm: directory controller specification Network_test-msg.sm: message type specification Network_test.slicc: container file   Cache Hierarchy This protocol assumes a 1-level cache hierarchy. The role of the cache is to simply send messages from the cpu to the appropriate directory (based on the address), in the appropriate virtual network (based on the message type). It does not track any state. Infact, no CacheMemory is created unlike other protocols. The directory receives the messages from the caches, but does not send any back. The goal of this protocol is to enable simulation/testing of just the interconnection network.\nStable States and Invariants    States Invariants     I Default state of all cache blocks    Cache controller  Requests, Responses, Triggers:  Load, Instruction fetch, Store from the core.   The network tester (in src/cpu/testers/networktest/networktest.cc) generates packets of the type ReadReq, INST_FETCH, and WriteReq, which are converted into RubyRequestType:LD, RubyRequestType:IFETCH, and RubyRequestType:ST, respectively, by the RubyPort (in src/mem/ruby/system/RubyPort.hh/cc). These messages reach the cache controller via the Sequencer. The destination for these messages is determined by the traffic type, and embedded in the address. More details can be found here.\n Main Operation:  The goal of the cache is only to act as a source node in the underlying interconnection network. It does not track any states. On a LD from the core:  it returns a hit, and maps the address to a directory, and issues a message for it of type MSG, and size Control (8 bytes) in the request vnet (0). Note: vnet 0 could also be made to broadcast, instead of sending a directed message to a particular directory, by uncommenting the appropriate line in the a_issueRequest action in Network_test-cache.sm  On a IFETCH from the core:  it returns a hit, and maps the address to a directory, and issues a message for it of type MSG, and size Control (8 bytes) in the forward vnet (1).  On a ST from the core:  it returns a hit, and maps the address to a directory, and issues a message for it of type MSG, and size Data (72 bytes) in the response vnet (2).  Note: request, forward and response are just used to differentiate the vnets, but do not have any physical significance in this protocol.   Directory controller  Requests, Responses, Triggers:  MSG from the cores    Main Operation:  The goal of the directory is only to act as a destination node in the underlying interconnection network. It does not track any states. The directory simply pops its incoming queue upon receiving the message.   Other features  - This protocol assumes only 3 vnets.  It should only be used when running the ruby network test.   "
},
{
	"uri": "http://localhost/deprecated/new-regression/",
	"title": "New regression framework",
	"tags": [],
	"description": "",
	"content": " We\u0026rsquo;d like to revamp the regression tests by moving to a new framework. This page is intended to host a discussion of features and design for the new framework.\nAli\u0026rsquo;s plan for a new implementation  Use pytest It has, by far, the best documentation of any of the python testing frameworks and seems to be the most active Seems to be completely extensible via python plugins and hooks Supports outputting JUnit XML incase we want to use a continuous integration solution such as Jenkins or Hudson The pytest xdist and plugin support running tests on multiple-cpus or multiple machines Good collection of tasks and tutorials  How things would work  Marks may be assigned to tests either with python decorators or a class attribute if we want to stay python 2.5 compatible  The decorators would probably include cpu model, memory system, ISA, mode, and run length. We might want to use pytest_addoption to be able to pass lists specifically for each of the decorators and generate tests that match appropriately with this Alternatively we could use pytest-markfiltration although the syntax can be rather contrived   Outstanding Questions  How would we do test discovery?  pytest will search py files looking for tests Files can match a pattern, classes in files can match a pattern or functions can match a pattern or it can only match things that inherit from Python UnitTest  Should we use xunit style or func args style setups? Should we have a class that inherits from Python.UnitTest and does the heavy lifting or should we have a completely separate class that does the heavy lifting and use a factory class to create a bunch of instances of the seperate class? Should gem5 be called as a library or on the command line? How should we store output files? Same way we do now? should each directory just have a init.py and then the tests can be referred to as long.linux_boot.arm.linux.o3?  Desirable features  Ability to add regressions via EXTRAS  For example, move eio tests into eio module so we don\u0026rsquo;t try to run them when it\u0026rsquo;s not compiled in If we used py.test on the build directory that would find the eio code or whatever else is in there and pick them up. This could be through any of the methods described above \u0026ndash;Saidi 18:09, 22 August 2011 (PDT)  Ability to not run regressions for which binaries or other inputs aren\u0026rsquo;t available  This could be done easily based on the BaseClass that we use and tell about the file dependencies. e.g. if not dependencies: pytest.skip(\u0026quot;test binaries unavailable\u0026quot;) \u0026ndash;Saidi 18:09, 22 August 2011 (PDT) With maybe some nice semi-automated way of downloading binaries when they\u0026rsquo;re publicly available Perhaps we should make use of the large file extensions to mercurial or have a test binaries repository??  Graph run time, performance results over time  Headline text  Better categorization of tests, and ability to run tests by category, e.g.:  by CPU model by ISA by Ruby protocol by length I believe these can all be satisfied via decorators and command line options we define \u0026ndash;Saidi 18:09, 22 August 2011 (PDT)  More directed tests that cover specific functionality and complete faster. Running spec benchmarks is important but spends a lot of time doing the same thing over and over. Those should only be a component of our testing, not almost all of it like it is now. This is a desirable feature of our testing strategy, not necessarily something that impacts the regression framework.  This is future work. needed for sure, but a bit orthoginal \u0026ndash;Saidi 18:09, 22 August 2011 (PDT)  Better checkpoint testing  I believe we can create dependent tests, although it\u0026rsquo;s an open question exactly how this interacts with the xdist stuff. \u0026ndash;Saidi 18:09, 22 August 2011 (PDT) some of this doesn\u0026rsquo;t really depend on the regression framework, just needs new tests e.g., integrating util/checkpoint-tester.py  I would rather duplicate/steal the functionality into a Base class for regression testing \u0026ndash;Saidi 18:09, 22 August 2011 (PDT)   Support for random testing (e.g., for background testing processes)  Random latencies? Random testing a la memory testers but with different seeds, longer intervals  I don\u0026rsquo;t think there is anything preventing us from calling random.randint() in a test, and it\u0026rsquo;s all python \u0026ndash;Saidi 18:09, 22 August 2011 (PDT)   Decouple from SCons  Avoid having scons dependency bugs force unnecessary re-running of tests, particularly for update-refs  not quite sure how we would do updaterefs with py.test. I\u0026rsquo;ve been using a 4 line bash script anyway, because I don\u0026rsquo;t want to update every test, only the failed ones \u0026ndash;Saidi 18:09, 22 August 2011 (PDT)  Don\u0026rsquo;t rely on scons to run jobs\u0026hellip; running scons -j8 with a bunch of tests and a batch queing system means that 8 cpus are consumed, even if there is only one job running. Either make scons be able to submit the jobs or have something else that manages the jobs and their completion status  pytest should be able to do this, again, open question it if does it via multi-threads and bsub -K on each job or if we should have pytest directly connect to running machines.   Easy support for running separate tests where only the input parameters differ  As long as we do a good job creating the base class, this should be trivial \u0026ndash;Saidi 18:09, 22 August 2011 (PDT) For example, several protocols utilize different state transitions depending on configuration flags. It would be great if we could test these without having to create new directories and tests.  Big question is how do we store all of the file data, especially when the stats are python objects (hint hint nate). Perhaps putting everything in a SQLite DB might not be a bad approach so we don\u0026rsquo;t need complex directory structures \u0026ndash;Saidi 18:09, 22 August 2011 (PDT)  Similarly, we could/should test topologies this way as well.  Automated way to use nightly regressions as a basis for updating \u0026ldquo;m5-stable\u0026rdquo;  I would vote to defer this until we have a continuous integration solution \u0026ndash;Saidi 18:09, 22 August 2011 (PDT) How do you identify the last working revision? (from Ali) Maybe need a bug-tracking system so we could record facts like \u0026ldquo;changeset Y fixes a bug introduced in changeset X\u0026rdquo; then we could automatically exclude changesets between X and Y, but we don\u0026rsquo;t have that. (from stever)  Better definitions of success criteria.  E.g. Stats were changed, but output is all still correct vs simply passed and failed. (Passed, stats diffs, failed) For example you could say that the terminal output changing is fail, or the stdout and spec binary outputs changing are failed, but a 1% difference in stats is a stats difference, which needs to be addresses I envision this as providing reasonable certainty that if you create a change you know will modify the stats, you have a quick verification that nothing broke horribly before updating the stats.  I think all of these can be wrapped up into what pass/fail means and what is printed. Depending on the test we could say that stats don\u0026rsquo;t matter but simout must be the same, etc. \u0026ndash;Saidi 18:09,    22 August 2011 (PDT)\n - It would be nice to also track macroop/regular instruction and microop counts separately. In SE mode at least, if the number of ISA level instructions changes it\u0026rsquo;s more important than if the number of microops change. That way when you change microcode, you can tell if the same instructions are executing more or less and just their implementation has changed. \u0026ndash;Gblack 21:35, 22 August 2011 (PDT)  For directed tests like the ones that test specific instructions, it might be worthwhile to add pseudo ops that signal success or completion through some sort of backdoor. There could be similar instructions which pause the simulation and give the simulation script a chance to look at state (I don\u0026rsquo;t think we can now, but it would be handy) and decide if, say, the add put the right thing in the right registers, or memory was updated correctly. One major drawback to this sort of approach is that we couldn\u0026rsquo;t easily run these on real machines. On the positive side, we could avoid problems where complementary bugs make a test pass when it shouldn\u0026rsquo;t. For instance, maybe adds and subtracts have the same bug, so both the output and checking the output against reference values might be wrong.\u0026ndash;Gblack 21:35, 22 August 2011 (PDT)   Implementation ideas Just ideas\u0026hellip; no definitive decisions have been made yet.\n Use Python\u0026rsquo;s unittest module, or something that extends it such as nose Use SCons to manage dependencies between binaries/test inputs and test results, but in a different SCons invocation (i.e., in its own SConstruct/SConscript)  Tests wish list  Checkpointing creates valid checkpoint Restore of checkpoint (maybe run something like the auto-checkpoint tester) CPU switching SMARTS sampling tests in between spec and hello world. Maybe the hpc games benchmarks http://www.google.com/codesearch#iw4AHL8Ip0w/~kohl/HPC.GAMES/hpcg1.0.tar.gz would be a good choice and could be distributed next to gem5. Getting rid of tests that run for more that 1 hour Using a memory tester to create interesting coherence actions, specifically around prefetching, \u0026ldquo;bonus\u0026rdquo; block allocation, etc.  "
},
{
	"uri": "http://localhost/deprecated/old-documentation/",
	"title": "Old documentation",
	"tags": [],
	"description": "",
	"content": " This page is an archive of old M5 documentation predating the GEMS merger and subsequent gem5 renaming. See the Documentation page for the current documentation.\nComplementary information from our tutorials, including about three hours of video from our last tutorial, is available here. The tutorials provide basic information about using M5 and are time well spent for anyone new to the simulator.\nStarting Out  Source Tree - Brief tour of the source tree Compiling M5 - How to compile the simulator Running M5 - How to run the simulator Debugging - Debugging tips and strategies Reporting Problems - What to do before you report a problem Adding Functionality - How to extend M5 (and update to new versions with almost no work)  Starting out in Full System  Running M5 in Full-System Mode - How to run in full-system mode m5term - How to use the m5term m5ops - Pseudo instructions and a command line interface to them Using linux-dist to Create Disk Images and Kernels for M5  How to create a Linux disk image/kernel   Simulator Internals  Doxygen \u0026ndash; Doxygen documentation Memory System - Overview of M5\u0026rsquo;s new memory system Simulation Scripts Explained - Overview of simulation scripts Using the Statistics Package - Using the statistics package ISA description system - The new ISA description primer CPU Models - Overview of the various CPU models in M5 Sampling - Overview of how to use sampling. IO Base Classes - Information about the I/O device base classes SimObject Initialization - Information about the phases of SimObject creation and initialization, including unserializing Events \u0026ndash; Information about the Event and EventWrapper  Extending M5  Regression Tests - How to Run \u0026amp; Create your own regression tests in M5 Coding Style Documentation Guidelines Adding New Features to M5:  Defining CPU Models - Steps toward defining your own CPU model in M5  m5 stable, changeset 6230:240a5a39e56f m5 2.0, beta 4 m5 2.0, beta 3  Defining ISAs - Steps toward defining a new ISA for M5 (updated for M5 2.0 beta 3)   ISAs  X86  External Documentation This documentation, gathered from outside sources, could be useful as reference when working with\nM5.   Alpha SPARC MIPS  Frequently Asked Questions Frequently Asked Questions - Frequently Asked Questions and Answers NOTOC\n"
},
{
	"uri": "http://localhost/developer/packet-command-attribute/",
	"title": "Packet command attributes",
	"tags": [],
	"description": "",
	"content": "Each packet contains a command, which consists of an a set of attributes. The following table indicates per command type, the type of response required (or InvalidCmd if none), and a set of flags. The contents of this table is based on src/mem/packet.hh (containing all command types) and src/mem/packet.cc (defining attributes per command type).\n                        Command response IsRead IsWrite IsUpgrade IsInvalidate NeedsExclusive IsRequest IsResponse NeedsResponse IsSWPrefetch IsHWPrefetch IsLlsc HasData IsError IsPrint IsFlush   InvalidCmd InvalidCmd                  ReadReq ReadResp IsRead     IsRequest  NeedsResponse          ReadResp InvalidCmd IsRead      IsResponse     HasData      ReadRespWithInvalidate InvalidCmd IsRead   IsInvalidate   IsResponse     HasData      WriteReq WriteResp  IsWrite   NeedsExclusive IsRequest  NeedsResponse    HasData      WriteResp InvalidCmd  IsWrite   NeedsExclusive  IsResponse           Writeback InvalidCmd  IsWrite   NeedsExclusive IsRequest      HasData      SoftPFReq SoftPFResp IsRead     IsRequest  NeedsResponse IsSWPrefetch         HardPFReq HardPFResp IsRead     IsRequest  NeedsResponse  IsHWPrefetch        SoftPFResp InvalidCmd IsRead      IsResponse  IsSWPrefetch   HasData      HardPFResp InvalidCmd IsRead      IsResponse   IsHWPrefetch  HasData      UpgradeReq UpgradeResp   IsUpgrade IsInvalidate NeedsExclusive IsRequest  NeedsResponse          SCUpgradeReq UpgradeResp   IsUpgrade IsInvalidate NeedsExclusive IsRequest  NeedsResponse   IsLlsc       UpgradeResp InvalidCmd   IsUpgrade  NeedsExclusive  IsResponse           SCUpgradeFailReq UpgradeFailResp    IsInvalidate NeedsExclusive IsRequest  NeedsResponse   IsLlsc       UpgradeFailResp InvalidCmd     NeedsExclusive  IsResponse           ReadExReq ReadExResp IsRead   IsInvalidate NeedsExclusive IsRequest  NeedsResponse          ReadExResp InvalidCmd IsRead    NeedsExclusive  IsResponse     HasData      LoadLockedReq ReadResp IsRead     IsRequest  NeedsResponse   IsLlsc       StoreCondReq StoreCondResp  IsWrite   NeedsExclusive IsRequest  NeedsResponse   IsLlsc HasData      StoreCondFailReq StoreCondResp  IsWrite   NeedsExclusive IsRequest  NeedsResponse   IsLlsc HasData      StoreCondResp InvalidCmd  IsWrite   NeedsExclusive  IsResponse    IsLlsc       SwapReq SwapResp IsRead IsWrite   NeedsExclusive IsRequest  NeedsResponse    HasData      SwapResp InvalidCmd IsRead IsWrite   NeedsExclusive  IsResponse     HasData      IntReq MessageResp  IsWrite    IsRequest  NeedsResponse    HasData      IntResp InvalidCmd  IsWrite     IsResponse           NetworkNackError InvalidCmd       IsResponse      IsError     InvalidDestError InvalidCmd       IsResponse      IsError     BadAddressError InvalidCmd       IsResponse      IsError     FunctionalReadError InvalidCmd IsRead      IsResponse      IsError     FunctionalWriteError InvalidCmd  IsWrite     IsResponse      IsError     PrintReq InvalidCmd      IsRequest        IsPrint    Flush Request InvalidCmd     NeedsExclusive IsRequest         IsFlush   Invalidation Request InvalidCmd    IsInvalidate NeedsExclusive IsRequest             "
},
{
	"uri": "http://localhost/deprecated/parallelize/",
	"title": "Parallelize gem5",
	"tags": [],
	"description": "",
	"content": " Parallelizing the simulation engine to run on shared-memory multicore systems has long been a goal for several of us. Unfortunately, it rarely makes it to the top of anyone\u0026rsquo;s to-do list. This page tries to summarize where we\u0026rsquo;re at and which direction we\u0026rsquo;re headed so that others who would like to help can jump in and help (or provide feedback on our future direction).\nGoals  Our primary goal is to get decent speedup modeling moderate-size parallel configurations (say 16-64 cores) on small-scale shared-memory multi-core systems (say 4-8 cores). You can\u0026rsquo;t even buy single-core systems anymore, and most developers use multi-core systems to accelerate compilations; our focus is on getting some benefit out of those additional cores when you\u0026rsquo;d like to reduce the latency of a single simulation. In particular, we are not focusing on:  Getting linear speedup (see Wood \u0026amp; Hill \u0026ldquo;Cost-Effective Parallel Computing\u0026rdquo; on why that\u0026rsquo;s not necessarily a worthwhile goal) Speeding up simulations of systems with one or just a few cores Scaling speedup to very large numbers of host cores (more than a typical developer would have available) - We\u0026rsquo;re not opposed to these things, if someone wants to try; they\u0026rsquo;re just not things we\u0026rsquo;re focusing on. In particular, objections to our current plan based on \u0026ldquo;but that will never support X\u0026rdquo;, where X is on the list above, are not likely to be considered convincing.  Parallel simulation should be deterministic by default. This is a necessity for maintaining developer sanity. Modes that optionally forgo determinism to provide enhanced performance are OK, but they should not be the default mode of parallel execution. The parallelization scheme should be flexible enough to allow experimentation. There has been a mini-renaissance of parallel simulation work recently (e.g., SlackSim from USC and Graphite from MIT), and it would be good if gem5 could be a platform for further research on parallel simulation techniques.  Status  In fall 2008 (!) Nate revamped the Event objects so they are no longer explicitly associated with specific queues. Instead, the EventManager class (which is a base class of SimObject) controls which queue an event gets scheduled on. This allows each SimObject to schedule its events on a specific queue. At this point all SimObjects point to the single global event queue (mainEventQueue) though. See http://repo.gem5.org/gem5/rev/b194a80157e2. Just before Christmas 2010, Steve did an initial implementation of multiple event queues. Because this work is still very preliminary, it is not committed, but is available as a patch on reviewboard here. See the patch for further documentation. At this rate, there will be another major development in early 2013 :-).  Next Steps  Get feedback on multi-queue patch and enhance it. Figure out a synchronization strategy; my expectation is that our inter-object synchronization would revolve around Ruby MessageBuffers (or perhaps Ports?) so that we can have each CPU node and the interconnect running on separate threads. ???  Nate\u0026rsquo;s old plan The sections below are the original contents of this page, written by Nate in 2008 (before he made the EventManager change described above). Several of these steps have already been taken care of, and many of the future steps aren\u0026rsquo;t exactly aligned with the current plan, but I (Steve) thought I\u0026rsquo;d leave it here so we can look back occasionally and see if we\u0026rsquo;re missing anything.\nParallelizing M5 Parallelizing M5 has been a long term goal of mine (nate) for quite some time.\nHere\u0026rsquo;s my plan for going about making this happen:\n Get rid of the global mainEventQueue Add an EventQueue pointer to every SimObject and add schedule()/deschedule()/reschedule() functions to the Base SimObject to use that event queue pointer. Change all calls to event scheduling to use that EventQueue pointer. An example of this is something like this:  old:  new LinkDelayEvent(this, packet, curTick + linkDelay);  new:  Event *event = new LinkDelayEvent(this, packet); this-\u0026gt;schedule(event, curTick + linkDelay);   Remove the schedule/deschedule/reschedule functions on the Event object. Now, you must create an event and schedule it on an event queue.  See note below about outstanding issues.  Add an EventQueue pointer to the SimObjectParams class We\u0026rsquo;re going to keep the mainEventQueue, but it will be for certain global functions like managing barriers simulator exits and the like. Create EventQueues in python and pass the pointer to each SimObject via the Params struct for every object  In the first phase of implementing parallel M5, I do all of the steps up to this point, and just create one event queue (the mainEventQueue) and populate every sim object with that one event queue. This should essentially keep the status quo.  Tell SCons to link M5 with -lpthread Create a set of wrapper classes for the pthread stuff since it would be nice to eventually support other mechanisms. Add support for the python code to determine the number of CPU cores it has available (automatically using /proc maybe, but with the ability to override the number with a command line option). Create a barrier event that can be used to synchronize sets of event queues  Initially, the barrier event will cause ticks to be run in lock-step, guaranteeing that all cycles are doing in order  Create a point-to-point synchronization events for controlling the slack between event queues  The plan is to rely mainly on these events for maintaining slack in the system. One major idea with these events is that they will be squashed as frequently as possible to avoid synchronization. It may make more sense to build this directly into the event queue, but the all-to-all nature of the synchronization may make this less desirable.  Create one event queue per thread and one thread per CPU core. Bind logical groups of objects to different EventQueues. Create certain objects which can use multiple EventQueues.  This will be done on is the EtherLink object, allowing two separate systems to be simulated on two separate cores Next, I\u0026rsquo;ll do this on is the bus object so that each core can run on a different event queue I\u0026rsquo;ll probably also create some sort of etherlink like or etherbridge like object for connecting two arbitrary memory objects across event queues. This may be done instead of doing the bus directly.   Outstanding Issues I\u0026rsquo;d like to remove the queue pointer from the event object since there is only one use case where you\u0026rsquo;ve scheduled an event and you don\u0026rsquo;t know which queue it\u0026rsquo;s on if you want to de/reschedule it. It\u0026rsquo;s for repeat events like the SimLoopExitEvent.\nHere are the options:\n Leave the queue pointer in the object Pass the queue pointer as a parameter to the process() function Record the queue pointer in just those objects that require it Create a new flag to the event called AutoRepeat, create a virtual function that can be called to determine the repeat interval, and add support for repeat in the event queue Create a thread local global variable called currentEventQueue. (I hate this idea)  I go back and forth as to the right thing to do. I\u0026rsquo;d really like to avoid the queue pointer in all objects so we can keep events small, but I guess it can easily be argued that I shouldn\u0026rsquo;t keep that optimization unless I know that it will pay off, but the only way to know if it will pay off is to just do it. I also basically hate the last idea and it\u0026rsquo;s on the bottom of my list. One issue is that because of the committed instruction queue, there can be more than one event queue in a given thread.\nNOTOC\n"
},
{
	"uri": "http://localhost/cpu-models/inorder/pipeline-stages/",
	"title": "Pipeline stages",
	"tags": [],
	"description": "",
	"content": " Overview Pipeline stages in the InOrder CPU are implemented as abstract implementations of what a pipeline stage would be in any CPU model. Typically, one would imagine a particularly pipeline stage being responsible for:\n(1) Performing specific operations such as \u0026ldquo;Decode\u0026rdquo; or \u0026ldquo;Execute\u0026rdquo; and either\n(2a) Sending that instruction to the next stage if that operation was successful and the next stage\u0026rsquo;s buffer has room for incoming instructions\nor\n(2b) Keeping that instruction in the pipeline\u0026rsquo;s instruction buffer if that operation was unsuccesful or there is no room in the next stage\u0026rsquo;s buffer\nThe \u0026ldquo;PipelineStage\u0026rdquo; class maintains the functionality of (2a) and (2b) but abstracts (1) out of the implementation. More specifically, no pipeline stage is explicitly marked \u0026ldquo;Decode\u0026rdquo; or \u0026ldquo;Execute\u0026rdquo;. Instead, the PipelineStage class allows the instruction and it\u0026rsquo;s corresponding instruction schedule to define what tasks they want to do in a particular stage.\nImplementing the \u0026ldquo;PipelineStage\u0026rdquo; class in this manner allows for the modeling of arbitrarily long pipelines (easy replication of pipeline stages) as well as other potentially complex pipeline models.\nRelevant source files:\n first_stage.[hh,cc] pipeline_stage.[hh,cc] pipeline_traits.[hh,cc] cpu.[hh,cc]  Interstage Communication Pipeline stages need to communicate with each other in a few scenarios:\n Sending an instruction to the next stage Sending a stall signal back to previous stages Sending a squash (e.g. branch misprediction) signal back to previous stages  Forwards Communication This is the simplest case in which a PipelineStage has finished doing all the processing it needs for an instruction and will place the instruction in a queue (implemented through wires) for the next stage to read from.\n The information in the \u0026ldquo;wire\u0026rdquo; is taken from the \u0026ldquo;InterStageComm\u0026rdquo; struct found in \u0026ldquo;comm.hh\u0026rdquo;.  Backwards Communication To handle backwards communication (stall/squash), the InOrderCPU uses the concepts of a \u0026ldquo;wire\u0026rdquo; to pass information back to previous stages. When a CPU is setup, each \u0026ldquo;wires\u0026rdquo; are defined with parameterized latencies that govern how many ticks a stage must wait to retrieve the information from that \u0026ldquo;wire\u0026rdquo;. As an example, let\u0026rsquo;s say Stage 3 wants to send a squash signal to Stage 1. If the the user wants Stage 1 to see information from Stage 3 the next clock tick, they would set that wire\u0026rsquo;s latency to 1. In this case, if Stage 3 wanted to squash instructions on Cycle X, Stage 1 would operate undeterred on Cycle X, but notice the squash signal on Cycle X+1.\nPipeline Stage Processing Each stage follows these generic steps each tick:\n Read stall and/or squash signals from stages further in the pipeline  If stall or squash is requested, then perform operation and do not process instructions that tick If no stall or squash, continue processing.  Select a thread to process instructions from Process all resource requests for an instruction and send instruction to next stage  If all resource requests are completed for an instruction, proceed to next instruction as long as the pipeline bandwidth is not used up. If all resource requests are not completed for an instruction, save instruction in stage \u0026ldquo;skidBuffer\u0026rdquo; and stall remaining instructions in pipeline stage from that thread   First Stage The FirstStage derives the \u0026ldquo;PipelineStage\u0026rdquo; class and implements specific operations that are needed for any first stage of the pipeline. Because there is no stage in front of the FirstStage there are no instructions to read from so the appropriate adjustments are made for this particular stage to create a new instruction each pipeline tick instead of read it from the previous stage.\nImplementations In it\u0026rsquo;s most basic incarnation, the In-Order model models a 5-stage pipeline with the following stages:\n Instruction Fetch (IF) Instruction Decode (ID) Execute (EX) Memory Access (MEM) Register Write Back (WB)  An example of a 9-stage pipeline is also available in the code directory. To Test it out, you\u0026rsquo;ll need to copy the overwrite the \u0026ldquo;pipeline_traits.hh,cc\u0026rdquo; files with the 9-stage versions.\nPipeline Customization Adding Your Own Stages  Python Configuration: TBD   Instruction Schedule Information: Customizing the pipeline involves changing in what order an instruction will ask for it\u0026rsquo;s resources. For example, the developer has the option to allow a load instruction to calculate it\u0026rsquo;s address in stage 3 or stage 4. The developer would need to alter the instruction\u0026rsquo;s schedule to enforce a particular pipeline configuration. Check out pipeline_traits.cc for more details on this process.  "
},
{
	"uri": "http://localhost/publications/",
	"title": "Publications",
	"tags": [],
	"description": "",
	"content": " If you use gem5 in your research, we would appreciate a citation to the original paper in any publications you produce. Moreover, we would appreciate if you cite also the speacial features of gem5 which have been developed and contributed to the main line since the publication of the original paper in 2011. In other words, if you use feature X please also cite the according paper Y from the list below.\nTOC\nOriginal Paper  The gem5 Simulator. Nathan Binkert, Bradford Beckmann, Gabriel Black, Steven K. Reinhardt, Ali Saidi, Arkaprava Basu, Joel Hestness, Derek R. Hower, Tushar Krishna, Somayeh Sardashti, Rathijit Sen, Korey Sewell, Muhammad Shoaib, Nilay Vaish, Mark D. Hill, and David A. Wood. May 2011, ACM SIGARCH Computer Architecture News.  Special Features of gem5 GPUs  Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level. Anthony Gutierrez, Bradford M. Beckmann, Alexandru Dutu, Joseph Gross, John Kalamatianos, Onur Kayiran, Michael LeBeane, Matthew Poremba, Brandon Potter, Sooraj Puthoor, Matthew D. Sinclair, Mark Wyse, Jieming Yin, Xianwei Zhang, Akshay Jain, Timothy G. Rogers. In Proceedings of the 24th IEEE International Symposium on High-Performance Computer Architecture (HPCA), February 2018.   NoMali: Simulating a realistic graphics driver stack using a stub GPU. René de Jong, Andreas Sandberg. In Proceedings of the International Symposium on Performance Analysis of Systems and Software (ISPASS), March 2016.   gem5-gpu: A Heterogeneous CPU-GPU Simulator. Jason Power, Joel Hestness, Marc S. Orr, Mark D. Hill, David A. Wood. Computer Architecture Letters vol. 13, no. 1, Jan 2014  DRAM Controller, DRAM Power Estimation  Simulating DRAM controllers for future system architecture exploration. Andreas Hansson, Neha Agarwal, Aasheesh Kolli, Thomas Wenisch and Aniruddha N. Udipi. In Proceedings of the International Symposium on Performance Analysis of Systems and Software (ISPASS), March 2014.   DRAMPower: Open-source DRAM Power \u0026amp; Energy Estimation Tool. Karthik Chandrasekar, Christian Weis, Yonghui Li, Sven Goossens, Matthias Jung, Omar Naji, Benny Akesson, Norbert Wehn, and Kees Goossens, URL: http://www.drampower.info.  KVM  Full Speed Ahead: Detailed Architectural Simulation at Near-Native Speed. Andreas Sandberg, Nikos Nikoleris, Trevor E. Carlson, Erik Hagersten, Stefanos Kaxiras, David Black-Schaffer. 2015 IEEE International Symposium on Workload Characterization  Elastic Traces  Exploring system performance using elastic traces: Fast, accurate and portable. Radhika Jagtap, Matthias Jung, Stephan Diestelhorst, Andreas Hansson, Norbert Wehn. IEEE International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS), 2016  SystemC Couping  System Simulation with gem5 and SystemC: The Keystone for Full Interoperability. C. Menard, M. Jung, J. Castrillon, N. Wehn. IEEE International Conference on Embedded Computer Systems Architectures Modeling and Simulation (SAMOS), July, 2017  Other Publications related to gem5  Enabling Realistic Logical Device Interface and Driver for NVM Express Enabled Full System Simulations. Donghyun Gouk, Jie Zhang and Myoungsoo Jung. IFIP International Conference on Network and Parallel Computing (NPC) and Invited for International Journal of Parallel Programming (IJPP), 2017   SimpleSSD: Modeling Solid State Drives for Holistic System Simulation. Myoungsoo Jung, Jie Zhang, Ahmed Abulila, Miryeong Kwon, Narges Shahidi, John Shalf, Nam Sung Kim and Mahmut Kandemir. IEEE Computer Architecture Letters (CAL), 2017   \u0026ldquo;dist-gem5: Distributed Simulation of Computer Clusters,\u0026rdquo; Mohammad Alian, Gabor Dozsa, Umur Darbaz, Stephan Diestelhorst, Daehoon Kim, and Nam Sung Kim. IEEE International Symposium on Performance Analysis of Systems (ISPASS), April 2017   pd-gem5: Simulation Infrastructure for Parallel/Distributed Computer Systems. Mohammad Alian, Daehoon Kim, and Nam Sung Kim. Computer Architecture Letters (CAL), 2016.   A Full-System Approach to Analyze the Impact of Next-Generation Mobile Flash Storage. Rene de Jong and Andreas Hansson. In Proceedings of the International Symposium on Performance Analysis of Systems and Software (ISPASS), March 2015.   Sources of Error in Full-System Simulation. A. Gutierrez, J. Pusdesris, R.G. Dreslinski, T. Mudge, C. Sudanthi, C.D. Emmons, M. Hayenga, and N. Paver. In Proceedings of the International Symposium on Performance Analysis of Systems and Software (ISPASS), March 2014.   Introducing DVFS-Management in a Full-System Simulator. Vasileios Spiliopoulos, Akash Bagdia, Andreas Hansson, Peter Aldworth and Stefanos Kaxiras. In Proceedings of the 21st International Symposium on Modeling, Analysis \u0026amp; Simulation of Computer and Telecommunication Systems (MASCOTS), August 2013.   Accuracy Evaluation of GEM5 Simulator System. A. Butko, R. Garibotti, L. Ost, and G. Sassatelli. In the proceeding of the IEEE International Workshop on Reconfigurable Communication-centric Systems-on-Chip (ReCoSoC), York, United Kingdom, July 2012. The M5 Simulator: Modeling Networked Systems. N. L. Binkert, R. G. Dreslinski, L. R. Hsu, K. T. Lim, A. G. Saidi, S. K. Reinhardt. IEEE Micro, vol. 26, no. 4, pp. 52-60, July/August, 2006. Multifacet\u0026rsquo;s General Execution-driven Multiprocessor Simulator (GEMS) Toolset. Milo M.K. Martin, Daniel J. Sorin, Bradford M. Beckmann, Michael R. Marty, Min Xu, Alaa R. Alameldeen, Kevin E. Moore, Mark D. Hill, and David A. Wood. Computer Architecture News (CAN), September 2005.  Publications using gem5 / m5 2017  [https://chess.eecs.berkeley.edu/pubs/1194/KimEtAl_CyPhy17.pdfAn Integrated Simulation Tool for Computer Architecture and Cyber-Physical Systems]. Hokeun Kim, Armin Wasicek, and Edward A. Lee. In Proceedings of the 6th Workshop on Design, Modeling and Evaluation of Cyber-Physical Systems (CyPhy\u0026rsquo;17), Seoul, Korea, October 19, 2017.   [http://www.lirmm.fr/~sassate/ADAC/wp-content/uploads/2017/06/opensuco17.pdfEfficient Programming for Multicore Processor Heterogeneity: OpenMP versus OmpSs]. Anastasiia Butko, Florent Bruguier, Abdoulaye Gamatié and Gilles Sassatelli. In Open Source Supercomputing (OpenSuCo’17) Workshop co-located with ISC\u0026rsquo;17, June 2017.   [https://hal-lirmm.ccsd.cnrs.fr/lirmm-01467328MAGPIE: System-level Evaluation of Manycore Systems with Emerging Memory Technologies]. Thibaud Delobelle, Pierre-Yves Péneau, Abdoulaye Gamatié, Florent Bruguier, Sophiane Senni, Gilles Sassatelli and Lionel Torres, 2nd International Workshop on Emerging Memory Solutions (EMS) co-located with DATE\u0026rsquo;17, March 2017.  2016  [http://ieeexplore.ieee.org/document/7776838An Agile Post-Silicon Validation Methodology for the Address Translation Mechanisms of Modern Microprocessors]. G. Papadimitriou, A. Chatzidimitriou, D. Gizopoulos, R. Morad, IEEE Transactions on Device and Materials Reliability (TDMR 2016), Volume: PP, Issue: 99, December 2016.   [http://ieeexplore.ieee.org/document/7753339Unveiling Difficult Bugs in Address Translation Caching Arrays for Effective Post-Silicon Validation]. G. Papadimitriou, D. Gizopoulos, A. Chatzidimitriou, T. Kolan, A. Koyfman, R. Morad, V. Sokhin, IEEE International Conference on Computer Design (ICCD 2016), Phoenix, AZ, USA, October 2016.   [http://ieeexplore.ieee.org/document/7833682/Loop optimization in presence of STT-MRAM caches: A study of performance-energy tradeoffs]. Pierre-Yves Péneau, Rabab Bouziane, Abdoulaye Gamatié, Erven Rohou, Florent Bruguier, Gilles Sassatelli, Lionel Torres and Sophiane Senni, 26th International Workshop on Power and Timing Modeling, Optimization and Simulation (PATMOS), September 21-23 2016.   [http://ieeexplore.ieee.org/abstract/document/7774439Full-System Simulation of big.LITTLE Multicore Architecture for Performance and Energy Exploration]. Anastasiia Butko, Florent Bruguier, Abdoulaye Gamatié, Gilles Sassatelli, David Novo, Lionel Torres and Michel Robert. Embedded Multicore/Many-core Systems-on-Chip (MCSoC), 2016 IEEE 10th International Symposium on, September 21-23, 2016.   [http://ieeexplore.ieee.org/document/7448986Exploring MRAM Technologies for Energy Efficient Systems-On-Chip]. Sophiane Senni, Lionel Torres, Gilles Sassatelli, Abdoulaye Gamatié and Bruno Mussard, IEEE Journal on Emerging and Selected Topics in Circuits and Systems , Volume: 6, Issue: 3, Sept. 2016.   [https://cpc2016.infor.uva.es/wp-content/uploads/2016/06/CPC2016_paper_11.pdfArchitectural exploration of heterogeneous memory systems]. Marcos Horro, Gabriel Rodríguez, Juan Touriño and Mahmut T. Kandemir. 19th Workshop on Compilers for Parallel Computing (CPC), July 2016.   [http://ieeexplore.ieee.org/document/7604675ISA-Independent Post-Silicon Validation for the Address Translation Mechanisms of Modern Microprocessors]. G. Papadimitriou, A. Chatzidimitriou, D. Gizopoulos and R. Morad, IEEE International Symposium on On-Line Testing and Robust System Design (IOLTS 2016), Sant Feliu de Guixols, Spain, July 2016.   Anatomy of microarchitecture-level reliability assessment: Throughput and accuracy. A.Chatzidimitriou, D.Gizopoulos, IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), Uppsala, Sweden, April 2016.   Agave: A benchmark suite for exploring the complexities of the Android software stack. Martin Brown, Zachary Yannes, Michael Lustig, Mazdak Sanati, Sally A. McKee, Gary S. Tyson, Steven K. Reinhardt, IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), Uppsala, Sweden, April 2016.  2015  [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7314163Differential Fault Injection on Microarchitectural Simulators]. M.Kaliorakis, S.Tselonis, A.Chatzidimitriou, N.Foutris, D.Gizopoulos, IEEE International Symposium on Workload Characterization (IISWC), Atlanta, GA, USA, October 2015.   Live Introspection of Target-Agnostic JIT in Simulation. B. Shingarov. International Workshop IWST\u0026rsquo;15 in cooperation with ACM, Brescia, Italy, 2015.   Security in MPSoCs: A NoC Firewall and an Evaluation Framework. M.D. Grammatikakis, K. Papadimitriou, P. Petrakis, A. Papagrigoriou, G. Kornaros, I. Christoforakis, O. Tomoutzoglou, G. Tsamis and M. Coppola. In IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), vol.34, no.8, pp.1344-1357, Aug. 2015   DPCS: Dynamic Power/Capacity Scaling for SRAM Caches in the Nanoscale Era. Mark Gottscho, Abbas BanaiyanMofrad, Nikil Dutt, Alex Nicolau, and Puneet Gupta. ACM Transactions on Architecture and Code Optimization (TACO), Vol. 12, No. 3, Article 27. Pre-print June 2015, published August 2015, print October 2015.   A predictable and command-level priority-based DRAM controller for mixed-criticality systems. Hokeun Kim, David Broman, Edward A. Lee, Michael Zimmer, Aviral Shrivastava, Junkwang Oh. Proceedings of the 21st IEEE Real-Time and Embedded Technology and Application Symposium (RTAS), Seattle, WA, USA, April, 2015.   Security Enhancements for Building Saturation-free, Low-Power NoC-based MPSoCs. Kyprianos Papadimitriou, Polydoros Petrakis, Miltos Grammatikakis, Marcello Coppola. In IEEE Conference on Communications and Network Security (CNS) - 1st IEEE Workshop on Security and Privacy in Cybermatics, Florence, Italy, 2015   Design Exploration For Next Generation High-Performance Manycore On-chip Systems: Application To big.LITTLE Architectures. Anastasiia Butko, Abdoulaye Gamatie, Gilles Sassatelli, Lionel Torres and Michel Robert. VLSI (ISVLSI), 2015 IEEE Computer Society Annual Symposium on, July 10, 2015   [http://dx.doi.org/10.1007/s11227-014-1375-7 Gem5v: a modified gem5 for simulating virtualized systems]. Seyed Hossein Nikounia, Siamak Mohammadi. Springer Journal of Supercomputing. The source code is available [https://github.com/nikoonia/gem5v here].   Micro-architectural simulation of embedded core heterogeneity with gem5 and McPAT. Fernando A. Endo, Damien Couroussé, Henri-Pierre Charles. RAPIDO \u0026lsquo;15 Proceedings of the 2015 Workshop on Rapid Simulation and Performance Evaluation: Methods and Tools. January 2015.   A trace-driven approach for fast and accurate simulation of manycore architectures. Anastasiia Butko, Rafael Garibotti, Luciano Ost, Vianney Lapotre, Abdoulaye Gamatie, Gilles Sassatelli and Chris Adeniyi-Jones. Design Automation Conference (ASP-DAC), 2015 20th Asia and South Pacific. January 19, 2015  2014  Evaluating Private vs. Shared Last-Level Caches for Energy Efficiency in Asymmetric Multi-Cores. A. Gutierrez, R.G. Dreslinski, and Trevor Mudge. In Proceedings of the 14th International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS), 2014.   [http://dx.doi.org/10.1109/HPCC.2014.173 Security Effectiveness and a Hardware Firewall for MPSoCs]. M. D. Grammatikakis, K. Papadimitriou, P. Petrakis, A. Papagrigoriou, G. Kornaros, I. Christoforakis and M. Coppola. In 16th IEEE International Conference on High Performance Computing and Communications - Workshop on Multicore and Multithreaded Architectures and Algorithms, 2014, pp. 1032-1039 Aug. 2014   [http://dx.doi.org/10.1145/2541940.2541951 Integrated 3D-Stacked Server Designs for Increasing Physical Density of Key-Value Stores]. Anthony Gutierrez, Michael Cieslak, Bharan Giridhar, Ronald G. Dreslinski, Luis Ceze, and Trevor Mudge. ASPLOS XIX   [http://dx.doi.org/10.1145/2593069.2593184 Power / Capacity Scaling: Energy Savings With Simple Fault-Tolerant Caches]. Mark Gottscho, Abbas BanaiyanMofrad, Nikil Dutt, Alex Nicolau, and Puneet Gupta. DAC, 2014.   \u0026rdquo;\u0026lsquo;Write-Aware Replacement Policies for PCM-Based Systems \u0026ldquo;\u0026rsquo;. R. Rodríguez-Rodríguez, F. Castro, D. Chaver*, R. Gonzalez-Alberquilla, L. Piñuel and F. Tirado. The Computer Journal, 2014.   \u0026rdquo;\u0026lsquo;Micro-architectural simulation of in-order and out-of-order ARM microprocessors with gem5 \u0026ldquo;\u0026rsquo;. Fernando A. Endo, Damien Couroussé, Henri-Pierre Charles. 2014 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS XIV). July 2014.  2013  Continuous Real-World Inputs Can Open Up Alternative Accelerator Designs. Bilel Belhadj, Antoine Joubert, Zheng Li, Rodolphe Héliot, and Olivier Temam. ISCA \u0026lsquo;13 Cache Coherence for GPU Architectures. Inderpreet Singh, Arrvindh Shriraman, Wilson WL Fung, Mike O’Connor, and Tor M. Aamodt. HPCA, 2013. Navigating Heterogeneous Processors with Market Mechanisms. Marisabel Guevara, Benjamin Lubin, and Benjamin C. Lee. HPCA, 2013 Power Struggles: Revisiting the RISC vs. CISC Debate on Contemporary ARM and x86 Architectures. Emily Blem, Jaikrishnan Menon, and Karthikeyan Sankaralingam. HPCA 2013. Coset coding to extend the lifetime of memory. Adam N. Jacobvitz, Robert Calderbank, Daniel J. Sorin. HPCA \u0026lsquo;13. The McPAT Framework for Multicore and Manycore Architectures: Simultaneously Modeling Power, Area, and Timing. Sheng Li, Jung Ho Ahn, Richard D. Strong, Jay B. Brockman, Dean M. Tullsen, Norman P. Jouppi. ACM Transactions on Architecture and Code Optimization (TACO), Volume 10, Issue 1, April 2013 Optimization and Mathematical Modeling in Computer Architecture Nowatzki, T., Ferris, M., Sankaralingam, K., Estan, C., Vaish, N., \u0026amp; Wood, David A. (2013). Synthesis Lectures on Computer Architecture, 8(4), 1-144. Limits of Parallelism and Boosting in Dim Silicon. Nathaniel Pinckney, Ronald G. Dreslinski, Korey Sewell, David Fick, Trevor Mudge, Dennis Sylvester, David Blaauw, IEEE Micro, vol. 33, no. 5, pp. 30-37, Sept.-Oct., 2013  2012  Hardware Prefetchers for Emerging Parallel Applications, Biswabandan Panda, Shankar Balachandran. In the proceedings of the IEEE/ACM Parallel Architectures and Compilation Techniques,PACT, Minneapolis, October 2012. Lazy Cache Invalidation for Self-Modifying Codes. A. Gutierrez, J. Pusdesris, R.G. Dreslinski, and T. Mudge. In the proceedings of the International Conference on Compilers, Architecture and Synthesis for Embedded Systems (CASES), Tampere, Finland, October 2012. Accuracy Evaluation of GEM5 Simulator System. A. Butko, R. Garibotti, L. Ost, and G. Sassatelli. In the proceeding of the IEEE International Workshop on Reconfigurable Communication-centric Systems-on-Chip (ReCoSoC), York, United Kingdom, July 2012. Viper: Virtual Pipelines for Enhanced Reliability. A. Pellegrini, J. L. Greathouse, and V. Bertacco. In the proceedings of the International Symposium on Computer Architecture (ISCA), Portland, OR, June 2012. Reducing memory reference energy with opportunistic virtual caching. Arkaprava Basu, Mark D. Hill, Michael M. Swift. In the proceedings of the 39th International Symposium on Computer Architecture (ISCA 2012). Cache Revive: Architecting Volatile STT-RAM Caches for Enhanced Performance in CMPs. Adwait Jog, Asit Mishra, Cong Xu, Yuan Xie, V. Narayanan, Ravi Iyer, Chita Das. In the proceedings oF the IEEE/ACM Design Automation Conference (DAC), San Francisco, CA, June 2012.  2011  Full-System Analysis and Characterization of Interactive Smartphone Applications. A. Gutierrez, R.G. Dreslinski, T.F. Wenisch, T. Mudge, A. Saidi, C. Emmons, and N. Paver. In the proceeding of the IEEE International Symposium on Workload Characterization (IISWC), pages 81-90, Austin, TX, November 2011. Universal Rules Guided Design Parameter Selection for Soft Error Resilient Processors, L. Duan, Y. Zhang, B. Li, and L. Peng. Proceedings of the International Symposium on Performance Analysis of Systems and Software(ISPASS), Austin, TX, April 2011.  2010  Using Hardware Vulnerability Factors to Enhance AVF Analysis, V. Sridharan, D. R. Kaeli. Proceedings of the International Symposium on Computer Architecture (ISCA-37), Saint-Malo, France, June 2010. Leveraging Unused Cache Block Words to Reduce Power in CMP Interconnect, H. Kim, P. Gratz. IEEE Computer Architecture Letters, vol. 99, (RapidPosts), 2010. A Fast Timing-Accurate MPSoC HW/SW Co-Simulation Platform based on a Novel Synchronization Scheme, Mingyan Yu, Junjie Song, Fangfa Fu, Siyue Sun, and Bo Liu. Proceedings of the International MultiConfernce of Engineers and Computer Scientists. 2010 pdf Simulation of Standard Benchmarks in Hardware Implementations of L2 Cache Models in Verilog HDL, Rosario M. Reas, Anastacia B. Alvarez, Joy Alinda P. Reyes, Computer Modeling and Simulation, International Conference on, pp. 153-158, 2010 12th International Conference on Computer Modelling and Simulation, 2010 A Simulation of Cache Sub-banking and Block Buffering as Power Reduction Techniques for Multiprocessor Cache Design, Jestoni V. Zarsuela, Anastacia Alvarez, Joy Alinda Reyes, Computer Modeling and Simulation, International Conference on, pp. 515-520, 2010 12th International Conference on Computer Modelling and Simulation, 2010  2009  Efﬁcient Implementation of Decoupling Capacitors in 3D Processor-DRAM Integrated Computing Systems. Q. Wu, J. Lu, K. Rose, and T. Zhang. Great Lakes Symposium on VLSI. 2009. Evaluating the Impact of Job Scheduling and Power Management on Processor Lifetime for Chip Multiprocessors. A. K. Coskun, R. Strong, D. M. Tullsen, and T. S. Rosing. Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems. 2009. \u0026rdquo; Devices and architectures for photonic chip-scale integration.\u0026rdquo; J. Ahn, M. Fiorentino1, R. G. Beausoleil, N. Binkert, A. Davis, D. Fattal, N. P. Jouppi, M. McLaren, C. M. Santori, R. S. Schreiber, S. M. Spillane, D. Vantrease and Q. Xu. Journal of Applied Physics A: Materials Science \u0026amp; Processing. February 2009. System-Level Power, Thermal and Reliability Optimization. C. Zhu. Thesis at Queen’s University. 2009.   A light-weight fairness mechanism for chip multiprocessor memory systems. M. Jahre, L. Natvig. Proceedings of the 6th ACM conference on Computing Frontiers. 2009. Decoupled DIMM: building high-bandwidth memory system using low-speed DRAM devices. H. Zheng, J. Lin, Z. Zhang, and Z. Zhu. International Symposium on Computer Architecture (ISCA). 2009. On the Performance of Commit-Time-Locking Based Software Transactional Memory. Z. He and B. Hong. The 11th IEEE International Conference on. High Performance Computing and Communications (HPCC-09). 2009.   A Quantitative Study of Memory System Interference in Chip Multiprocessor Architectures. M. Jahre, M. Grannaes and L. Natvig. The 11th IEEE International Conference on. High Performance Computing and Communications (HPCC-09). 2009. Hardware Support for Debugging Message Passing Applications for Many-Core Architectures. C. Svensson. Masters Thesis at the University of Illinois at Urbana-Champaign, 2009. Initial Experiments in Visualizing Fine-Grained Execution of Parallel Software Through Cycle-Level Simulation. R. Strong, J. Mudigonda, J. C. Mogul, N. Binkert. USENIX Workshop on Hot Topics in Parallelism (HotPar). 2009. MPreplay: Architecture Support for Deterministic Replay of Message Passing Programs on Message Passing Many-core Processors. C. Erik-Svensson, D. Kesler, R. Kumar, and G. Pokam. University of Illinois Technical Report number UILU-09-2209. Low-power Inter-core Communication through Cache Partitioning in Embedded Multiprocessors. C. Yu, X. Zhou, and P. Petrov .Symposium on Integrated Circuits and System Design (sbcci). 2009. Integrating NAND flash devices onto servers. D. Roberts, T. Kgil, T. Mudge. Communications of the ACM (CACM). 2009. A High-Performance Low-Power Nanophotonic On-Chip Network. Z. Li, J. Wu, L. Shang, A. Mickelson, M. Vachharajani, D. Filipovic, W. Park∗ and Y. Sun. International Symposium on Low Power Electronic Design (ISLPED). 2009. Core monitors: monitoring performance in multicore processors. P. West, Y. Peress, G. S. Tyson, and S. A. McKee. Computing Frontiers. 2009. Parallel Assertion Processing using Memory Snapshots. M. F. Iqbal, J. H. Siddiqui, and D. Chiou. Workshop on Unique Chips and Systems (UCAS). April 2009. Leveraging Memory Level Parallelism Using Dynamic Warp Subdivision. J. Meng, D. Tarjan, and K. Skadron. Univ. of Virginia Dept. of Comp. Sci. Tech Report (CS-2009-02). Reconfigurable Multicore Server Processors for Low Power Operation. R. G. Dreslinski, D. Fick, D. Blaauw, D. Sylvester and T. Mudge. 9th International Symposium on Systems, Architectures, MOdeling and Simulation (SAMOS). July 2009. Near Threshold Computing: Overcoming Performance Degradation from Aggressive Voltage Scaling R. G. Dreslinski, M. Wieckowski, D. Blaauw, D. Sylvester, and T. Mudge. Workshop on Energy Efficient Design (WEED), June 2009. Workload Adaptive Shared Memory Multicore Processors with Reconfigurable Interconnects. S. Akram, R. Kumar, and D. Chen. IEEE Symposium on Application Specific Processors, July 2009.   Eliminating Microarchitectural Dependency from Architectural Vulnerability. V. Sridharan, D. R. Kaeli. Proceedings of the 15th International Symposium on High-Performance Computer Architecture (HPCA-15), February 2009. Producing Wrong Data Without Doing Anything Obviously Wrong! T. Mytkowicz, A. Diwan, M. Hauswirth, P. F. Sweeney. Proceedings of the 14th international conference on Architectural support for programming languages and operating systems (ASPLOS). 2009. End-To-End Performance Forecasting: Finding Bottlenecks Before They Happen A. Saidi, N. Binkert, S. Reinhardt, T. Mudge. Proceedings of the 36th International Symposium on Computer Architecture (ISCA-36), June 2009. Fast Switching of Threads Between Cores. R. Strong, J. Mudigonda, J. C. Mogul, N. Binkert, D. Tullsen. ACM SIGOPS Operating Systems Review. 2009. Express Cube Topologies for On-Chip Interconnects. B. Grot, J. Hestness, S. W. Keckler, O. Mutlu. Proceedings of the 15th International Symposium on High-Performance Computer Architecture (HPCA-15), February 2009. Enhancing LTP-Driven Cache Management Using Reuse Distance Information. W. Lieu, D. Yeung. Journal of Instruction-Level Parallelism 11 (2009).  2008  Analyzing the Impact of Data Prefetching on Chip MultiProcessors. N. Fukumoto, T. Mihara, K. Inoue, and K. Murakami. Asia-Pacific Computer Systems Architecture Conference. 2008. Historical Study of the Development of Branch Predictors. Y. Peress. Masters Thesis at Florida State University. 2008.   Hierarchical Domain Partitioning For Hierarchical Architectures. J. Meng, S. Che, J. W. Sheaffer, J. Li, J. Huang, and K. Skadron. Univ. of Virginia Dept. of Comp. Sci. Tech Report CS-2008-08. 2008. Memory Access Scheduling Schemes for Systems with Multi-Core Processors. H. Zheng, J. Lin, Z. Zhang, and Z. Zhu. International Conference on Parallel Processing, 2008.   Register Multimapping: Reducing Register Bank Conflicts Through One-To-Many Logical-To-Physical Register Mapping. N. L. Duong and R. Kumar. Tehnical Report CHRC-08-07. Cross-Layer Custimization Platform for Low-Power and Real-Time Embedded Applications. X. Zhou. Dissertation at the University of Maryland. 2008. Probabilistic Replacement: Enabling Flexible Use of Shared Caches for CMPs. W. Liu and D. Yeung. University of Maryland Technical Report UMIACS-TR-2008-13. 2008. Observer Effect and Measurement Bias in Performance Analysis. T. Mytkowicz, P. F. Sweeney, M. Hauswirth, and A. Diwan. University of Colorado at Boulder Technical Report CU-CS 1042-08. June, 2008. Power-Aware Dynamic Cache Partitioning for CMPs. I. Kotera, K. Abe, R. Egawa, H. Takizawa, and H. Kobayashi. 3rd International Conference on High Performance and Embedded Architectures and Compilers (HiPEAC). 2008. Modeling of Cache Access Behavior Based on Zipf’s Law. I. Kotera, H. Takizawa, R. Egawa, H. Kobayashi. MEDEA 2008. Hierarchical Verification for Increasing Performance in Reliable Processors. J. Yoo, M. Franklin. Journal of Electronic Testing. 2008.   Transaction-Aware Network-on-Chip Resource Reservation. Z. Li, C. Zhu, L. Shang, R. Dick, Y. Sun. Computer Architecture Letters. Volume PP, Issue 99, Page(s):1 - 1.   Predictable Out-of-order Execution Using Virtual Traces. J. Whitham, N. Audsley. Proceedings of the 29th IEEE Real-time Systems Symposium, December 2008. pdf   Architectural and Compiler Mechanisms for Acelerating Single Thread Applications on Multicore Processors. H. Zhong. Dissertation at The University of Michigan. 2008.   Mini-Rank: Adaptive DRAM Architecture for Improving Memory Power Efficiency. H. Zheng, J. Lin, Z. Zhang, E. Gorbatov, H. David, Z. Zhu. Proceedings of the 41st Annual Symposium on Microarchitecture (MICRO-41), November 2008.   Reconfigurable Energy Efficient Near Threshold Cache Architectures. R. Dreslinski, G. Chen, T. Mudge, D. Blaauw, D. Sylvester, K. Flautner. Proceedings of the 41st Annual Symposium on Microarchitecture (MICRO-41), November 2008.   Distributed and low-power synchronization architecture for embedded multiprocessors. C. Yu, P. Petrov. Internation Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS), October 2008.   Thermal Monitoring Mechanisms for Chip Multiprocessors. J. Long, S.O. Memik, G. Memik, R. Mukherjee. ACM Transactions on Architecture and Code Optimization (TACO), August 2008.   Multi-optimization power management for chip multiprocessors. K. Meng, R. Joseph, R. Dick, L. Shang. Proceedings of the 17th international conference on Parallel Architectures and Compilation Techniques (PACT), 2008.   \u0026rdquo; Three-Dimensional Chip-Multiprocessor Run-Time Thermal Management.\u0026rdquo; C. Zhu, Z. Gu, L. Shang, R.P. Dick, R. Joseph. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), August 2008.   \u0026rdquo; Latency and bandwidth efficient communication through system customization for embedded multiprocessors\u0026rdquo;. C. Yu and P. Petrov. DAC 2008, June 2008.   Corona: System Implications of Emerging Nanophotonic Technology. D. Vantrease, R. Schreiber, M. Monchiero, M. McLaren, N., P. Jouppi, M. Fiorentino, A. Davis, N. Binkert, R. G. Beausoleil, and J. Ahn. Proceedings of the 35th International Symposium on Computer Architecture (ISCA-35), June 2008.   Improving NAND Flash Based Disk Caches. T. Kgil, D. Roberts and T. N. Mudge. Proceedings of the 35th International Symposium on Computer Architecture (ISCA-35), June 2008.   A Taxonomy to Enable Error Recovery and Correction in Software. V. Sridharan, D. A. Liberty, and D. R. Kaeli. Workshop on Quality-Aware Design (W-QUAD), in conjunction with the 35th International Symposium on Computer Architecture (ISCA-35), June 2008.   Quantifying Software Vulnerability. V. Sridharan and D. R. Kaeli. First Workshop on Radiation Effects and Fault Tolerance in Nanometer Technologies, in conjunction with the ACM International Conference on Computing Frontiers, May 2008.   Core Monitors: Monitoring Performance in Multicore Processors. P. West. Masters Thesis at Florida State University. April 2008.   Full System Critical Path Analysis. A. Saidi, N. Binkert, T. N. Mudge, and S. K. Reinhardt. 2008 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), April 2008.   A Power and Temperature Aware DRAM Architecture. S. Liu, S. O. Memik, Y. Zhang, G. Memik. 45th annual conference on Design automation (DAC), 2008.   Streamware: Programming General-Purpose Multicore Processors Using Streams. J. Gummaraju, J. Coburn, Y. Turner, M. Rosenblum. Procedings of the Thirteenth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), March 2008.   Application-aware snoop filtering for low-power cache coherence in embedded multiprocessors. X. Zhou, C. Yu, A. Dash, and P. Petrov. Transactions on Design Automation of Electronic Systems (TODAES). January 2008.   An approach for adaptive DRAM temperature and power management. Song Liu, S. O. Memik, Y. Zhang, and G. Memik. Proceedings of the 22nd annual international conference on Supercomputing. 2008.  2007  Modeling and Characterizing Power Variability in Multicore Architectures. K. Meng, F. Huebbers, R, Joseph, and Y. Ismail. ISPASS-2007. A High Performance Adaptive Miss Handling Architecture for Chip Multiprocessors. M. Jahre, and L. Natvig. HiPEAC Journal 2007. Performance Effects of a Cache Miss Handling Architecture in a Multi-core Processor. M. Jahre and L. Natvig. NIK-2007 conference. 2007. Prioritizing Verification via Value-based Correctness Criticality. J. Yoo, M. Franklin. Proceedings of the 25th International Conference on Computer Design (ICCD), 2007.   DRAM-Level Prefetching for Fully-Buffered DIMM: Design, Performance and Power Saving. J. Lin, H. Zheng, Z. Zhu, Z. Zhang ,H. David. ISPASS 2007.   \u0026rdquo; Virtual Exclusion: An architectural approach to reducing leakage energy in caches for multiprocessor systems\u0026rdquo;. M. Ghosh, H. Lee. Proceedings of the International Conference on Parallel and Distributed Systems. December 2007.   Dependability-Performance Trade-off on Multiple Clustered Core Processors. T. Funaki, T. Sato. Proceedings of the 4th International Workshop on Dependable Embedded Systems. October 2007.   Predictive Thread-to-Core Assignment on a Heterogeneous Multi-core Processor. T. Sondag, V. Krishnamurthy, H. Rajan. PLOS \u0026lsquo;07: ACM SIGOPS 4th Workshop on Programming Languages and Operating Systems. October 2007.   Power deregulation: eliminating off-chip voltage regulation circuitry from embedded systems. S. Kim, R. P. Dick, R. Joseph. 5th IEEE/ACM International Conference on Hardware/Software Co-Design and System Synthesis (CODES+ISSS). October 2007.   Aggressive Snoop Reduction for Synchronized Producer-Consumer Communication in Energy-Efficient Embedded Multi-Processors. C. Yu, P. Petrov. 5th IEEE/ACM International Conference on Hardware/Software Co-Design and System Synthesis (CODES+ISSS). October 2007.   Three-Dimensional Multiprocessor System-on-Chip Thermal Optimization. C. Sun, L. Shang, R.P. Dick. 5th IEEE/ACM International Conference on Hardware/Software Co-Design and System Synthesis (CODES+ISSS). October 2007.   Sampled Simulation for Multithreaded Processors. M. Van Biesbrouck. (Thesis) UC San Diego Technical Report CS2007-XXXX. September 2007.   Representative Multiprogram Workloads for Multithreaded Processor Simulation. M. Van Biesbroucky, L. Eeckhoutz, B. Calder. IEEE International Symposium on Workload Characterization (IISWC). September 2007.   The Interval Page Table: Virtual Memory Support in Real-Time and Memory-Constrained Embedded Systems. X. Zhou, P. Petrov. Proceedings of the 20th annual conference on Integrated circuits and systems design. 2007.   A power-aware shared cache mechanism based on locality assessment of memory reference for CMPs. I. Kotera, R. Egawa, H. Takizawa, H. Kobayashi. Proceedings of the 2007 workshop on MEmory performance: DEaling with Applications, systems and architecture (MEDEA). September 2007.   Architectural Support for the Stream Execution Model on General-Purpose Processors. J. Gummaraju, M. Erez, J. Coburn, M. Rosenblum, W. J. Dally. The Sixteenth International Conference on Parallel Architectures and Compilation Techniques (PACT). September 2007.   An Energy Efficient Parallel Architecture Using Near Threshold Operation. R. Dreslinski, B. Zhai, T. Mudge, D. Blaauw, D. Sylvester. The Sixteenth International Conference on Parallel Architectures and Compilation Techniques (PACT). September 2007.   When Homogeneous becomes Heterogeneous: Wearout Aware Task Scheduling for Streaming Applications. D. Roberts, R. Dreslinski, E. Karl, T. Mudge, D. Sylvester, D. Blaauw. Workshop on Operationg System Support for Heterogeneous Multicore Architectures (OSHMA). September 2007.   \u0026rdquo; On-Chip Cache Device Scaling Limits and Effective Fault Repair Techniques in Future Nanoscale Technology\u0026rdquo;. D. Roberts, N. Kim,T. Mudge. Digital System Design Architectures, Methods and Tools (DSD). August 2007.   Energy Efficient Near-threshold Chip Multi-processing. B. Zhai, R. Dreslinski, D. Blaauw, T. Mudge, D. Sylvester. International Symposium on Low Power Electronics and Design (ISLPED). August 2007.   \u0026rdquo; A Burst Scheduling Access Reordering Mechanism\u0026rdquo;. J. Shao, B.T. Davis. IEEE 13th International Symposium on High Performance Computer Architecture (HPCA). 2007.   Enhancing LTP-Driven Cache Management Using Reuse Distance Information. W. Liu, D. Yeung. University of Maryland Technical Report UMIACS-TR-2007-33. June 2007.   Thermal modeling and management of DRAM memory systems. J. Lin, H. Zheng, Z. Zhu, H. David, and Z. Zhang. Proceedings of the 34th Annual international Symposium on Computer Architecture (ISCA). June 2007.   Duplicating and Verifying LogTM with OS Support in the M5 Simulator. G. Blake, T. Mudge. Sixth Annual Workshop on Duplicating, Deconstructing, and Debunking (WDDD). June 2007.   Analysis of Hardware Prefetching Across Virtual Page Boundaries. R. Dreslinski, A. Saidi, T. Mudge, S. Reinhardt. Proc. of the 4th Conference on Computing Frontiers. May 2007.   Reliability in the Shadow of Long-Stall Instructions. V. Sridharan, D. Kaeli, A. Biswas. Third Workshop on Silicon Errors in Logic - System Effects (SELSE-3). April 2007.   Extending Multicore Architectures to Exploit Hybrid Parallelism in Single-thread Applications. H. Zhong, S. A. Lieberman, S. A. Mahlke. Proc. 13th Intl. Symposium on High Performance Computer Architecture (HPCA). February 2007.  2006  Evaluation of the Data Vortex Photonic All-Optical Path Interconnection Network for Next-Generation Supercomputers. W. C. Hawkins. Dissertation at Georgia Tech. December 2006.   Running the manual: an approach to high-assurance microkernel development. P. Derrin, K. Elphinstone, G. Klein, D. Cock, M. M. T. Chakravarty. Proceedings of the 2006 ACM SIGPLAN workshop on Haskell. 2006.   The Filter Checker: An Active Verification Management Approach. J. Yoo, M. Franklin. 21st IEEE International Symposium on Defect and Fault-Tolerance in VLSI Systems (DFT\u0026rsquo;06), 2006.   Physical Resource Matching Under Power Asymmetry. K. Meng, F. Huebbers, R. Joseph, Y. Ismail. Presented at the 2006 P=ac2 Conference. 2006. pdf   Process Variation Aware Cache Leakage Management. K. Meng, R. Joseph. Proceedings of the 2006 International Symposium on Low Power Electronics and Design (ISLPED). October 2006.   FlashCache: a NAND flash memory file cache for low power web servers. T. Kgil, T. Mudge. Proceedings of the 2006 international conference on Compilers, Architecture and Synthesis for Embedded Systems (CASES). October 2006.   PicoServer: Using 3D Stacking Technology To Enable A Compact Energy Efficient Chip Multiprocessor. T. Kgil, S. D\u0026rsquo;Souza, A. Saidi, N. Binkert, R. Dreslinski, S. Reinhardt, K. Flautner, T. Mudge. 12th Int\u0026rsquo;l Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS). October 2006.   Integrated Network Interfaces for High-Bandwidth TCP/IP. N. L. Binkert, A. G. Saidi, S. K. Reinhardt. 12th Int\u0026rsquo;l Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS). October 2006.   Communist, utilitarian, and capitalist cache policies on CMPs: caches as a shared resource. L. R. Hsu, S. K. Reinhardt, R. Iyer, S. Makineni. Proc. 15th Int\u0026rsquo;l Conf. on Parallel Architectures and Compilation Techniques (PACT), September 2006.   Impact of CMP Design on High-Performance Embedded Computing. P. Crowley, M. A. Franklin, J. Buhler, and R. D. Chamberlain. Proc. of 10th High Performance Embedded Computing Workshop. September 2006.   BASS: A Benchmark suite for evaluating Architectural Security Systems. J. Poe, T. Li. ACM SIGARCH Computer Architecture News. Vol. 34, No. 4, September 2006.   The M5 Simulator: Modeling Networked Systems. N. L. Binkert, R. G. Dreslinski, L. R. Hsu, K. T. Lim, A. G. Saidi, S. K. Reinhardt. IEEE Micro, vol. 26, no. 4, pp. 52-60, July/August, 2006.Link   Considering All Starting Points for Simultaneous Multithreading Simulation. M. Van Biesbrouck, L. Eeckhout, B. Calder. Proc. of the Int\u0026rsquo;l Symp. on Performance Analysis of Systems and Software (ISPASS). 2006.pdf   Dynamic Thread Assignment on Heterogeneous Multiprocessor Architectures. M. Becchi, P. Crowley. Proc. of the 3rd Conference on Computing Frontiers. pp29-40. May 2006. pdf   Integrated System Architectures for High-Performance Internet Servers. N. L. Binkert. Dissertation at the University of Michigan. February 2006.   Exploring Salvage Techniques for Multi-core Architectures. R. Joseph. 2nd Workshop on High Performance Computing Reliability Issues. February 2006. pdf   A Simple Integrated Network Interface for High-Bandwidth Servers. N. L. Binkert, A. G. Saidi, S. K. Reinhardt. University of Michigan Technical Report CSE-TR-514-06, January 2006. pdf  2005  Software Defined Radio - A High Performance Embedded Challenge. H. lee, Y. Lin, Y. Harel, M. Woh, S. Mahlke, T. Mudge, K. Flautner. Proc. 2005 Int\u0026rsquo;l Conf. on High Performance Embedded Architectures and Compilers (HiPEAC). November 2005. pdf   How to Fake 1000 Registers. D. W. Oehmke, N. L. Binkert, S. K. Reinhardt, and T. Mudge. Proc. 38th Ann. Int\u0026rsquo;l Symp. on Microarchitecture (MICRO), November 2005. pdf   Virtualizing Register Context. D. W. Oehmke. Dissertation at the University of Michigan, 2005. pdf   Performance Validation of Network-Intensive Workloads on a Full-System Simulator. A. G. Saidi, N. L. Binkert, L. R. Hsu, and S. K. Reinhardt. First Ann. Workshop on Iteraction between Operating System and Computer Architecture (IOSCA), October 2005. pdf  An extended version appears as University of Michigan Technical Report CSE-TR-511-05, July 2005. pdf    Performance Analysis of System Overheads in TCP/IP Workloads. N. L. Binkert, L. R. Hsu, A. G. Saidi, R. G. Dreslinski, A. L. Schultz, and S. K. Reinhardt. Proc. 14th Int\u0026rsquo;l Conf. on Parallel Architectures and Compilation Techniques (PACT), September 2005. pdf   Sampling and Stability in TCP/IP Workloads. L. R. Hsu, A. G. Saidi, N. L. Binkert, and S. K. Reinhardt. Proc. First Annual Workshop on Modeling, Benchmarking, and Simulation (MoBS), June  pdf    A Unified Compressed Memory Hierarchy. E. G. Hallnor and S. K. Reinhardt. Proc. 11th Int\u0026rsquo;l Symp. on High-Performance Computer Architecture (HPCA), February 2005. pdf   Analyzing NIC Overheads in Network-Intensive Workloads. N. L. Binkert, L. R. Hsu, A. G. Saidi, R. G. Dreslinski, A. L. Schultz, and S. K. Reinhardt. Eighth Workshop on Computer Architecture Evaluation using Commercial Workloads (CAECW), February 2005. pdf  An extended version appears as University of Michigan Technical Report CSE-TR-505-04, December 2004. pdf   2004  Emulation of realisitic network traffic patterns on an eight-node data vortex interconnection network subsytem. B. Small, A. Shacham, K. Bergman, K. Athikulwongse, C. Hawkins, and D.S. Will. Journal of Optical Networking Vol. 3, No.11, pp 802-809, November 2004. pdf   ChipLock: Support for Secure Microarchitectures. T. Kgil, L Falk, and T. Mudge. Proc. Workshop on Architectural Support for Security and Anti-virus (WASSA), October 2004, pp. 130-139. pdf   Design and Applications of a Virtual Context Architecture. D. Oehmke, N. Binkert, S. Reinhardt, and T. Mudge. University of Michigan Technical Report CSE-TR-497-04, September 2004. pdf   The Performance Potential of an Integrated Network Interface. N. L. Binkert, R. G. Dreslinski, E. G. Hallnor, L. R. Hsu, S. E. Raasch, A. L. Schultz, and S. K. Reinhardt. Proc. Advanced Networking and Communications Hardware Workshop (ANCHOR), June 2004. pdf   A Co-Phase Matrix to Guide Simultaneous Multithreading Simulation. M. Van Biesbrouck, T. Sherwood, and B. Calder. IEEE International Symposium on Performance Analysis and Software (ISPASS), March 2004. pdf   A Compressed Memory Hierarchy using an Indirect Index Cache. E. G. Hallnor and S. K. Reinhardt. Proc. 3rd Workshop on Memory Performance Issues (WMPI), June 2004. pdf  An extended version appears as University of Michigan Technical Report CSE-TR-488-04, March 2004. pdf   2003  The Impact of Resource Partitioning on SMT Processors. S. E. Raasch and S. K. Reinhardt. Proc. 12th Int\u0026rsquo;l Conf. on Parallel Architectures and Compilation Techniques (PACT), pp. 15-25, Sept.  pdf    Network-Oriented Full-System Simulation using M5. N. L. Binkert, E. G. Hallnor, and S. K. Reinhardt. Sixth Workshop on Computer Architecture Evaluation using Commercial Workloads (CAECW), February  pdf    Design, Implementation and Use of the MIRV Experimental Compiler for Computer Architecture Research. D. A. Greene. Dissertation at the Universtiy of Michigan, 2003. [http://www.eecs.umich.edu/~tnm/theses/daveg.pdg\u0026ldquo;\u0026gt;pdf ]  2002  A Scalable Instruction Queue Design Using Dependence Chains. S. E. Raasch, N. L. Binkert, and S. K. Reinhardt. Proc. 29th Annual Int\u0026rsquo;l Symp. on Computer Architecture (ISCA), pp. 318-329, May 2002. pdf ps ps.gz  \nNOTOC\n"
},
{
	"uri": "http://localhost/developer/python-params/",
	"title": "Python Parameter Types",
	"tags": [],
	"description": "",
	"content": "The information below came from src/python/m5/params.py and src/python/m5/util/convert.py. Reference those files for the most up to date information.\n  Python Type Name\n C++ Type\n Format\n Notes\n    String\n std::string\n    Int\n int\n  32 bits\n  Unsigned\n unsigned\n  32 bits\n  Int8\n int8_t\n    UInt8\n uint8_t\n    Int16\n int16_t\n    UInt16\n uint16_t\n    Int32\n int32_t\n    UInt32\n uint32_t\n    Int64\n int64_t\n    UInt64\n uint64_t\n    Counter\n Counter\n    Tick\n Tick\n    TcpPort\n uint16_t\n    UdpPort\n uint16_t\n    Percent\n int\n  Between 0 and 100.\n  Float\n double\n    MemorySize\n uint64_t\n A string formatted as [value][unit] where value is a base 10 number and unit is one of the following:\n* PB =\u0026gt; pebibytes\n* TB =\u0026gt; tebibytes\n* GB =\u0026gt; gibibytes\n* MB =\u0026gt; mebibytes\n* kB =\u0026gt; kibibytes\n* B =\u0026gt; bytes\n kibi, mebi, etc. are true powers of 2.\n  MemorySize32\n uint32_t\n See \u0026quot;MemorySize\u0026quot; above.\n See \u0026quot;MemorySize\u0026quot; above.\n  Addr\n Addr\n See \u0026quot;MemorySize\u0026quot; above\n See \u0026quot;MemorySize\u0026quot; above. Also, an addr may be specified as a string with units, or a raw integer value.\n  Range\n Range\u0026lt;[type]\u0026gt;, type is int by default\n  Defined as a \u0026quot;start\u0026quot; and \u0026quot;end\u0026quot; or \u0026quot;size\u0026quot; value. Exactly one of \u0026quot;end\u0026quot; or \u0026quot;size\u0026quot; is recognized as a keyword argument. A positional argument will be treated as \u0026quot;end\u0026quot;, and \u0026quot;size\u0026quot; arguments are convected to \u0026quot;end\u0026quot; internally by adding to \u0026quot;start\u0026quot; and subtracting one.\n  AddrRange\n Range\n  See \u0026quot;Range\u0026quot; above.\n  TickRange\n Range\n  See \u0026quot;Range\u0026quot; above.\n  Bool\n bool\n    EthernetAddr\n Net::EthAddr\n Six pairs of 2 digit hex values seperated by \u0026quot;:\u0026quot;s, for instance \u0026quot;01:23:45:67:89:AB\u0026quot;\n May be set to NextEthernetAddr. All EthernetAddrs set to NextEthernetAddr will be assigned to incremental ethernet addresses starting with 00:90:00:00:00:01.\n  IpAddress\n Net::IpAddress\n Four decimal values between 0 and 255 separated by \u0026quot;.\u0026quot;s, for instance \u0026quot;1.23.45.67\u0026quot;, or an integer where the leftmost component is the most significant byte.\n   IpNetmask\n Net::IpNetmask\n A string representation of an IpAddress followed by either \u0026quot;/n\u0026quot; where n is a decimal value from 0 to 32 or \u0026quot;/e.f.g.h\u0026quot; where e-h are integer values between 0 and 255 and where when represented as binary from left to right the number is all 1s and the all 0s. The ip and netmask can also be passed in as positional or keyword arguments where the ip is an integer as described in IpAddress, and the netmask is a decimal value from 0 to 32.\n   IpWithPort\n Net::IpWithPort\n A string representation of an IpAddress followed by \u0026quot;:p\u0026quot; where p is a decimal value from 0 to 65535. The ip and port can also be passed in as positional or keyword arguments where the ip is an integer as described in IpAddress, and the port is a decimal value from 0 to 65535.\n   Time\n tm\n May be a Python struct_time, int, long, datetime, or date, the string \u0026quot;Now\u0026quot; or \u0026quot;Today\u0026quot;, or a string parseable by Python's strptime with one of the following formats:\n* \u0026quot;%a %b %d %H:%M:%S %Z %Y\u0026quot;\n* \u0026quot;%a %b %d %H:%M:%S %Z %Y\u0026quot;\n* \u0026quot;%Y/%m/%d %H:%M:%S\u0026quot;\n* \u0026quot;%Y/%m/%d %H:%M\u0026quot;\n* \u0026quot;%Y/%m/%d\u0026quot;\n* \u0026quot;%m/%d/%Y %H:%M:%S\u0026quot;\n* \u0026quot;%m/%d/%Y %H:%M\u0026quot;\n* \u0026quot;%m/%d/%Y\u0026quot;\n* \u0026quot;%m/%d/%y %H:%M:%S\u0026quot;\n* \u0026quot;%m/%d/%y %H:%M\u0026quot;\n* \u0026quot;%m/%d/%y\u0026quot;\n   subclasses of Enum\n enum named after the parameter type\n A string defined as part of the enum\n This description applies to all enum parameter types which are defined as subclasses of Enum. The possible string values and optionally their mappings are specified in a dict called \u0026quot;map\u0026quot; or a list called \u0026quot;vals\u0026quot; defined as members of the Enum subclass itself.\n  Latency\n Tick\n Can be assigned an existing Clock or Frequency parameter, or a string with the format [value][unit] where value is a base 10 number and unit is one of the following:\n* t =\u0026gt; Ticks\n* ps =\u0026gt; picoseconds\n* ns =\u0026gt; nanoseconds\n* us =\u0026gt; microseconds\n* ms =\u0026gt; milliseconds\n* s =\u0026gt; seconds\n   Frequency\n Tick\n Can be assigned an existing Latency or Clock parameter, or a string with the format [value][unit] where value is a base 10 number and unit is one of the following:\n* THz =\u0026gt; terahertz\n* GHz =\u0026gt; gigahertz\n* MHz =\u0026gt; megahertz\n* kHz =\u0026gt; kilohertz\n* Hz =\u0026gt; hertz\n The frequency value is converted into a period in units of Ticks when transfered to C++.\n  Clock\n Tick\n Can be assigned an existing Latency or Frequency parameter, or a string with the format [value][unit] where value is a base 10 number and unit is one of the following:\n* t =\u0026gt; Ticks\n* ps =\u0026gt; picoseconds\n* ns =\u0026gt; nanoseconds\n* us =\u0026gt; microseconds\n* ms =\u0026gt; milliseconds\n* s =\u0026gt; seconds\n* THz =\u0026gt; terahertz\n* GHz =\u0026gt; gigahertz\n* MHz =\u0026gt; megahertz\n* kHz =\u0026gt; kilohertz\n* Hz =\u0026gt; hertz\n This type is like a combination of the Frequency and Latency types described above.\n  NetworkBandwidth\n float\n A floating point value specifying bits per second, or a string formatted as [value][unit] where value is a base 10 number and unit is one of the following:\n* Tbps =\u0026gt; terabits per second\n* Gbps =\u0026gt; gigabits per second\n* Mbps =\u0026gt; megabits per second\n* kbps =\u0026gt; kilobits per second\n* bps =\u0026gt; bits per second\n The network bandwidth value is converted to Ticks per byte before being transfered to C++.\n  MemoryBandwidth\n float\n A string formatted as [value][unit] where value is a base 10 number and unit is one of the following:\n* PB/s =\u0026gt; pebibytes per second\n* TB/s =\u0026gt; tebibytes per second\n* GB/s =\u0026gt; gibibytes per second\n* MB/s =\u0026gt; mebibytes per second\n* kB/s =\u0026gt; kibibytes per second\n* B/s =\u0026gt; bytes per second\n The memory bandwidth value is converted to Ticks per byte before being transferred to C++. kibi, mebi, etc. are true powers of 2.\n  subclass of SimObject\n defined in subclass\n  These parameter types are for assigning one simobject to another as a parameter. The may be set to nothing using the special \u0026quot;NULL\u0026quot; python object.\n    "
},
{
	"uri": "http://localhost/developer/ref-counting/",
	"title": "Refcounted Pointers",
	"tags": [],
	"description": "",
	"content": "I\u0026rsquo;m putting this in a wiki because I keep asking Nate and I keep forgetting his answer.\nQuestion: Which STL structures might have problems with calling ref counted pointer destructors? Or is it just certain operations like clear()?\nAnswer: It\u0026rsquo;s with operations like clear. There may be references on them, or when you remove an element from a datastructure like a deque or a queue. If they\u0026rsquo;re implemented in terms of vector, they don\u0026rsquo;t get freed until they\u0026rsquo;re reallocated. Another problem is if you\u0026rsquo;re using a special memory allocator that doesn\u0026rsquo;t destroy your object.\nThese problems affect classes such as the hash_map, which probably uses a vector as its data structure. Keep this in mind when calling erase() or clear() on such structures, as you might have to manually set the Ref counted pointer to NULL prior to erasing/clearing, or else they will be sitting around taking up resources for a long time.\n"
},
{
	"uri": "http://localhost/developer/register-indexing/",
	"title": "Register Indexing",
	"tags": [],
	"description": "",
	"content": " CPU register indexing in gem5 is a complicated by the need to support multiple ISAs with sometimes very different register semantics (register windows, condition codes, mode-based alternate register sets, etc.). In addition, this support has evolved gradually as new ISAs have been added, so older code may not take advantage of newer features or terminology.\nTypes of Register Indices There are three types of register indices used internally in the CPU models: relative, unified, and flattened.\nRelative A relative register index is the index that is encoded in a machine instruction. There is a separate index space for each class of register (integer, floating point, etc.), starting at 0. The register class is implied by the opcode. Thus a value of \u0026ldquo;1\u0026rdquo; in a source register field may mean integer register 1 (e.g., \u0026ldquo;%r1\u0026rdquo;) or floating point register 1 (e.g., \u0026ldquo;%f1\u0026rdquo;) depending on the type of the instruction.\nUnified While relative register indices are good for keeping instruction encodings compact, they are ambiguous, and thus not convenient for things like managing dependencies. To avoid this ambiguity, the decoder maps the relative register indices into a unified register space by adding class-specific offsets to relocate each relative index range into a unique position. Integer registers are unmodified, and continue to start at zero. Floating-point register indices are offset by (at least) the number of integer registers, so that the first FP register (e.g., \u0026ldquo;%f0\u0026rdquo;) gets a unified index that is greater than that of the last integer register. Similarly, miscellaneous (a.k.a. control) registers are mapped past the end of the FP register index space.\nFlattened Unified register indices provide an unambiguous description of all the registers that are accessible as instruction operands at a given point in the execution. Unfortunately, due to the complex features of some ISAs, they do not always unambiguously identify the actual state that the instruction is referencing. For example, in ISAs with register windows (notably SPARC), a particular register identifier such as \u0026ldquo;%o0\u0026rdquo; will refer to a different register after a \u0026ldquo;save\u0026rdquo; or \u0026ldquo;restore\u0026rdquo; operation than it did previously. Several ISAs have registers that are hidden in normal operation, but get mapped on top of ordinary registers when an interrupt occurs (e.g., ARM\u0026rsquo;s mode-specific registers), or under explicit supervisor control (e.g., SPARC\u0026rsquo;s \u0026ldquo;alternate globals\u0026rdquo;).\nWe solve this problem by maintaining a flattened register space which provides a distinct index for every unique register storage location. For example, the integer portion of the SPARC flattened register space has distinct indices for the globals and the alternate globals, as well as for each of the available register windows. The \u0026ldquo;flattening\u0026rdquo; process of translating from a unified or relative register index to a flattened register index varies by ISA. On some ISAs, the mapping is trivial, while others use table lookups to do the translation.\nA key distinction between the generation of unified and flattened register indices is that the former can always be done statically while the latter often depends on dynamic processor state. That is, the translation from relative to unified indices depends only on the context provided by the instruction itself (which is convenient as the translation is done in the decoder). In contrast, the mapping to a flattened register index may depend on processor state such as the interrupt level or the current window pointer on SPARC.\nCombining Register Index Types Although the typical progression for modifying register indices is relative -\u0026gt; unified -\u0026gt; flattened, it turns out that relative vs. unified and flattened vs. unflattened are orthogonal attributes. Relative vs. unified indicates whether the index is relative to the base register for its register class (integer, FP, or misc) or has the base offset for its class added in. Flattened vs. unflattened indicates whether the index has been adjusted to account for runtime context such as register window adjustments or alternate register file modes. Thus a relative flattened register index is one in which the runtime context has been accounted for, but is still expressed relative to the base offset for its class.\nA single set of class-specific offsets is used to generate unified indices from relative indices regardless of whether the indices are flattened or unflattened. Thus the offsets must be large enough to separate the register classes even when flattened addresses are being used. As a result, the unflattened unified register space is often discontiguous.\nIllustration As an illustration, consider a hypothetical architecture with four integer registers (%r0-%r4), three FP registers (%f0-%f2), and two misc/control registers (%msr0-%msr1). In addition, the architecture supports a complete set of alternate integer and FP registers for fast context switching.\nThe resulting register file layout, along with the unified flattened register file indices, is shown at right. Although the indices in the picture range from 0 to 15, the actual set of valid indices depends on the type of index and (for relative indices) the register class as well:\n         Relative unflattened Int: 0-3; FP: 0-2; Misc: 0-1   Unified unflattened 0-3, 8-10, 14-15   Relative flattened Int: 0-7; FP: 0-5; Misc: 0-1   Unified flattened 0-15    In this example, register %f1 in the alternate FP register file could be referred to via the relative flattened index 4 as well as the relative unflattened index 1, the unified unflattened index 9, or the unified flattened index 12. Note that the difference between the relative and unified indices is always 8 (regardless of flattening), and the difference between the unflattened and flattened indices is 3 (regardless of relative vs. unified status).\nCaveats  Although the gem5 code is unfortunately not always clear about which type of register index is expected by a particular function, functions whose name incorporates a register class (e.g., readIntReg()) expect a relative register index, and functions that expect a flattened index often have \u0026ldquo;flat\u0026rdquo; in the function name. Although the general case is complicated, the common case can be deceptively simple. For example, because integer registers start at the beginning of the unified register space, relative and unified register indices are identical for integer registers. Furthermore, in an architecture with no (or rarely-used) alternate integer registers, the unflattened and flattened indices are (almost always) the same as well, meaning that all four types of register indices are interchangeable in this case. While this situation seems to be a simplification, it also tends to hide bugs where the wrong register index type is used. The description above is intended to illustrate the typical usage of these index types. There may be exceptions that don\u0026rsquo;t precisely follow this description, but I got tired of writing \u0026ldquo;typically\u0026rdquo; in every sentence. The terms \u0026lsquo;relative\u0026rsquo; and \u0026lsquo;unified\u0026rsquo; were invented for use in this documentation, so you are unlikely see them in the code until the code starts catching up with this page. This discussion pertains only to the *architectural* registers. An out-of-order CPU model such as O3 adds another layer of complexity by renaming these architectural registers (using the flattened register indices) to an underlying physical register file.  Outstanding Questions  The SPARC code looks broken, in that the FP_Base_DepTag value is much smaller than NumIntRegs (see src/arch/sparc/registers.hh). Is it really broken, or is there some subtlety not captured here that allows it to work?  NOTOC\n"
},
{
	"uri": "http://localhost/developer/register-windows/",
	"title": "Register Windows",
	"tags": [],
	"description": "",
	"content": " Register windows prove to be a bit problematic with the current model of StaticInsts and the read/write register methods. The issue is that based on the register\u0026rsquo;s index, it is possible that it will be one of the registers whose actual index is determined by a combination of its index and the Current Window Pointer (CWP). In SPARC, r0-r7 are non-windowed registers, while r8-r31 are windowed registers.\nStaticInst Register windows provide a form of dynamic renaming that is troublesome with our current StaticInst. Because the CWP\u0026rsquo;s value has nothing to do with the current instruction, it is impossible to statically decode the registers in use based on the machine instruction alone. Thus renaming of registers must happen dynamically, either within the CPU\u0026rsquo;s readReg() or writeReg() function, or in the instruction\u0026rsquo;s execute() function. The latter is difficult because the current readReg() and writeReg() functions refer to a source or destination index, and not the architectural register index.\nSimpleCPU Windowing will be built into the SPARC IntRegFile object, which SimpleCPU will use via CPUExecContext. On writes to the CWP, a callback from the MiscRegFile object to the CPUExecContext (e.g., changeIntRegContext()) will be reflected by the CPUExecContext back down to the IntRegFile so that it can update its internal notion of the CWP.\nchangeIntRegContext() will be an ISA-independent function. Presumably it will take as an argument a pointer to some ISA-specific object (derived from an ISA-independent base object) that describes the change (sort of like the new Fault objects)\u0026hellip; unless anyone has a better idea.\n-\nI think a better parameter would be an index which specified what part of the context you were changing and a new value. Otherwise, all of the architectures would have to use a constraining (or trivial) base class for their integer register file context, and keep around the value of the parts they don't want to change. An example would be changing the cwp without changing between sets of global registers.  --Gblack 13:04, 6 April 2006 (EDT)\nDetailed CPU In the detailed CPU case, register window renaming will happen prior to the normal rename. The initial arch reg index will be renamed by adding the appropriate register window shift to them. Then normal renaming can happen given the unique, physical register index. This will allow all current dependence mechanisms to work normally. The first rename must happen at decode/rename. A CWP register at decode or rename will get speculatively updated on SAVE and RESTORE instructions. It will have to be restored upon squashing.\nIn this case, the MiscRegFile callback changeIntRegContext() will go to the proxy ExecContext, which can forward it up to the CPU object to update its internal CWP. The fact that this update has to be done at decode/rename (before execute) is unresolved\u0026hellip; likely we will have to introduce a new virtual StaticInst method to capture decode/rename-time execution effects.\nAdding this function raises several questions (that probably ought to have their own wiki page):\n In the common case where there\u0026rsquo;s nothing to do, is it faster to have a null virtual function or a bool that suppresses calling the function? Are there other behaviors currently hard-coded into the pipeline (or implemented via flags) that could be moved to this function? Should we do the same thing for commit (create a virtual function and move some existing code there)? I think probably yes\u0026hellip;  "
},
{
	"uri": "http://localhost/developer/regression/",
	"title": "Regression Tests",
	"tags": [],
	"description": "",
	"content": " Running Regressions Running the M5 regression tests is the recommended way to make sure that any changes to the simulator still comply with M5. Regression Testing is performed using SCons to help guide which tests are run. All current tests are located in the: tests/ directory. The command for running regression tests has the following\nformat: % scons build/\u0026lt;arch\u0026gt;/tests/\u0026lt;binary_type\u0026gt;/\u0026lt;test_directory\u0026gt;/\u0026lt;mode\u0026gt;/\u0026lt;test directory\u0026gt;/\u0026lt;ISA\u0026gt;/linux/\u0026lt;config script\u0026gt;  Here is an example of running all of the \u0026lsquo;quick\u0026rsquo; regression tests for the ALPHA architecture in syscall-emulation (SE) mode. You can leave out any of the trailing details in the directory string to execute all of the sub-tests.\n% scons build/ALPHA/tests/debug/quick/se ... **** build/ALPHA/tests/debug/quick/se/00.hello/alpha/linux/o3-timing PASSED! ... **** build/ALPHA/tests/debug/quick/se/00.hello/alpha/linux/simple-atomic PASSED! ... **** build/ALPHA/tests/debug/quick/se/00.hello/alpha/linux/simple-timing PASSED! ... **** build/ALPHA/tests/debug/quick/se/00.hello/alpha/tru64/o3-timing PASSED! ...  The script util/regress can be used for running regression tests. Run the script with the option \u0026lsquo;-h\u0026rsquo; or \u0026lsquo;\u0026ndash;help\u0026rsquo; for the options can be passed to the script. This scripts executes both compiling (scons build/\u0026lt;build_opts (ISA)\u0026gt;/gem5.\u0026lt;compile-variants (opt,debug,etc.)\u0026gt;) and running regressions (scons build//tests/\u0026lt;test-variants (opt,debug,etc.)\u0026gt;//\u0026lt;modes (se/fs)\u0026gt;\nThe options contained in the test directory passed to scons are detailed below.\n : The build to run. This can be any file in gem5/build_opts : The binary to run (e.g., opt, debug) : \u0026ldquo;quick\u0026rdquo; or \u0026ldquo;long\u0026rdquo; see tests/ : se or fs : The directory in tests//of the test to run. This usually specifies the workload to run (e.g., 00.hello) : Lowercase version of ISA to use : Script from tests/configs to run  Regression scripts execution description The regression scripts are complicated. Below, we describe the interaction and execution order of all of the scripts.\n util/regress executes scons with varying options depending on its parameters. scons is executed. For compile tests, this is the same way you compile gem5. For regression tests scons executes builds//gem5.tests/run.py  Control is now transferred to tests/run.py run.py execute the primary configuration script (the last directory on the scons command line) The way scons knows which tests to run is by looking in tests////ref//linux/ - The name of this directory should match the primary script (in tests/configs) that you want to execute.  tests/run.py executes the python file tests/configs/.py After the file in tests/configs executes to completion, run.py executes the secondary config file, or the file specific to this test - The file is found in tests////test.py This file is usually used to specify the binary to simulate tests/run.py sets up some other parameters, like initializing CPUs. tests/run.py executes the function run_test(root) - Note: At some point in the files that tests/run.py executes, you must specify a variable \u0026ldquo;root\u0026rdquo; You can overload run_test(root) by specifying it in one of the executed files. These files are executed in python as if their code were contained in run.py. Usually, in run_test(root), gem5 simulation is started. The output is written to build//tests//////linux/ The output is compared to the reference in tests////ref//linux/  Creating Your Own Regressions Adding a regression test is done by adding a new directory to the \u0026lsquo;m5/tests\u0026rsquo; directory and filling that directory up with the appropriate reference files. Every regression test needs these files in order to compare the current changes of M5 with the \u0026lsquo;known working state:\n simout - Standard Output Stream simerr - Standard Error Stream config.ini - M5 Configuration File stats.txt - Statistics From M5 Simulation  One needs to run the M5 with the appropriate binary in order to get the aforementioned simulation files. For example, a command line to collect simulation data for a hello world regression test might\nbe: % build/ALPHA/gem5.fast -re configs/example/se.py --cmd=tests/test-progs/hello/bin/alpha/hello  Creating the \u0026lsquo;hello world\u0026rsquo; regression test directory and copying over the reference output might then follow this command line sequence:\n% mkdir -p tests/quick/00.hello/ref/alpha/linux/simple-timing % cp config.ini stats.txt simout simerr tests/quick/00.hello/ref/alpha/linux/simple-timing  You then need to create the test configuration file (e.g., simple-timing.py in this case) in tests/configs, if it does not already exist. This file executes during tests/run.py. In this file you must create a variable named \u0026ldquo;root\u0026rdquo;, and you can optionally create function name run_test(root) which takes a single parameter (the root of the system). This function overloads a simple version in tests/run.py\nNext, you need to add your primary script name to the SConscript file in tests (tests/SConscript). In this file there is a variable named \u0026ldquo;configs\u0026rdquo;. You need to add the name of you primary script to that list. Without this scons will not recognize your new test.\nconfigs += ['simple-timing']  The last thing one needs to do is add a \u0026lsquo;test.py\u0026rsquo; configuration file that specifies the workload for the regression test. For this \u0026lsquo;hello world\u0026rsquo; example, the \u0026lsquo;test.py\u0026rsquo; file would just\ncontain: root.system.cpu.workload = LiveProcess(cmd='hello', executable = binpath('hello'))  This file gets executed during tests/run.py after the primary script is executed.\nOnce the test.py is copied to the \u0026lsquo;hello world\u0026rsquo; regression test directory, the individual regression test can be run:\n% cp test.py tests/quick/00.hello % scons build/ALPHA/tests/debug/quick/00.hello/alpha/linux/simple-timing  Or, you can execute all of the tests in 00.hello by running:\n% scons build/ALPHA/tests/debug/quick/00.hello/  Updating Regression Tests Once you\u0026rsquo;ve made and verified changes to the simulator, you can update any regression tests with the \u0026ldquo;update\u0026rdquo; option:\n% scons --update-ref build/ALPHA/tests/debug/quick/00.hello  "
},
{
	"uri": "http://localhost/cpu-models/inorder/resource-request-model/",
	"title": "Resource request model",
	"tags": [],
	"description": "",
	"content": " Overview Resources consists of any CPU object that an instruction wants to access. This could be a branch predictor, a cache, a execution unit, etc. In the InOrder CPU model we abstract what a resource is into a generic \u0026ldquo;Resource\u0026rdquo; class that all specific resources must derive from. In any given pipeline stage, an instruction will request that a resource perform a specific operation on it\u0026rsquo;s behalf. If an instruction can complete all it\u0026rsquo;s resource requests for a given stage, then it may pass to the next stage.\nRelevant source files:\n resource.[hh,cc] resources/*.[hh,cc] pipeline_traits.[hh,cc] cpu.[hh,cc]  Resource-Request Model Ideally, resources can used by instructions as a black box that will perform specific operation.The basic steps between an instruction and resource are as follows:\n An instruction accesses a resource by generating a resource-request that contains:   The ID of the resource to be accessed A command for the resource to perform   After generating a resource-request, an instruction sends the request to the Resource Pool which will forward the request to the appropriate resource.   Once a resource receives a request, it will:   Check to see if there is room (i.e. bandwidth) to process that resource request. If there is room, a slot is allocated to that instruction Attempt to process the command that the instruction requests of it.  If that command is successful, the resource mark the request as completed and return it back to the instruction. If that command is unsuccessful, the resource will mark the request as not completed before returning it back to the instruction.   Resource Internals Commands Each Resource contains a enumerated list of commands that it recognizes. The execute() function expects an instruction to request a resource to perform one of the commands on this list for processing.\nAn example from resources/branch_predictor.hh:\n enum Command { PredictBranch, UpdatePredictor };  Slots Slots represent the bandwidth of the resource (or how many concurrent instructions can access the resource). Each time an instruction makes a request to a resource, the resource allocates a slot to that instruction to represent the resources\u0026rsquo; bandwidth being used.\nExecute Once an instruction is allocated a slot, it is eligible to be executed within that resource. The execute function read the command from the resource-request and then tries to perform the appropriate operation. If the operation isnt completed successfully, execute() will mark the completed variable \u0026ldquo;false\u0026rdquo; and if the the operation is successful, the completed variable for that resource-request will marked as true.\nAn example from resources/branch_predictor.cc:\nvoid BranchPredictor::execute(int slot_num) { ... ResourceRequest* bpred_req = reqMap[slot_num]; ... switch (bpred_req-\u0026gt;cmd) { case PredictBranch: { ... bpred_req-\u0026gt;done(); } break; case UpdatePredictor: { ... bpred_req-\u0026gt;done(); } break; default: fatal(\u0026quot;Unrecognized command to %s\u0026quot;, resName); } }  Resource Pool The ResourcePool is the interface that the CPU must go through in order to access a resource. The \u0026ldquo;pool\u0026rdquo; instantiates all resources, allocates IDs to resources, and can schedule events to be processed on one or all of the resources available to the CPU.\nRelevant source files:\n resource_pool.[hh,cc] resource.[hh,cc] resources/*.[hh,cc] pipeline_traits.[hh,cc] cpu.[hh,cc]  Interaction w/CPU As stated before, the resource pool encapsulates all of the resources that a CPU accesses. Consequently, the CPU must first access the resource pool before receiving access to any specific resource. Conversely, a resource can talk to the CPU by grabbing a pointer to the CPU through the resource pool.\nTypically, the CPU will access the resource pool in just a few cases:\n An instruction request in a pipeline stage Broadcasting a CPU Event Non-cycle-accurate instances such as initialization, system calls, and in the future checkpointing.  Resource Pool Events The resource pool allows the CPU to broadcast events that all of the resources will see and process. This is beneficial to provide the CPU ability to transfer information to resources and provide for resources the ability to generically pass information to other resources.\nBy default, any CPU event that is scheduled (excluding the Tick event) is copied and scheduled as a ResourcePool event. A list of CPU events is available in cpu.hh:\n enum CPUEventType { ActivateThread, ActivateNextReadyThread, DeactivateThread, HaltThread, SuspendThread, Trap, InstGraduated, SquashFromMemStall, UpdatePCs, NumCPUEvents };  Each of these events are implemented as virtual functions in the \u0026ldquo;Resource\u0026rdquo; base class, such that any resource that needs to react to any of the aforementioned events simply needs to override the virtual function in it\u0026rsquo;s implementation.\nThe ResourcePool also has a few specific events that resources can schedule themselves. The current list is found in resource_pool.hh:\n enum ResPoolEventType { InstGraduated = InOrderCPU::NumCPUEvents, SquashAll, UpdateAfterContextSwitch, Default };  Predefined Resources The following pipeline resources are defined for InOrderCPU:\n Fetch Unit Instruction Cache (I-Cache) Branch Prediction Unit (BPred Unit) Register File Manager (RF Manager) Address Generation Unit (AGen Unit) Execution Unit (EXU) Integer Multiply and Divide Unit (Int MDU) Data Cache (D-Cache) Graduation Unit (Grad Unit)  Defining Your Own Resources The easiest way to define your own resource is to find a resource that is similar to what you are trying to create, and then use that as a template for your design.\nMore specifically, you\u0026rsquo;ll need to derive from the \u0026ldquo;Resource\u0026rdquo; class and then define your own resource-specific \u0026ldquo;execute\u0026rdquo; function. In the simplest case, where your resource is of zero (same-cycle) latency, then this should do the trick. If you resource processes requests on multiple cycles, then a good example of that is the instruction buffer, the caches, or the multiply/divide units.\nAlso note, for an instruction to use your resource, you need to :\n Add the resource to the resource pool Add the resource to that instruction\u0026rsquo;s instruction schedule  "
},
{
	"uri": "http://localhost/ruby/ruby/",
	"title": "Ruby",
	"tags": [],
	"description": "",
	"content": " High level components of Ruby Ruby implements a detailed simulation model for the memory subsystem. It models inclusive/exclusive cache hierarchies with various replacement policies, coherence protocol implementations, interconnection networks, DMA and memory controllers, various sequencers that initiate memory requests and handle responses. The models are modular, flexible and highly configurable. Three key aspects of these models are:\n Separation of concerns \u0026ndash; for example, the coherence protocol specifications are separate from the replacement policies and cache index mapping, the network topology is specified separately from the implementation. Rich configurability \u0026ndash; almost any aspect affecting the memory hierarchy functionality and timing can be controlled. Rapid prototyping \u0026ndash; a high-level specification language, SLICC, is used to specify functionality of various controllers.  The following picture, taken from the GEMS tutorial in ISCA 2005, shows a high-level view of the main components in Ruby. SLICC + Coherence protocols: SLICC stands for Specification Language for Implementing Cache Coherence. It is a domain specific language that is used for specifying cache coherence protocols. In essence, a cache coherence protocol behaves like a state machine. SLICC is used for specifying the behavior of the state machine. Since the aim is to model the hardware as close as possible, SLICC imposes constraints on the state machines that can be specified. For example, SLICC can impose restrictions on the number of transitions that can take place in a single cycle. Apart from protocol specification, SLICC also combines together some of the components in the memory model. As can be seen in the following picture, the state machine takes its input from the input ports of the inter-connection network and queues the output at the output ports of the network, thus tying together the cache / memory controllers with the inter-connection network itself.\nThe following cache coherence protocols are supported:\n MI_example: example protocol, 1-level cache. MESI_Two_Level: single chip, 2-level caches, strictly-inclusive hierarchy. MOESI_CMP_directory: multiple chips, 2-level caches, non-inclusive (neither strictly inclusive nor exclusive) hierarchy. MOESI_CMP_token: 2-level caches. TODO. MOESI_hammer: single chip, 2-level private caches, strictly-exclusive hierarchy. Garnet_standalone: protocol to run the Garnet network in a standalone manner. MESI Three Level: 3-level caches, strictly-inclusive hierarchy.  Commonly used notations and data structures in the protocols have been described in detail here.\nProtocol independent memory components  Sequencer Cache Memory Replacement Policies Memory Controller  In general cache coherence protocol independent components comprises of the Sequencer, Cache Memory structure, Cache Replacement policies and the Memory controller. The Sequencer class is responsible for feeding the memory subsystem (including the caches and the off-chip memory) with load/store/atomic memory requests from the processor. Every memory request when completed by the memory subsystem also send back the response to the processor via the Sequencer. There is one Sequencer for each hardware thread (or core) simulated in the system. The Cache Memory models a set-associative cache structure with parameterizable size, associativity, replacement policy. L1, L2, L3 caches (if exists)in the system are instances of Cache Memory. The Cache Replacement policies are kept modular from the Cache Memory, so that different instances of Cache Memory can use different replacement policies of their choice. Currently two replacement polices \u0026ndash; LRU and Pseudo-LRU \u0026ndash; are distributed with the release. Memory Controller is responsible for simulating and servicing any request that misses on all the on-chip caches of the simulated system. Memory Controller currently simple, but models DRAM ban contention, DRAM refresh faithfully. It also models close-page policy for DRAM buffer.\nEach component is described in details here.\nInterconnection Network The interconnection network connects the various components of the memory hierarchy (cache, memory, dma controllers) together.\nThe key components of an interconnection network are:\n Topology Routing Flow Control Router Microarchitecture  More details about the network model implementation are described here.\nAlternatively, Interconnection network could be replaced with the external simulator TOPAZ. This simulator is ready to run within gem5 and adds a significant number of features over original ruby network simulator. It includes, new advanced router micro-architectures, new topologies, precision-performance adjustable router models, mechanisms to speed-up network simulation, etc \u0026hellip; The presentation of the tool (and the reason why is not included in the gem5 repostories) is here\nLife of a memory request in Ruby In this section we will provide a high level overview of how a memory request is serviced by Ruby as a whole and what components in Ruby it goes through. For detailed operations within each components though, refer to previous sections describing each component in isolation.\n A memory request from a core or hardware context of gem5 enters the jurisdiction of Ruby through the RubyPort::recvTiming interface (in src/mem/ruby/system/RubyPort.hh/cc). The number of Rubyport instantiation in the simulated system is equal to the number of hardware thread context or cores (in case of non-multithreaded cores). A port from the side of each core is tied to a corresponding RubyPort. The memory request arrives as a gem5 packet and RubyPort is responsible for converting it to a RubyRequest object that is understood by various components of Ruby. It also finds out if the request is for some PIO or not and maneuvers the packet to correct PIO. Finally once it has generated the corresponding RubyRequest object and ascertained that the request is a normal memory request (not PIO access), it passes the request to the Sequencer::makeRequest interface of the attached Sequencer object with the port (variable ruby_port holds the pointer to it). Observe that Sequencer class itself is a derived class from the RubyPort class. As mentioned in the section describing Sequencer class of Ruby, there are as many objects of Sequencer in a simulated system as the number of hardware thread context (which is also equal to the number of RubyPort object in the system) and there is an one-to-one mapping between the Sequencer objects and the hardware thread context. Once a memory request arrives at the Sequencer::makeRequest, it does various accounting and resource allocation for the request and finally pushes the request to the Ruby\u0026rsquo;s coherent cache hierarchy for satisfying the request while accounting for the delay in servicing the same. The request is pushed to the Cache hierarchy by enqueueing the request to the mandatory queue after accounting for L1 cache access latency. The mandatory queue (variable name m_mandatory_q_ptr) effectively acts as the interface between the Sequencer and the SLICC generated cache coherence files. L1 cache controllers (generated by SLICC according to the coherence protocol specifications) dequeues request from the mandatory queue and looks up the cache, makes necessary coherence state transitions and/or pushes the request to the next level of cache hierarchy as per the requirements. Different controller and components of SLICC generated Ruby code communicates among themselves through instantiations of MessageBuffer class of Ruby (src/mem/ruby/buffers/MessageBuffer.cc/hh) , which can act as ordered or unordered buffer or queues. Also the delays in servicing different steps for satisfying a memory request gets accounted for scheduling enqueue-ing and dequeue-ing operations accordingly. If the requested cache block may be found in L1 caches and with required coherence permissions then the request is satisfied and immediately returned. Otherwise the request is pushed to the next level of cache hierarchy through MessageBuffer. A request can go all the way up to the Ruby\u0026rsquo;s Memory Controller (also called Directory in many protocols). Once the request get satisfied it is pushed upwards in the hierarchy through *MessageBuffer*s. The MessageBuffers also act as entry point of coherence messages to the on-chip interconnect modeled. The MesageBuffers are connected according to the interconnect topology specified. The coherence messages thus travel through this on-chip interconnect accordingly. Once the requested cache block is available at L1 cache with desired coherence permissions, the L1 cache controller informs the corresponding Sequencer object by calling its readCallback or \u0026lsquo;writeCallback\u0026rdquo; method depending upon the type of the request. Note that by the time these methods on Sequencer are called the latency of servicing the request has been implicitly accounted for. The Sequencer then clears up the accounting information for the corresponding request and then calls the RubyPort::ruby_hit_callback method. This ultimately returns the result of the request to the corresponding port of the core/ hardware context of the frontend (gem5).  Directory Structure  src/mem/  protocols: SLICC specification for coherence protocols slicc: implementation for SLICC parser and code generator ruby  common: frequently used data structures, e.g. Address (with bit-manipulation methods), histogram, data block filters: various Bloom filters (stale code from GEMS) network: Interconnect implementation, sample topology specification, network power calculations, message buffers used for connecting controllers profiler: Profiling for cache events, memory controller events recorder: Cache warmup and access trace recording slicc_interface: Message data structure, various mappings (e.g. address to directory node), utility functions (e.g. conversion between address \u0026amp; int, convert address to cache line address) structures: Protocol independent memory components – CacheMemory, DirectoryMemory system: Glue components – Sequencer, RubyPort, RubySystem    "
},
{
	"uri": "http://localhost/ruby/ruby-network-test/",
	"title": "Ruby Network Tester",
	"tags": [],
	"description": "",
	"content": " The Ruby Network Tester provides a framework for simulating the interconnection network with controlled inputs. This is useful for network testing/debugging, or for network-only simulations with synthetic traffic (especially with the garnet network). The tester works in conjunction with the Network_test coherence protocol.\nRelated Files  configs/example/ruby_network_test.py: file to invoke the network tester src/cpu/testers/networktest: files implementing the tester.  NetworkTest.py networktest.hh networktest.cc   How to run First build gem5 with the Network_test coherence protocol:\nscons build/ALPHA_Network_test/gem5.debug  Sample command to\nrun: ./build/ALPHA_Network_test/gem5.debug configs/example/ruby_network_test.py \\ --num-cpus=16 --num-dirs=16 --topology=Mesh --mesh-rows=4 \\ --sim-cycles=1000 \\ --injectionrate=0.01 \\ --synthetic=0 \\ --fixed-pkts \\ --maxpackets=1 \\ --garnet-network=fixed     Parameter Description     \u0026ndash;num-cpus Number of cpus. This is the number of source (injection) nodes in the network.   \u0026ndash;num-dirs Number of directories. This is the number of destination (ejection) nodes in the network.   \u0026ndash;topology Topology for connecting the cpus and dirs to the network routers/switches. The Mesh topology creates a network with \u0026ndash;num-cpus number of routers/switches. It requires equal number of cpus and dirs, and connects one cpu, and one dir each to one network router. More detail about different topologies can be found here.   \u0026ndash;mesh-rows The number of rows in the mesh. Only valid when \u0026ndash;topology is Mesh or MeshDirCorners.   \u0026ndash;sim-cycles Total number of cycles for which the simulation should run.   \u0026ndash;injectionrate Traffic Injection Rate in packets/node/cycle. It can take any decimal value between 0 and 1. The number of digits of precision after the decimal point can be controlled by \u0026ndash;precision which is set to 3 as default in ruby_network_test.py.   \u0026ndash;synthetic The type of synthetic traffic to be injected. Currently, 3 types of synthetic traffic: uniform-random (\u0026ndash;synthetic=0), tornado (\u0026ndash;synthetic=1), and bit-complement (\u0026ndash;synthetic=2) can be injected by the tester code in networktest.cc.   \u0026ndash;maxpackets Maximum number of packets to be injected by each cpu node. This parameter only works when \u0026ndash;fixed-pkts is declared. This is useful for network debugging.   \u0026ndash;fixed-pkts Inject only a fixed number of packets (specified by \u0026ndash;maxpackets) from each cpu. This is useful for network debugging. If this parameter is not declared, cpus will keep injecting at the specified rate till \u0026ndash;sim-cycles cycles when the simulation ends.   \u0026ndash;garnet-network Enables the garnet network. This parameter can take two values: fixed and flexible. More details about these two garnet networks can be found here.    Implementation of network tester The network tester is implemented in networktest.cc. The sequence of steps involved in generating and sending a packet are as follows.\n Every cycle, each cpu performs a bernouli trial with probability equal to \u0026ndash;injectionrate to determine whether to generate a packet or not. If \u0026ndash;fixed-pkts is enabled, each cpu stops generating new packets after generating \u0026ndash;maxpackets number of packets. The tester terminates after \u0026ndash;sim-cycles. If the cpu has to generate a new packet, it computes the destination for the new packet based on the synthetic traffic type (\u0026ndash;synthetic).  0 (Uniform Random): send to a destination directory chosen at random from the available directories. 1 (Tornado): send to the destination directory half-way across X dimension. This only makes sense for Mesh or Torus topologies. 2 (Bit-Complement): send to the destination directory whose location is the bit-complement of the source cpu location. This only makes sense for Mesh or Torus topologies.  This destination is embedded into the bits after block offset in the packet address. The generated packet is randomly tagged as a ReadReq, or an INST_FETCH, or a WriteReq, and sent to the Ruby Port (src/mem/ruby/system/RubyPort.hh/cc). The Ruby Port converts the packet into a RubyRequestType:LD, RubyRequestType:IFETCH, and RubyRequestType:ST, respectively, and sends it to the Sequencer, which in turn sends it to the Network_test cache controller. The cache controller extracts the destination directory from the packet address. The cache controller injects the LD, IFETCH and ST into virtual networks 0, 1 and 2 respectively.  LD and IFETCH are injected as control packets (8 bytes), while ST is injected as a data packet (72 bytes).  The packet traverses the network and reaches the directory. The directory controller simply drops it.  "
},
{
	"uri": "http://localhost/ruby/ruby-random-test/",
	"title": "Ruby Random Tester",
	"tags": [],
	"description": "",
	"content": " A cache coherence protocol usually has several different types of state machines, with state machine having several different states. For example, the MESI CMP directory protocol has four different state machines (L1, L2, directory, dma). Testing such a protocol for functional correctness is a challenging task. gem5 provides a random tester for testing coherence protocols. It is called the Ruby Random Tester. The source files related to the tester are present in the directory src/cpu/testers/rubytest. The file configs/examples/ruby_random_test.py is used for configuration and execution of the test. For example, the following command can be used for testing a protocol \u0026ndash;\n./build/X86/gem5.fast ./configs/example/ruby_random_test.py  Though one can specify many different options to the random tester, some of them are note worthy.\n   Parameter Description     -n, \u0026ndash;num-cpus Number of cpus injecting load/store requests to the memory system.   \u0026ndash;num-dirs Number of directory controllers in the system.   -m, \u0026ndash;maxtick Number of cycles to simulate.   -l, \u0026ndash;checks Number of loads to be performed.   \u0026ndash;random_seed Seed for initialization of the random number generator.    Testing a coherence protocol with the random tester is a tedious task and requires patience. First, build gem5 with the protocol to be tested. Then, run the ruby random tester as mentioned above. Initially one should run the tester with a single processor, and few loads. It is likely that one would encounter problems. Use the debug flags to get a trace of the events ocurring in the system. You may find the flag ProtocolTrace particularly useful. As these are rectified, keep on increasing the number of loads, say by a factor of 10 each time till one can execute one to ten million loads. Once it starts working for a single processor, a similar process now needs to be followed for a two processor system, followed by larger systems.\nTheoretical approaches exist for verifying coherence protocols[1], but gem5 currently does not include any testers based on those.\nReferences \nFong Pong and Michel Dubois. 1997. Verification techniques for cache coherence protocols. ACM Comput. Surv. 29, 1 (March 1997)\n"
},
{
	"uri": "http://localhost/deprecated/running/",
	"title": "Running gem5",
	"tags": [],
	"description": "",
	"content": " THIS IS AN OBSOLETE PAGE. DO NOT EDIT. for the most recent version of this documentation.\n Quick Start We\u0026rsquo;ll assume that you\u0026rsquo;ve already built an ALPHA_FS version of the M5 simulator, and downloaded and installed the full-system binary and disk image files. Then you can just run the fs.py configuration file in the m5/configs/examples directory. For example:\n% build/ALPHA_FS/m5.debug -d /tmp/output configs/example/fs.py M5 Simulator System Copyright (c) 2001-2006 The Regents of The University of Michigan All Rights Reserved M5 compiled Aug 16 2006 18:51:57 M5 started Wed Aug 16 21:53:38 2006 M5 executing on zeep command line: ./build/ALPHA_FS/m5.debug configs/example/fs.py 0: system.tsunami.io.rtc: Real-time clock set to Sun Jan 1 00:00:00 2006 Listening for console connection on port 3456 0: system.remote_gdb.listener: listening for remote gdb #0 on port 7000 warn: Entering event queue @ 0. Starting simulation... \u0026lt;...simulation continues...\u0026gt;  Basic Operation By default, the fs.py script boots Linux and starts a shell on the system console. To keep console traffic separate from simulator input and output, this simulated console is associated with a TCP port. To interact with the console, you must connect to the port using a program such as telnet, for example:\n % telnet localhost 3456\nTelnet\u0026rsquo;s echo behavior doesn\u0026rsquo;t work well with m5, so if you are using the console regularly, you probably want to use M5term instead of telnet. By default m5 will try to use port 3456, as in the example above. However, if that port is already in use, it will increment the port number until it finds a free one. The actual port number used is printed in the m5 output.\nIn addition to loading a Linux kernel, M5 mounts one or more disk images for its filesystems. At least one disk image must be mounted as the root filesystem. Any application binaries that you want to run must be present on these disk images. To begin running benchmarks without requiring an interactive shell session, M5 can load .rcS files that replace the normal Linux boot scripts to directly execute from after booting the OS. These .rcS files can be used to configure ethernet interfaces, execute special m5 instructions, or begin executing a binary on the disk image. The pointers for the linux binary, disk images, and .rcS files are all set in the simulation script. (To see how these files work, see Simulation Scripts Explained.) Examples: Going into / of root filesystem and typing ls will show:\n benchmarks etc lib mnt sbin usr bin floppy lost+found modules sys var dev home man proc tmp z  Snippet of an .rcS file:\necho -n \u0026quot;setting up network...\u0026quot; /sbin/ifconfig eth0 192.168.0.10 txqueuelen 1000 /sbin/ifconfig lo 127.0.0.1 echo -n \u0026quot;running surge client...\u0026quot; /bin/bash -c \u0026quot;cd /benchmarks/surge \u0026amp;\u0026amp; ./Surge 2 100 1 192.168.0.1 5. echo -n \u0026quot;halting machine\u0026quot; m5 exit  Full System Benchmarks We have several full-system benchmarks already up and running. The binaries are available in the disk images you can obtain/download from us, and the .rcS files are in the m5/configs/boot/ directory. To run any of them, you merely need to set the benchmark option to the name of the test you want to run. For example:\n%./build/ALPHA_FS/m5.opt configs/example/fs.py -b NetperfMaerts  To see a comprehensive list of all benchmarks available:\n%./build/ALPHA_FS/m5.opt configs/examples/fs.py -h  Not every benchmark is commonly used though, and not all are guaranteed to be useful or in working condition. However, we do often run:\n NetperfMaerts NetperfStreamNT SurgeSpecweb  These should run without a problem, since we have flushed out most bugs.\nCurrently under development:\n NFS iSCSI video streaming  "
},
{
	"uri": "http://localhost/docs/running/",
	"title": "Running gem5",
	"tags": [],
	"description": "",
	"content": " Usage The gem5 command line has four parts, the gem5 binary, options for the binary, a simulation script, and options for the script. The options that are passed to the gem5 binary and those passed to the script are handled separately, so be sure any options you use are being passed to the right component.\n% \u0026lt;gem5 binary\u0026gt; [gem5 options] \u0026lt;simulation script\u0026gt; [script options]  gem5 Options Running gem5 with the \u0026ldquo;-h\u0026rdquo; flag prints a help message that includes all of the supported simulator options. Here\u0026rsquo;s a snippet:\n% build/ALPHA/gem5.debug -h Usage ===== gem5.debug [gem5 options] script.py [script options] Copyright (c) 2001-2008 The Regents of The University of Michigan All Rights Reserved gem5 is copyrighted software; use the --copyright option for details. Options ======= --version show program's version number and exit --help, -h show this help message and exit --build-info, -B Show build information --copyright, -C Show full copyright information --readme, -R Show the readme --outdir=DIR, -d DIR Set the output directory to DIR [Default: m5out] --redirect-stdout, -r Redirect stdout (\u0026amp; stderr, without -e) to file --redirect-stderr, -e Redirect stderr to file --stdout-file=FILE Filename for -r redirection [Default: simout] --stderr-file=FILE Filename for -e redirection [Default: simerr] --interactive, -i Invoke the interactive interpreter after running the script --pdb Invoke the python debugger before running the script --path=PATH[:PATH], -p PATH[:PATH] Prepend PATH to the system path when invoking the script --quiet, -q Reduce verbosity ...  The default options that gem5 uses to run can be set by creating an ~/.m5/options.py file and placing options that you are interested in there. For example, if you would like to always redirect standard error and out to a file you could add: options.stdout_file=simout to options.py.\nScript Options The script section of the command line begins with a path to your script file and includes any options that you\u0026rsquo;d like to pass to that script. Most Example scripts allow you to pass a \u0026lsquo;-h\u0026rsquo; or \u0026lsquo;\u0026ndash;help\u0026rsquo; flag to the script to see script specific options. An example is as follows:\ngem5 compiled Apr 2 2011 00:57:11 gem5 started Apr 3 2011 21:16:02 gem5 executing on zooks command line: build/ALPHA/gem5.opt configs/example/se.py -h Usage: se.py [options] Options: -h, --help show this help message and exit -c CMD, --cmd=CMD The binary to run in syscall emulation mode. -o OPTIONS, --options=OPTIONS The options to pass to the binary, use \u0026quot; \u0026quot; around the entire string -i INPUT, --input=INPUT Read stdin from a file. --output=OUTPUT Redirect stdout to a file. --errout=ERROUT Redirect stderr to a file. --ruby -d, --detailed -t, --timing --inorder -n NUM_CPUS, --num-cpus=NUM_CPUS --caches --l2cache --fastmem --clock=CLOCK --num-dirs=NUM_DIRS --num-l2caches=NUM_L2CACHES --l1d_size=L1D_SIZE --l1i_size=L1I_SIZE --l2_size=L2_SIZE --l1d_assoc=L1D_ASSOC --l1i_assoc=L1I_ASSOC --l2_assoc=L2_ASSOC ...  The script file documentation page (Configuration / Simulation Scripts) describes how to write your own simulation scripts, and the Options section explains how to add your own command line options. The simulation scripts that are most commonly used are se.py and fs.py. These scripts are present in configs/examples directory. se.py is meant for simulation using the system call emulation mode, while fs.py is for full-system simulations. In most cases, it should be possible to use either of these two scripts without any modifications. Understanding how these two scripts work can help you decide on what modifications are required for your particular case.\nSystem Call Emulation (SE) Mode In this mode, one only needs to specify the binary file to be simulated. This binary file can be statically/dynamically linked. configs/examples/se.py is used for configuring and running simulations in this mode. What follows is probably the simplest example of how to use se.py. The binary file to simulated is specified with option\n**-c**. $ ./build/ALPHA/gem5.opt ./configs/example/se.py -c ./tests/test-progs/hello/bin/alpha/linux/hello gem5 Simulator System. http://gem5.org gem5 is copyrighted software; use the --copyright option for details. gem5 compiled Mar 2 2014 00:06:39 gem5 started Mar 4 2014 10:52:10 gem5 executing on $ command line: ./build/ALPHA/gem5.opt ./configs/example/se.py -c ./tests/test-progs/hello/bin/alpha/linux/hello Global frequency set at 1000000000000 ticks per second 0: system.remote_gdb.listener: listening for remote gdb #0 on port 7000 **** REAL SIMULATION **** info: Entering event queue @ 0. Starting simulation... info: Increasing stack size by one page. Hello world! Exiting @ tick 3233000 because target called exit()  Specifying Command-Line Arguments In order to pass command line arguments to a binary you can use --options=\u0026quot;arg1 arg2 ...\u0026quot; to specify them as a script option in your simulation command.\nFull System (FS) Mode This mode simulates a complete system which provides an operating system based simulation environment. For full system mode, you can use the file configs/example/fs.py for configuration and simulation. Sensible default values have been set for the options that this script uses. We provide examples for ALPHA and ARM based full system simulations.\n{{#ev:youtube|gd_DtxQD5kc|400|center|Example video showing gem5 full system simulation for ARM. Host system is x86 64bit Ubuntu 12.04. Video resolution can be set to 1080}}\nBooting Linux We\u0026rsquo;ll assume that you\u0026rsquo;ve already built an ALPHA version of the gem5 simulator, and downloaded and installed the full-system binary and disk image files. Then you can run the fs.py configuration file in the gem5/configs/examples directory. For example:\n% build/ALPHA/gem5.debug -d /tmp/output configs/example/fs.py gem5 Simulator System Copyright (c) 2001-2006 The Regents of The University of Michigan All Rights Reserved gem5 compiled Aug 16 2006 18:51:57 gem5 started Wed Aug 16 21:53:38 2006 gem5 executing on zeep command line: ./build/ALPHA/gem5.debug configs/example/fs.py 0: system.tsunami.io.rtc: Real-time clock set to Sun Jan 1 00:00:00 2006 Listening for console connection on port 3456 0: system.remote_gdb.listener: listening for remote gdb #0 on port 7000 warn: Entering event queue @ 0. Starting simulation... \u0026lt;...simulation continues...\u0026gt;  Basic Operation By default, the fs.py script boots Linux and starts a shell on the system console. To keep console traffic separate from simulator input and output, this simulated console is associated with a TCP port. To interact with the console, you must connect to the port using a program such as telnet, for example:\n % telnet localhost 3456\nTelnet\u0026rsquo;s echo behavior doesn\u0026rsquo;t work well with gem5, so if you are using the console regularly, you probably want to use M5term instead of telnet. By default gem5 will try to use port 3456, as in the example above. However, if that port is already in use, it will increment the port number until it finds a free one. The actual port number used is printed in the gem5 output.\nIn addition to loading a Linux kernel, gem5 mounts one or more disk images for its filesystems. At least one disk image must be mounted as the root filesystem. Any application binaries that you want to run must be present on these disk images. To begin running benchmarks without requiring an interactive shell session, gem5 can load .rcS files that replace the normal Linux boot scripts to directly execute from after booting the OS. These .rcS files can be used to configure ethernet interfaces, execute special gem5 instructions, or begin executing a binary on the disk image. The pointers for the linux binary, disk images, and .rcS files are all set in the simulation script. (To see how these files work, see Configuration / Simulation Scripts.) Examples: Going into / of root filesystem and typing ls will show:\n benchmarks etc lib mnt sbin usr bin floppy lost+found modules sys var dev home man proc tmp z  Snippet of an .rcS file:\necho -n \u0026quot;setting up network...\u0026quot; /sbin/ifconfig eth0 192.168.0.10 txqueuelen 1000 /sbin/ifconfig lo 127.0.0.1 echo -n \u0026quot;running surge client...\u0026quot; /bin/bash -c \u0026quot;cd /benchmarks/surge \u0026amp;\u0026amp; ./Surge 2 100 1 192.168.0.1 5. echo -n \u0026quot;halting machine\u0026quot; m5 exit  m5term The m5term program allows the user to connect to the simulated console interface that full-system gem5 provides. Simply change into the util/term directory and build m5term:\n % cd gem5/util/term % make gcc -o m5term term.c % make install sudo install -o root -m 555 m5term /usr/local/bin  The usage of m5term is:\n ./m5term \u0026lt;host\u0026gt; \u0026lt;port\u0026gt; \u0026lt;host\u0026gt; is the host that is running gem5 \u0026lt;port\u0026gt; is the console port to connect to. gem5 defaults to using port 3456, but if the port is used, it will try the next higher port until it finds one available. If there are multiple systems running within one simulation, there will be a console for each one. (The first system's console will be on 3456 and the second on 3457 for example) m5term uses '~' as an escape character. If you enter the escape character followed by a '.', the m5term program will exit.  m5term can be used to interactively work with the simulator, though users must often set various terminal settings to get things to work\nA slightly shortened example of m5term in action:\n % m5term localhost 3456 ==== m5 slave console: Console 0 ==== M5 console Got Configuration 127 memsize 8000000 pages 4000 First free page after ROM 0xFFFFFC0000018000 HWRPB 0xFFFFFC0000018000 l1pt 0xFFFFFC0000040000 l2pt 0xFFFFFC0000042000 l3pt_rpb 0xFFFFFC0000044000 l3pt_kernel 0xFFFFFC0000048000 l2reserv 0xFFFFFC0000046000 CPU Clock at 2000 MHz IntrClockFrequency=1024 Booting with 1 processor(s) ... ... VFS: Mounted root (ext2 filesystem) readonly. Freeing unused kernel memory: 480k freed init started: BusyBox v1.00-rc2 (2004.11.18-16:22+0000) multi-call binary PTXdist-0.7.0 (2004-11-18T11:23:40-0500) mounting filesystems... EXT2-fs warning: checktime reached, running e2fsck is recommended loading script... Script from M5 readfile is empty, starting bash shell... # ls benchmarks etc lib mnt sbin usr bin floppy lost+found modules sys var dev home man proc tmp z #  Full System Benchmarks We have several full-system benchmarks already up and running. The binaries are available in the disk images you can obtain/download from us, and the .rcS files are in the gem5/configs/boot/ directory. To run any of them, you merely need to set the benchmark option to the name of the test you want to run. For example:\n%./build/ALPHA/gem5.opt configs/example/fs.py -b NetperfMaerts  To see a comprehensive list of all benchmarks available:\n%./build/ALPHA/gem5.opt configs/examples/fs.py -h  Experimenting with DVFS This is a quick hands-on tutorial to start a DVFS-enabled system where the Linux DVFS governors can change voltage and frequencies of the ongoing simulation. Right now, the driver and interface components live in ARM-specific parts of the Linux kernel / gem5, but there is no fundamental reason why this could not be ported to work on other architectures, too.\nQuick Instructions These instructions apply for a Ubuntu-based machine, but can be easily adapted / extended etc. for other use cases and systems.\n Get gem5 with the proper changesets added Anything after 1  hg clone http://repo.gem5.org/gem5  \nBuild gem5\n\nscons build/ARM/gem5.opt -j 8  \nGet a DVFS-enabled Linux kernel\n\n From here: 2 Anything after / including 3  git clone --depth 10 git://www.linux-arm.org/linux-linaro-tracking-gem5.git  \nGet a cross-compile tool chain\n\nsudo apt-get install gcc-arm-linux-gnueabihf  \nBuild the kernel\n\n See also Linux_kernel  make ARCH=arm vexpress_gem5_dvfs_defconfig make ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- -j8  \nCheck / select the right DTS / DTB file\n\nls arch/arm/boot/dts/vexpress-v2*dvfs*  \nGet / prepare a disk image\n\nwget http://www.gem5.org/dist/current/arm/arm-system-2013-07.tar.bz2 tar xvjf arm-system-2013-07.tar.bz2  \nAdd DVFS points to the configuration\n\n Enable and link the energy controller / DVFS handler\npatch -p1 \u0026lt;\u0026lt; EOF diff --git a/configs/example/fs.py b/configs/example/fs.py --- a/configs/example/fs.py +++ b/configs/example/fs.py @@ -106,7 +106,11 @@ # Create a source clock for the CPUs and set the clock period test_sys.cpu_clk_domain = SrcClockDomain(clock = options.cpu_clock, voltage_domain = - test_sys.cpu_voltage_domain) + test_sys.cpu_voltage_domain, + domain_id = 0) + + test_sys.dvfs_handler.domains = test_sys.cpu_clk_domain + test_sys.dvfs_handler.enable = 1 if options.kernel is not None: test_sys.kernel = binary(options.kernel) EOF    Can also change the clock frequencies here, or from command line  \nStart a simple test\nsimulation  \nM5_PATH=$(pwd)/.. ./build/ARM/gem5.opt --debug-flags=DVFS,EnergyCtrl \\ --debug-file=dfvs_debug.log configs/example/fs.py --cpu-type=AtomicSimpleCPU \\ -n 2 --machine-type=VExpress_EMM --kernel=../linux-linaro-tracking-gem5/vmlinux \\ --dtb-filename=../linux-linaro-tracking-gem5/arch/arm/boot/dts/\\ vexpress-v2p-ca15-tc1-gem5_dvfs_2cpus.dtb \\ --disk-image=../disks/arm-ubuntu-natty-headless.img \\ --cpu-clock=\\['1 GHz','750 MHz','500 MHz'\\]  \nTest DVFS functionality\n\nutil/term/m5term 3456 \u0026lt;login\u0026gt; cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_available_frequencies echo 750187 \u0026gt; /sys/devices/system/cpu/cpu0/cpufreq/scaling_max_freq  \nFuther Experiments  Set up different voltages for the operating points\npatch -p1 \u0026lt; EOF diff --git a/configs/example/fs.py b/configs/example/fs.py --- a/configs/example/fs.py +++ b/configs/example/fs.py @@ -101,12 +101,16 @@ voltage_domain = test_sys.voltage_domain) # Create a CPU voltage domain - test_sys.cpu_voltage_domain = VoltageDomain() + test_sys.cpu_voltage_domain = VoltageDomain(voltage = ['1V','0.9V','0.8V']) # Create a source clock for the CPUs and set the clock period test_sys.cpu_clk_domain = SrcClockDomain(clock = options.cpu_clock, voltage_domain = - test_sys.cpu_voltage_domain) + test_sys.cpu_voltage_domain, + domain_id = 0) + + test_sys.dvfs_handler.domains = test_sys.cpu_clk_domain + test_sys.dvfs_handler.enable = 1 if options.kernel is not None: test_sys.kernel = binary(options.kernel) EOF  Per-core DVFS\n   Set up separate clock (and voltage) domains per core Separate clock domains need separate clusters in the device tree diff -u linux-linaro-tracking-gem5/arch/arm/boot/dts/\nvexpress-v2p-ca15-tc1-gem5dvfs{,percore}4cpus.dts Change the socket_id to have a separate socket per CPU core  "
},
{
	"uri": "http://localhost/developer/scons/",
	"title": "SCons",
	"tags": [],
	"description": "",
	"content": " This page provides some tips on using M5\u0026rsquo;s build system.\nM5\u0026rsquo;s build system uses SCons, a powerful replacement for make (and autoconf, and ccache). SCons uses standard Python to describe the software build process, enabling some very sophisticated \u0026amp; complex build procedures.\nThis page is not an attempt to fully document the build system. Because many people aren\u0026rsquo;t familiar with SCons, and because gem5\u0026rsquo;s build system in some areas takes advantage of the complexity that SCons enables, this page tries to collect useful pointers that might not otherwise be obvious.\nSCons tips  As with make, you can build multiple targets by specifying them all on a single command line. As with make, the \u0026ldquo;-j\u0026rdquo; option can be used to enable parallel builds (useful on multiprocessor hosts).  -\nFor example, the following command will build both the optimized Alpha syscall emulation and debug MIPS syscall emulation targets using up to 4 concurrent processes:  % scons -j 4 build/ALPHA/m5.opt build/MIPS/m5.debug\n You can print out all the generic SCons options using \u0026ldquo;scons -H\u0026rdquo;. If you specify a directory rather than a file, SCons will build all of the targets it knows about under that directory. Thus \u0026ldquo;scons build/ALPHA\u0026rdquo; will build all of the various ALPHA binaries and run the regression tests on them (since the regression test targets are in the subdirectory build/ALPHA/tests). The --debug=explain and --debug=tree options are very useful for figuring out why SCons does (or doesn\u0026rsquo;t) rebuild a target when you change a source file.  gem5 build system tips  There are a number of command-line options you can set of the form \u0026ldquo;option=value\u0026rdquo;, for example:  % scons CC=gcc44 USE_MYSQL=False build/ALPHA/gem5.opt\n-\nAlmost all of these options are *sticky*, that is, if you specify them once, they remain in force for all future builds in that particular target (e.g., ALPHA) until they are changed explicitly in a future invocation. (The only non-sticky option is `update_ref`, which, when set, causes the results of any regression tests to overwrite the reference outputs in preparation for being committed as the new reference outputs.)   You can print out all the gem5-specific build options using \u0026ldquo;scons -h\u0026rdquo;. You can define your own configurations beyond ALPHA, ARM, etc. These names are just the names of files in the build_opts directory containing option settings. You can generate a new configuration by specifying an predefined configuration (using default=) and overriding other options on the command line, e.g.:  % scons --default ALPHA USE_MYSQL=False build/ALPHA_NOSQL/gem5.debug % scons --default ALPHA CC=mycc CXX=myc++ build/ALPHA_MYCOMPILERS/gem5.debug\n-\nThe configuration name is actually totally arbitrary, and need not include the ISA name or anything else:  % scons --default MIPS build/FOOBAR/gem5.debug\n The build options for a particular configuration are stored in the build/options directory, so you can blow away your configuration-specific build directory (\u0026rdquo;rm -rf build/ALPHA\u0026rdquo;) without losing the settings of your sticky options. All the build state, including SCons state as well as any non-default build option settings, are stored under the build directory. Thus if you delete the entire build directory (\u0026ldquo;\u0026rdquo;rm -rf build\u0026rdquo;) then you are back at the state you were in when you first unpacked the source tree. You can build binaries in locations other than the \u0026ldquo;build\u0026rdquo; subdirectory of your source tree. This feature can be useful in some circumstances, for example if you have a central NFS-mounted copy of your source tree, but you want to build that tree on multiple different hosts using the local disk of each host. (Supporting this model is another reason why all the bulid state is kept in the build directory and not in the root of the source tree.) See the comments at the top of the m5/SConstruct file.  "
},
{
	"uri": "http://localhost/ruby/slicc/",
	"title": "SLICC",
	"tags": [],
	"description": "",
	"content": " SLICC is a domain specific language for specifying cache coherence protocols. The SLICC compiler generates C++ code for different controllers, which can work in tandem with other parts of Ruby. The compiler also generates an HTML specification of the protocol. HTML generation is turned off by default. To enable HTML output, pass the option \u0026ldquo;SLICC_HTML=True\u0026rdquo; to scons when compiling.\nInput To the Compiler The SLICC compiler takes, as input, files that specify the controllers involved in the protocol. The .slicc file specifies the different files used by the particular protocol under consideration. For example, if trying to specify the MI protocol using SLICC, then we may use MI.slicc as the file that specifies all the files necessary for the protocol. The files necessary for specifying a protocol include the definitions of the state machines for different controllers, and of the network messages that are passed on between these controllers.\nThe files have a syntax similar to that of C++. The compiler, written using PLY (Python Lex-Yacc), parses these files to create an Abstract Syntax Tree (AST). The AST is then traversed to build some of the internal data structures. Finally the compiler outputs the C++ code by traversing the tree again. The AST represents the hierarchy of different structures present with in a state machine. We describe these structures next.\nProtocol State Machines In this section we take a closer look at what goes in to a file containing specification of a state machine.\nSpecifying Data Members Each state machine is described using SLICC\u0026rsquo;s machine datatype. Each machine has several different types of members. Machines for cache and directory controllers include cache memory and directory memory data members respectively. We will use the MI protocol available in src/mem/protocol as our running example. So here is how you might want to start writing a state machine\n machine(MachineType:L1Cache, \u0026quot;MI Example L1 Cache\u0026quot;)  : Sequencer * sequencer,  CacheMemory * cacheMemory,  int cache_response_latency = 12,  int issue_latency = 2 {  // Add rest of the stuff  }\n In order to let the controller receive messages from different entities in the system, the machine has a number of Message Buffers. These act as input and output ports for the machine. Here is an example specifying the output ports.   MessageBuffer requestFromCache, network=\u0026quot;To\u0026quot;, virtual_network=\u0026quot;2\u0026quot;, ordered=\u0026quot;true\u0026quot;;  MessageBuffer responseFromCache, network=\u0026quot;To\u0026quot;, virtual_network=\u0026quot;4\u0026quot;, ordered=\u0026quot;true\u0026quot;;\nNote that Message Buffers have some attributes that need to be specified correctly. Another example, this time for specifying the input ports.\n MessageBuffer forwardToCache, network=\u0026quot;From\u0026quot;, virtual_network=\u0026quot;3\u0026quot;, ordered=\u0026quot;true\u0026quot;;  MessageBuffer responseToCache, network=\u0026quot;From\u0026quot;, virtual_network=\u0026quot;4\u0026quot;, ordered=\u0026quot;true\u0026quot;;\n Next the machine includes a declaration of the states that machine can possibly reach. In cache coherence protocol, states can be of two types \u0026ndash; stable and transient. A cache block is said to be in a stable state if in the absence of any activity (in coming request for the block from another controller, for example), the cache block would remain in that state for ever. Transient states are required for transitioning between stable states. They are needed when ever the transition between two stable states can not be done in an atomic fashion. Next is an example that shows how states are declared. SLICC has a keyword state_declaration that has to be used for declaring states.   state_declaration(State, desc=\u0026quot;Cache states\u0026quot;) {  I, AccessPermission:Invalid, desc=\u0026quot;Not Present/Invalid\u0026quot;;  II, AccessPermission:Busy, desc=\u0026quot;Not Present/Invalid, issued PUT\u0026quot;;  M, AccessPermission:Read_Write, desc=\u0026quot;Modified\u0026quot;;  MI, AccessPermission:Busy, desc=\u0026quot;Modified, issued PUT\u0026quot;;  MII, AccessPermission:Busy, desc=\u0026quot;Modified, issued PUTX, received nack\u0026quot;;  IS, AccessPermission:Busy, desc=\u0026quot;Issued request for LOAD/IFETCH\u0026quot;;  IM, AccessPermission:Busy, desc=\u0026quot;Issued request for STORE/ATOMIC\u0026quot;;  }\nThe states I and M are the only stable states in this example. Again note that certain attributes have to be specified with the states.\n The state machine needs to specify the events it can handle and thus transition from one state to another. SLICC provides the keyword enumeration which can be used for specifying the set of possible events. An example to shed more light on this -   enumeration(Event, desc=\u0026quot;Cache events\u0026quot;) {  // From processor  Load, desc=\u0026quot;Load request from processor\u0026quot;;  Ifetch, desc=\u0026quot;Ifetch request from processor\u0026quot;;  Store, desc=\u0026quot;Store request from processor\u0026quot;;  Data, desc=\u0026quot;Data from network\u0026quot;;  Fwd_GETX, desc=\u0026quot;Forward from network\u0026quot;;  Inv, desc=\u0026quot;Invalidate request from dir\u0026quot;;  Replacement, desc=\u0026quot;Replace a block\u0026quot;;  Writeback_Ack, desc=\u0026quot;Ack from the directory for a writeback\u0026quot;;  Writeback_Nack, desc=\u0026quot;Nack from the directory for a writeback\u0026quot;;  }\n While developing a protocol machine, we may need to define structures that represent different entities in a memory system. SLICC provides the keyword structure for this purpose. An example follows   structure(Entry, desc=\u0026quot;...\u0026quot;, interface=\u0026quot;AbstractCacheEntry\u0026quot;) {  State CacheState, desc=\u0026quot;cache state\u0026quot;;  bool Dirty, desc=\u0026quot;Is the data dirty (different than memory)?\u0026quot;;  DataBlock DataBlk, desc=\u0026quot;Data in the block\u0026quot;;  }\nThe cool thing about using SLICC\u0026rsquo;s structure is that it automatically generates for you the get and set functions on different fields. It also writes a nice print function and overloads the \u0026lt;\u0026lt; operator. But in case you would prefer do everything on your own, you can make use of the keyword external in the declaration of the structure. This would prevent SLICC from generating C++ code for this structure.\n structure(TBETable, external=\u0026quot;yes\u0026quot;) {  TBE lookup(Address);  void allocate(Address);  void deallocate(Address);  bool isPresent(Address);  }\nIn fact many predefined types exist in src/mem/protocol/RubySlicc_*.sm files. You can make use of them, or if you need new types, you can define new ones as well. You can also use the keyword interface to make use of inheritance features available in C++. Note that currently SLICC supports public inheritance only.\n We can also declare and define functions as we do in C++. There are certain functions that the compiler expects would always be defined by the controller. These include  getState() setState()   Input for the Machine Since protocol is state machine, we need to specify how to machine transitions from one state to another on receiving inputs. As mentioned before, each machine has several input and output ports. For each input port, the in_port keyword is used for specifying the behavior of the machine, when a message is received on that input port. An example follows that shows the syntax for declaring an input port.\n in_port(mandatoryQueue_in, RubyRequest, mandatoryQueue, desc=\u0026quot;...\u0026quot;) {  if (mandatoryQueue_in.isReady()) {  peek(mandatoryQueue_in, RubyRequest, block_on=\u0026quot;LineAddress\u0026quot;) {  Entry cache_entry := getCacheEntry(in_msg.LineAddress);  if (is_invalid(cache_entry) \u0026amp;\u0026amp;  cacheMemory.cacheAvail(in_msg.LineAddress) == false ) {  // make room for the block  trigger(Event:Replacement, cacheMemory.cacheProbe(in_msg.LineAddress),  getCacheEntry(cacheMemory.cacheProbe(in_msg.LineAddress)),  TBEs[cacheMemory.cacheProbe(in_msg.LineAddress)]);  }  else {  trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress,  cache_entry, TBEs[in_msg.LineAddress]);  }  }  }  }\n As you can see, in_port takes in multiple arguments. The first argument, mandatoryQueue_in, is the identifier for the in_port that is used in the file. The next argument, RubyRequest, is the type of the messages that this input port receives. Each input port uses a queue to store the messages, the name of the queue is the third argument.   The keyword peek is used to extract messages from the queue of the input port. The use of this keyword implicitly declares a variable in_msg which is of the same type as specified in the input port\u0026rsquo;s declaration. This variable points to the message at the head of the queue. It can be used for accessing the fields of the message as shown in the code above.   Once the incoming message has been analyzed, it is time for using this message for taking some appropriate action and changing the state of the machine. This done using the keyword trigger. The trigger function is actually used only in SLICC code and is not present in the generated code. Instead this call is converted in to a call to the doTransition() function which appears in the generated code. The doTransition() function is automatically generated by SLICC for each of the state machines. The number of arguments to trigger depend on the machine itself. In general, the input arguments for trigger are the type of the message that needs to processed, the address for which this message is meant for, the cache and the transaction buffer entries for that address.   trigger also increments a counter that is checked before a transition is made. In one ruby cycle, there is a limit on the number of transitions that can be carried out. This is done to resemble more closely to a hardware based state machine. @TODO: What happens if there are no more transitions left? Does the wakeup abort?  Actions In this section we will go over how the actions that a state machine can carry out are defined. These actions will be called in to action when the state machine receives some input message which is then used to make a transition. Let\u0026rsquo;s go over an example on how the key word action can be made use of.\n action(a_issueRequest, \u0026quot;a\u0026quot;, desc=\u0026quot;Issue a request\u0026quot;) {  enqueue(requestNetwork_out, RequestMsg, latency=issue_latency) {  out_msg.Address := address;  out_msg.Type := CoherenceRequestType:GETX;  out_msg.Requestor := machineID;  out_msg.Destination.add(map_Address_to_Directory(address));  out_msg.MessageSize := MessageSizeType:Control;  }  }\n The first input argument is the name of the action, the next argument is the abbreviation used for generating the documentation and last one is the description of the action which used in the HTML documentation and as a comment in the C++ code.   Each action is converted in to a C++ function of that name. The generated C++ code implicitly includes up to three input parameters in the function header, again depending on the machine. These arguments are the memory address on which the action is being taken, the cache and transaction buffer entries pertaining to this address.   Next useful thing to look at is the enqueue keyword. This keyword is used for queuing a message, generated as a result of the action, to an output port. The keyword takes three input arguments, namely, the name of the output port, the type of the message to be queued and the latency after which this message can be dequeued. Note that in case randomization is enabled, the specified latency is ignored. The use of the keyword implicitly declares a variable out_msg which is populated by the follow on statements.  Transitions A transition function is a mapping from the cross product of set of states and set of events to the set of states. SLICC provides the keyword transition for specifying the transition function for state machines. An example follows \u0026ndash;\n transition(IM, Data, M) {  u_writeDataToCache;  sx_store_hit;  w_deallocateTBE;  n_popResponseQueue;  }\nIn this example, the initial state is IM. If an event of type Data occurs in that state, then final state would be M. Before making the transition, the state machine can perform certain actions on the structures that it maintains. In the given example, u_writeDataToCache is an action. All these operations are performed in an atomic fashion, i.e. no other event can occur before the set of actions specified with the transition has been completed.\n For ease of use, sets of events and states can be provided as input to transition. The cross product of these sets will map to the same final state. Note that the final state cannot be a set. If for a particular event, the final state is same as the initial state, then the final state can be omitted.   transition({IS, IM, MI, II}, {Load, Ifetch, Store, Replacement}) {  z_stall;  }\nSpecial Functions Stalling/Recycling/Waiting input ports One of the more complicated internal features of SLICC and the resulting state machines is how the deal with the situation when events cannot be process due to the cache block being in a transient state. There are several possible ways to deal with this situation and each solution has different tradeoffs. This sub-section attempts to explain the differences. Please email the gem5-user list for further follow-up.\nStalling the input port The simplest way to handle events that can\u0026rsquo;t be processed is to simply stall the input port. The correct way to do this is to include the \u0026ldquo;z_stall\u0026rdquo; action within the transition statement:\n transition({IS, IM, MI, II}, {Load, Ifetch, Store, Replacement}) {  z_stall;  }\nInternally SLICC will return a ProtocolStall for this transition and no subsequent messages from the associated input port will be processed until the stalled message is processed. However, the other input ports will be analyzed for ready messages and processed in parallel. While this is a relatively simple solution, one may notice that stalling unrelated messages on the same input port will cause excessive and unnecessary stalls.\nOne thing to note is Do Not leave the transition statement blank like so:\n transition({IS, IM, MI, II}, {Load, Ifetch, Store, Replacement}) {  // stall the input port by simply not popping the message  }\nThis will cause SLICC to return success for this transition and SLICC will continue to repeatedly analyze the same input port. The result is eventual deadlock.\nRecycling the input port The better performance but more unrealistic solution is to recycle the stalled message on the input port. The way to do this is to use the \u0026ldquo;zz_recycleMandatoryQueue\u0026rdquo; action:\n action(zz_recycleMandatoryQueue, \u0026quot;\\z\u0026quot;, desc=\u0026quot;Send the head of the mandatory queue to the back of the queue.\u0026quot;) {  mandatoryQueue_in.recycle();  }\n transition({IS, IM, MI, II}, {Load, Ifetch, Store, Replacement}) {  zz_recycleMandatoryQueue;  }\nThe result of this action is that the transition returns a Protocol Stall and the offending message moved to the back of the FIFO input port. Therefore, other unrelated messages on the same input port can be processed. The problem with this solution is that recycled messages may be analyzed and reanalyzed every cycle until an address changes state.\nStall and wait the input port An even better, but more complicated solution is to \u0026ldquo;stall and wait\u0026rdquo; the offending input message. The way to do this is to use the \u0026ldquo;z_stallAndWaitMandatoryQueue\u0026rdquo; action:\n action(z_stallAndWaitMandatoryQueue, \u0026quot;\\z\u0026quot;, desc=\u0026quot;recycle L1 request queue\u0026quot;) {  stall_and_wait(mandatoryQueue_in, address);  }\n transition({IS, IM, IS_I, M_I, SM, SINK_WB_ACK}, {Load, Ifetch, Store, L1_Replacement}) {  z_stallAndWaitMandatoryQueue;  }\nThe result of this action is that the transition returns success, which is ok because stall_and_wait moves the offending message off the input port and to a side table associated with the input port. The message will not be analyzed again until it is woken up. In the meantime, other unrelated messages will be processed.\nThe complicated part of stall and wait is that stalled messages must be explicitly woken up by other messages/transitions. In particular, transitions that move an address to a base state should wake up potentially stalled messages waiting for that address:\n action(kd_wakeUpDependents, \u0026quot;kd\u0026quot;, desc=\u0026quot;wake-up dependents\u0026quot;) {  wakeUpBuffers(address);  }\n transition(M_I, WB_Ack, I) {  s_deallocateTBE;  o_popIncomingResponseQueue;  kd_wakeUpDependents;  }\nReplacements are particularly complicated since stalled addresses are not associated with the same address they are actually waiting to change. In those situations all waiting messages must be woken up:\n action(ka_wakeUpAllDependents, \u0026quot;ka\u0026quot;, desc=\u0026quot;wake-up all dependents\u0026quot;) {  wakeUpAllBuffers();  }\n transition(I, L2_Replacement) {  rr_deallocateL2CacheBlock;  ka_wakeUpAllDependents;  }\nOther Compiler Features  SLICC supports conditional statements in form of if and else. Note that SLICC does not support else if.   Each function has return type which can be void as well. Returned values cannot be ignored.   SLICC has limited support for pointer variables. is_valid() and is_invalid() operations are supported for testing whether a given pointer \u0026lsquo;is not NULL\u0026rsquo; and \u0026lsquo;is NULL\u0026rsquo; respectively. The keyword OOD, which stands for Out of Domain, plays the role of keyword NULL used in C++.   SLICC does not support ! (the not operator).   Static type casting is supported in SLICC. The keyword static_cast has been provided for this purpose. For example, in the following piece of code, a variable of type AbstractCacheEntry is being casted in to a variable of type Entry.   Entry L1Dcache_entry := static_cast(Entry, \u0026quot;pointer\u0026quot;, L1DcacheMemory[addr]);\nSLICC Internals C++ to Slicc Interface - @note: What do each of these files do/define???\n src/mem/protocol/RubySlicc_interaces.sm  RubySlicc_Exports.sm RubySlicc_Defines.sm RubySlicc_Profiler.sm RubySlicc_Types.sm RubySlicc_MemControl.sm RubySlicc_ComponentMapping.sm   Variable Assignments\n Use the := operator to assign members in class (e.g. a member defined in RubySlicc_Types.sm):  an automatic \u0026ldquo;m_\u0026rdquo; is added to the name mentioned in the SLICC file.   See Also GEMS-gem5 SLICC Transition Guide\n"
},
{
	"uri": "http://localhost/ruby/simple/",
	"title": "Simple",
	"tags": [],
	"description": "",
	"content": " More details of the gem5 Ruby Interconnection Network are here.\nSimple Network The default network model in Ruby is the simple network.\n Related Files:  src/mem/ruby/network/Network.py src/mem/ruby/network/simple src/mem/ruby/network/simple/SimpleNetwork.py   Configuration Simple network uses the generic network parameters in Network.py:\n - number_of_virtual_networks: This is the maximum number of virtual networks. The actual number of active virtual networks is determined by the protocol.  control_msg_size: The size of control messages in bytes. Default is 8. m_data_msg_size in Network.cc is set to the block size in bytes + control_msg_size.   Additional parameters are specified in simple/SimpleNetwork.py:\n - buffer_size: Size of buffers at each switch input and output ports. A value of 0 implies infinite buffering.  endpoint_bandwidth: Bandwidth at the end points of the network in 1000th of byte. adaptive_routing: This enables adaptive routing based on occupancy of output buffers.   Switch Model The simple network models hop-by-hop network traversal, but abstracts out detailed modeling within the switches. The switches are modeled in simple/PerfectSwitch.cc while the links are modeled in simple/Throttle.cc. The flow-control is implemented by monitoring the available buffers and available bandwidth in output links before sending.\n"
},
{
	"uri": "http://localhost/developer/simple-thread/",
	"title": "SimpleThread",
	"tags": [],
	"description": "",
	"content": "The SimpleThread class derives from the ThreadState class, and is used to provide all architectural state for models that don\u0026rsquo;t need anything more complex. It is basically the context of a hardware thread, and provides all necessary functions needed to access it as defined by the ThreadContext class. However, SimpleThread does not actually derive from ThreadContext, but rather uses a proxy class to forward all ThreadContext function calls to its own functions. This allows the CPU models to use SimpleThread without paying for virtual function overhead.\nThe SimpleThread class has:\n An architected register file, including the PC, next PC, integer, FP, and miscellaneous registers A pointer to the thread\u0026rsquo;s CPU A pointer to the ITB, the DTB A pointer to the system A pointer to the thread\u0026rsquo;s ThreadContext proxy All state from ThreadState, such as kernel statistics and memory ports  "
},
{
	"uri": "http://localhost/docs/scripts/",
	"title": "Simulation Scripts",
	"tags": [],
	"description": "",
	"content": " Simulation scripts control the configuration and execution of gem5 simulations. The gem5 simulator itself is basically passive; on invoking gem5, it simply executes the user\u0026rsquo;s simulation script, and performs actions only when called by the script.\nSimulation scripts are written in Python and executed by the Python interpreter. Currently, the interpreter is linked into the gem5 executable, but for most purposes the script\u0026rsquo;s execution should be indistinguishable from invoking the Python interpreter directly.\nA typical simulation script has two phases: a configuration phase, where the target system is specified by constructing and interconnecting a hierarchy of Python simulation objects; and a simulation phase, where the actual simulation takes place. Simulation scripts can also define command-line options which allow users to control either or both of these phases.\nConfiguration The simulated system is built from a collection of simulator objects, or SimObjects. The script describes the simulated system by describing the SimObjects to be instantiated, their parameters, and their relationships. Specifically, as the program contained in the script file executes, it should create a hierarchy of Python objects that mirror the SimObjects to be created for the simulation. The easiest way to get started is to use an existing script file. Several examples are provided in the configs directory.\nPython classes gem5 provides a collection of Python object classes that correspond to its C++ simulation object classes. These Python classes are defined in a Python module called \u0026ldquo;m5.objects\u0026rdquo;. The Python class definitions for these objects can be found in .py files in src, typically in the same directory as their C++ definitions.\nThe first step in specifying a SimObject is to instantiate a Python object of the corresponding class. To make the Python classes visible, the configuration file must first import the class definitions from the m5 module as follows:\nfrom m5.objects import *  A Python object is instantiated by writing the class name (which, by our convention, starts with an uppercase letter) followed by a pair of parentheses. Thus the following code instantiates a SimpleCPU object and assigns it to the Python variable cpu:\ncpu = SimpleCPU()  SimObject parameters are specified using Python attributes (Python terminology for object fields or members). These attributes can be set at any time using direct Python assignments or at instantiation time using keywords inside the parentheses. The following instantiation:\ncpu = SimpleCPU(clock = '2GHz', width = 2)  is thus equivalent to:\ncpu = SimpleCPU() cpu.clock = '2GHz' cpu.width = 2  Parameter assignments are partially validated at the time the assignment is performed. The attribute name (e.g., clock) must be a defined parameter for the SimObject class, and the right-hand side of the assignment must be of (or convertible to) the correct type for that parameter. The m5 module defines a large number of domain-specific string-to-value conversions, allowing expressions such as \u0026lsquo;2GHz\u0026rsquo; and \u0026lsquo;64KB\u0026rsquo; for clock rates and memory sizes, respectively. A complete list of valid parameter types and units can be found here: Python Parameter Types.\nThe complete list of parameters for a given SimObject class (along with their types, default values, and brief descriptions) can be found by looking at its Python class definition located somewhere in the src directory. An attribute labeled Param.X defines a parameter of type X, while an attribute labeled VectorParam.X defines a parameter requiring a vector (Python list) whose elements must be of type X. Note that parameters are inherited: for example, the clock parameter for the SimpleCPU object above is not specified in the SimpleCPU class definition in SimpleCPU.py, but is inherited from SimpleCPU\u0026rsquo;s Python base class BaseCPU (specified in BaseCPU.py).\nConnections among SimObjects are formed by using a reference to one SimObject as a parameter value in the construction of a second SimObject. For example, a CPU\u0026rsquo;s instruction and data caches are specified by naming cache SimObjects as the values of the CPU\u0026rsquo;s icache and dcache parameters, respectively.\nThe configuration hierarchy To simplify the description of large systems, the overall simulation target specification is organized as a hierarchy (tree). Each node in the tree is a SimObject instance. Even if a SimObject is instantiated in Python, it will not be constructed for the simulation unless it is part of this hierarchy. The program must create a special object root of class Root to identify the root of the hierarchy. When the configuration program completes execution, the tree rooted at root is walked recursively to identify objects to construct. Children are added to SimObjects using the same syntax as setting parameters, i.e., by assigning to Python object attributes. The SimpleCPU object created above can be instantiated by making it a child of the root object as follows:\nroot = Root() root.cpu = cpu  As with parameters, children can be assigned at instantiation time using keyword assignment within parentheses. As a result, the instantiation of the CPU and attaching it to the root node can be done in a single line:\nroot = Root(cpu = SimpleCPU(clock = '2GHz', width = 2))  SimObjects may also become children when they are assigned to a parameter of another SimObject. For example, creating a cache object and assigning it to a CPU\u0026rsquo;s icache or dcache parameter makes the cache object a child of the CPU object in the configuration hierarchy. This effect only occurs for SimObjects that are not in the hierarchy; a SimObject that is already part of the hierarchy is not re-parented when it is assigned to another SimObject\u0026rsquo;s parameter.\nThe configuration hierarchy determines the final name of each instantiated object. The name is formed from the path from root to the particular object (not including root itself), joining elements with \u0026lsquo;.\u0026rsquo;. For example, consider the following configuration:\nmy_cpu = SimpleCPU(clock = '2GHz', width = 2) my_cpu.icache = BaseCache(size = '32KB', assoc = 2) my_cpu.dcache = BaseCache(size = '64KB', assoc = 2) my_system = LinuxSystem(cpu = my_cpu) root = Root(system = my_system)  In this case, the resulting SimObjects will have the internal names system, system.cpu, system.cpu.icache, and system.cpu.dcache. These names will be used in statistics output, etc. The names my_cpu and my_system are simply Python variables; they can be used within Python to set attributes, add children, etc., but they are not visible to the C++ portion of the simulation. Note that a Python object can be accessed using its configuration hierarchy path from within Python by prepending root. .\nA child attribute can also accept a vector of SimObjects. As with vector-valued parameters, these vectors are expressed as Python lists, for example, system.cpu = [ SimpleCPU(), SimpleCPU() ]. In Python, these objects can be accessed using standard list index notation (e.g., system.cpu[0]). The internal names for the objects are formed by directly appending the index to the attribute name (e.g., system.cpu0).\nIn detail, the semantics of assigning to SimObject attributes are as follows:\nIf the attribute name identifies one of the SimObject\u0026rsquo;s formal parameters, then the value on the right-hand side is converted to the parameter\u0026rsquo;s type. An error is raised if this conversion cannot be performed. If the value is a SimObject that is not associated with the configuration hierarchy, that SimObject also becomes a child of the SimObject whose attribute is being assigned. The parameter name is used as the final element in the assigned SimObject\u0026rsquo;s name.\nIf the attribute name does not correspond to a formal parameter and the right-hand value is a SimObject or a list of SimObjects, those SimObject(s) become children of the SimObject whose attribute is being assigned. The attribute name is used as the final element in the assigned SimObject\u0026rsquo;s name.\nIf the attribute name does not correspond to a formal parameter and the right-hand value is not a SimObject, an error is raised.\nInheritance and late binding SimObject instances inherit both parameters and values from the classes they instantiate. A key feature of the configuration system is that value inheritance is largely late binding, that is, values are propagated to instances when the hierarchy is instantiated, not when the instance is created. As a result, a value can be set on a class parameter after instances have been created, and the instances will receive the more recent parameter value (as long as the parameter has not been explicitly overridden on those instances). The following example demonstrates this behavior:\n# Instantiate some CPUs. scpu1 = SimpleCPU() scpu2 = SimpleCPU() fcpu1 = FullCPU() fcpu2 = FullCPU() # Since BaseCPU is a common base class for SimpleCPU and FullCPU, the # following statement will cause all of the above CPUs to have a clock # rate of 1GHz. BaseCPU.clock = '1GHz' # The following statement sets the width of both scpu1 and scpu2 to 4. SimpleCPU.width = 4 # We can override the clock rate for a specific CPU. Note that this # assignment will have the same effect whether it is before or after # the BaseCPU.clock assignment above. fcpu2.clock = '2GHz'  Subclassing Users can define new SimObject classes by deriving from existing gem5 classes. This feature can be useful for providing classes with differing sets of parameter values. These subclasses are defined using standard Python class syntax:\nclass CrazyFastCPU(FullCPU): rob_size = 10000 width = 100 clock = '10GHz'  Users can also subclass or instantiate the SimObject class directly, e.g., obj = SimObject(). These Python objects will not generate C++ SimObjects, but can be assigned children. They can be useful to create internal nodes in the configuration hierarchy that represent collections of SimObjects but do not correspond to C++ SimObjects themselves.\nRelative references In many situations, SimObject parameters have obvious default values that cannot be explicitly named in the general case. For example, many I/O devices need a pointer to the enclosing system\u0026rsquo;s physical memory object or to the enclosing system object itself. Similarly, a cache\u0026rsquo;s default latency might be expressed most conveniently in terms of the clock period of the attached CPU. However, the path names of those objects will vary from configuration to configuration. gem5\u0026rsquo;s configuration system solves this problem by providing relative reference objects. These are \u0026ldquo;proxy\u0026rdquo; objects that stand in for real objects and are resolved only after the entire hierarchy is constructed.\nThe m5 module provides two relative reference objects: Self and Parent. An attribute reference relative to Self resolves to the referencing object, while Parent is resolved by iteratively traversing up the hierarchy (towards root), starting at the parent of the referencing object, until a suitable match is found. A key feature of these objects is that resolution is relative to the final referencing object instance, not where the assignment is performed. Thus they can be assigned as default values to parameters in a SimObject class definition, and will be resolved independently for each instance that derives from that class.\nFor example, it is convenient to set the default clock speed of a CPU object to be the clock speed of the enclosing system; thus in a homogenous multiprocessor there is no need to explicitly set the clock rate on each CPU. We achieve this by setting the default value for the CPU\u0026rsquo;s clock parameter to be Parent.clock. During the final instantiation phase, an access to the CPU\u0026rsquo;s clock parameter will be resolved by iterating up the hierarchy, starting at the CPU\u0026rsquo;s parent, until an object with a clock parameter is found. The value of this parameter (which will be recursively resolved if it is also a relative reference) will be assigned to the CPU\u0026rsquo;s clock parameter.\nFor further flexibility, Self and Parent can take a special attribute, any, which instructs the resolution mechanism to find any value of the appropriate type, either a hierarchy node itself or a parameter of a node. The most common usage of this feature is to use Parent.any for a SimObject-valued parameter. For example, many devices use Parent.any as a default value to locate the enclosing system object or its physical memory object. To avoid ambiguity, an error will be raised if Parent.any could resolve to multiple values at the same level of the hierarchy.\nSimulation Once the simulation script has created the desired configuration in Python, it can move ahead with the actual simulation. At this point, the Python script must start interacting with gem5\u0026rsquo;s C++ simulation core. These interactions occur via Python function calls provided by the m5 module that get translated into C++ function calls. In order to access these functions, the simulation script must import the m5 package as follows:\nimport m5\nThis line is typically placed at the top of the script, along with the \u0026ldquo;from m5.objects import *\u0026rdquo; line mentioned above.\nInstantiating the C++ object hierarchy is as simple as calling the instantiate() function and passing it the root object of the hierarchy, by convention called \u0026lsquo;root\u0026lsquo;:\nm5.instantiate(root)\nOnce the C++ object hierarchy has been instantiated, actual simulation can begin. The simulate() function invokes the C++ event loop. By default, this function will simulate forever, or until some other factor causes the simulation loop to exit (such as the target program calling exit() or a CPU reaching the max_insts_any_thread limit). If the simulate function is passed a positive integer argument, it will simulate at most that number of additional ticks, but may exit sooner if another cause arises first. In any case, the simulate() function will return an event object that represents the reason for exiting. The object can be queried via its getCause() method for a string explaining that reason. A very simple yet user-friendly simulation script may end in the following two lines:\nexit_event = m5.simulate() print 'Exiting @ tick', m5.curTick(), 'because', exit_event.getCause()\nThis example also uses the m5 function curTick(), which returns the current simulation tick value (the value of the C++ variable curTick).\nThe simulation loop can be called multiple times, each time simulating until some reason causes a return to Python. For example, a script could extend the previous example to print a progress indication every million ticks as follows:\nwhile 1:  exit_event = m5.simulate(1000000)  if exit_event != 'simulate() limit reached':  break  print 'Simulation reached tick', m5.curTick() print 'Exiting @ tick', m5.curTick(), 'because', exit_event.getCause()\nOptions The options given on the command line after the script name (see Running gem5) are passed to the simulation script in the same manner that command-line arguments are passed to standard Python scripts (i.e. via sys.argv). These options allow a single script to be configurable in user-defined ways. Our example scripts in configs/example use script options to select the CPU model (simple vs. detailed) used within an otherwise similar configuration.\nBecause options are passed to the script in the standard Python fashion, script files can use standard Python tools to parse options. We generally use the optparse module from the Python standard library.\nHere is an example snippet of a simulation script that does option parsing using optparse, then uses the parsed option flags to select a CPU model:\nparser = optparse.OptionParser() parser.add_option(\u0026quot;-d\u0026quot;, \u0026quot;--detailed\u0026quot;, action=\u0026quot;store_true\u0026quot;) parser.add_option(\u0026quot;-t\u0026quot;, \u0026quot;--timing\u0026quot;, action=\u0026quot;store_true\u0026quot;) (options, args) = parser.parse_args() if options.timing: cpu = TimingSimpleCPU() elif options.detailed: cpu = DetailedO3CPU() else: cpu = AtomicSimpleCPU()  A secondary benefit of using the optparse module is that all available script options can be listed by using the \u0026ldquo;-h\u0026rdquo; flag, e.g.:\ngem5.opt \n\n-h\njust as gem5.opt -h lists all available gem5 options (see Running gem5).\n"
},
{
	"uri": "http://localhost/cpu-models/inorder/todo/",
	"title": "TODO",
	"tags": [],
	"description": "",
	"content": " Python Configurability  Resource Configuration - How can we specify what resources are instantiated via the Python config files?  ResourceType - Type of resource (Enum type)  ResourceParams - Parameters for this type of resource Request - List of requests for this type of resource (Enum type)  Latency - operation latency and issue latency (intra/inter thread)  Count - Number of such resource type     Pipeline Description  InstSchedule - Instruction schedule specified as a vector of InstClassSchedule  InstClassSchedule - Vector of schedules per instruction class - load/store, Int execute, FP execute, specialized inst, etc. (do we still want a distinction between front end and back end schedules?)  ResourceRequestList - Vector of ResourceRequest (per stage?)  ResourceRequest - Vector of requests for resources  ResourceType/Request options   Resources  Execution Unit  Fold Address Generation Unit (AGEN) into the EXE Pass a Function Unit Pool object to the EXE and specify what functions can operate through a specific unit (similar to O3)   Simulation Speed  Instruction Sleeping  Sleep instructions waiting for an long-delay event (instead of constantly ask a resource if it\u0026rsquo;s complete    Event Sleeping  Sleep CPU w/no activity - Implemented on a coarse-grain level, but Activity object can be tuned to be exact.    Resource Pool Instead of broadcasting all events to the resource pool, have resources declare what functions they have virtual functions for (or auto-detect this) and then only broadcast to the right set of resources every time (e.g. the BranchPredictor may not care about a Trap Event)  ISA Support  ALPHA - completed MIPS - completed SPARC - partially completed (not currently being developed) ARM - not completed - Support for Micro-Ops Needed (Template code from Simple or O3 CPU?) X86 - not completed - Support for Micro-Ops Needed (Template code from Simple or O3 CPU?) POWER - not completed  Full System Support  InOrder can boot Linux, but testing for benchmark suites  PARSEC SPLASH2 SPEC2K6   Checkpointing  The serialize/unserialize functions are currently unimplemented in InOrder  SwitchOut  Implement the drain function for InOrder  "
},
{
	"uri": "http://localhost/developer/thread-context/",
	"title": "ThreadContext",
	"tags": [],
	"description": "",
	"content": " ThreadContext is the interface to all state of a thread for anything outside of the CPU. It provides methods to read or write state that might be needed by external objects, such as the PC, next PC, integer and FP registers, and IPRs. It also provides functions to get pointers to important thread-related classes, such as the ITB, DTB, System, kernel statistics, and memory ports. It is an abstract base class; the CPU must create its own ThreadContext by either deriving from it, or using the templated ProxyThreadContext class.\nProxyThreadContext The ProxyThreadContext class provides a way to implement a ThreadContext without having to derive from it. ThreadContext is an abstract class, so anything that derives from it and uses its interface will pay the overhead of virtual function calls. This class is created to enable a user-defined Thread object to be used wherever ThreadContexts are used, without paying the overhead of virtual function calls when it is used by itself. The user-defined object must simply provide all the same functions as the normal ThreadContext, and the ProxyThreadContext will forward all calls to the user-defined object. See the code of SimpleThread for an example of using the ProxyThreadContext.\nDifference vs. ExecContext The ThreadContext is slightly different than the ExecContext. The ThreadContext provides access to an individual thread\u0026rsquo;s state; an ExecContext provides ISA access to the CPU (meaning it is implicitly multithreaded on SMT systems). Additionally the ThreadState is an abstract class that exactly defines the interface; the ExecContext is a more implicit interface that must be implemented so that the ISA can access whatever state it needs. The function calls to access state are slightly different between the two. The ThreadContext provides read/write register methods that take in an architectural register index. The ExecContext provides read/write register methdos that take in a StaticInst and an index, where the index refers to the i\u0026rsquo;th source or destination register of that StaticInst. Additionally the ExecContext provides read and write methods to access memory, while the ThreadContext does not provide any methods to access memory.\n"
},
{
	"uri": "http://localhost/developer/thread-state/",
	"title": "ThreadState",
	"tags": [],
	"description": "",
	"content": "The ThreadState class is used to hold thread state that is common across CPU models, such as the thread ID, thread status, kernel statistics, memory port pointers, and some statistics of number of instructions completed. Each CPU model can derive from ThreadState and build upon it, adding in thread state that is deemed appropriate. An example of this is SimpleThread, where all of the thread\u0026rsquo;s architectural state has been added in. However, it is not necessary (or even feasible in some cases) for all of the thread\u0026rsquo;s state to be centrally located in a ThreadState derived class. The DetailedCPU keeps register values and rename maps in its own classes outside of ThreadState. ThreadState is only used to provide a more convenient way to centrally locate some state, and provide sharing across CPU models.\n"
},
{
	"uri": "http://localhost/cpu-models/inorder/tutorial/",
	"title": "Tutorial",
	"tags": [],
	"description": "",
	"content": " From Instruction Fetch to Instruction Graduation TBD\n"
},
{
	"uri": "http://localhost/docs/m5ops/",
	"title": "m5ops",
	"tags": [],
	"description": "",
	"content": " This page explains the special opcodes that can be used in M5 to do checkpoints etc. The m5 utility program (on our disk image and in util/m5/*) provides some of this functionality on the command line. In many cases it is best to insert the operation directly in the source code of your application of interest. You should be able to link with the appropriate m5op_ARCH.o file and the m5op.h header file has prototypes for all the functions.\nThe m5 Utility (FS mode) The m5 utility (see util/m5/) can be used in FS mode to issue special instructions to trigger simulation specific functionality. It currently offers the following options:\n ivlb: Deprecated, present only for old binary compatibility ivle: Deprecated, present only for old binary compatibility initparam: Deprecated, present only for old binary compatibility sw99param: Deprecated, present only for old binary compatibility exit [delay]: Stop the simulation in delay nanoseconds. resetstats [delay [period]]: Reset simulation statistics in delay nanoseconds; repeat this every period nanoseconds. dumpstats [delay [period]]: Save simulation statistics to a file in delay nanoseconds; repeat this every period nanoseconds. dumpresetstats [delay [period]]: same as dumpstats; resetstats checkpoint [delay [period]]: Create a checkpoint in delay nanoseconds; repeat this every period nanoseconds. readfile: Print the file specified by the config parameter system.readfile. This is how the the rcS files are copied into the simulation environment. debugbreak: Call debug_break() in the simulator (causes simulator to get SIGTRAP signal, useful if debugging with GDB). switchcpu: Cause an exit event of type, \u0026ldquo;switch cpu,\u0026rdquo; allowing the Python to switch to a different CPU model if desired.  Other M5 ops These are other M5 ops that aren\u0026rsquo;t useful in command line form.\n quiesce: De-schedule the CPUs tick() call until an some asynchronous event wakes it (an interrupt) quiesceNS: Same as above, but automatically wakes after a number of nanoseconds if it\u0026rsquo;s not woken up prior quiesceCycles: Same as above but with CPU cycles instead of nanoseconds quisceTIme: The amount of time the CPU was quiesced for addsymbol: Add a symbol to the simulators symbol table. For example when a kernel module is loaded  Using gem5 ops in Java code These ops can also be used in Java code. These ops allow gem5 ops to be called from within java programs like the following:\nimport jni.gem5Op;\npublic class HelloWorld {\n public static void main(String[] args) {  gem5Op gem5 = new gem5Op();  System.out.println(\u0026quot;Rpns0:\u0026quot; + gem5.rpns());  System.out.println(\u0026quot;Rpns1:\u0026quot; + gem5.rpns());  }\n static {  System.loadLibrary(\u0026quot;gem5OpJni\u0026quot;);  } }\nWhen building you need to make sure classpath include gem5OpJni.jar:\njavac -classpath $CLASSPATH:/path/to/gem5OpJni.jar HelloWorld.java\nand when running you need to make sure both the java and library path are set:\njava -classpath $CLASSPATH:/path/to/gem5OpJni.jar -Djava.library.path=/path/to/libgem5OpJni.so HelloWorld\nUsing gem5 ops with Fortran code gem5\u0026rsquo;s special opcodes (psuedo instructions) can be used with Fortran programs. In the Fortran code, one can add calls to C functions that invoke the special opcode. While creating the final binary, compile the object files for the Fortran program and the C program (for opcodes) together. I found the documentation provided here useful. Read the section \u0026ndash; Compiling a mixed C-Fortran program.\n"
},
{
	"uri": "http://localhost/docs/m5term/",
	"title": "m5term",
	"tags": [],
	"description": "",
	"content": "The m5term program allows the user to connect to the simulated console interface that full-system m5 provides. Simply change into the util/term directory and build m5term:\n % cd m5/util/term % make gcc -o m5term term.c % make install sudo install -o root -m 555 m5term /usr/local/bin  The usage of m5term is:\n ./m5term \u0026lt;host\u0026gt; \u0026lt;port\u0026gt; \u0026lt;host\u0026gt; is the host that is running m5 \u0026lt;port\u0026gt; is the console port to connect to. m5 defaults to using port 3456, but if the port is used, it will try the next higher port until it finds one available. If there are multiple systems running within one simulation, there will be a console for each one. (The first system's console will be on 3456 and the second on 3457 for example) m5term uses '~' as an escape character. If you enter the escape character followed by a '.', the m5term program will exit.  m5term can be used to interactively work with the simulator, though users must often set various terminal settings to get things to work\nA slightly shortened example of m5term in action:\n % m5term localhost 3456 ==== m5 slave console: Console 0 ==== M5 console Got Configuration 127 memsize 8000000 pages 4000 First free page after ROM 0xFFFFFC0000018000 HWRPB 0xFFFFFC0000018000 l1pt 0xFFFFFC0000040000 l2pt 0xFFFFFC0000042000 l3pt_rpb 0xFFFFFC0000044000 l3pt_kernel 0xFFFFFC0000048000 l2reserv 0xFFFFFC0000046000 CPU Clock at 2000 MHz IntrClockFrequency=1024 Booting with 1 processor(s) ... ... VFS: Mounted root (ext2 filesystem) readonly. Freeing unused kernel memory: 480k freed init started: BusyBox v1.00-rc2 (2004.11.18-16:22+0000) multi-call binary PTXdist-0.7.0 (2004-11-18T11:23:40-0500) mounting filesystems... EXT2-fs warning: checktime reached, running e2fsck is recommended loading script... Script from M5 readfile is empty, starting bash shell... # ls benchmarks etc lib mnt sbin usr bin floppy lost+found modules sys var dev home man proc tmp z #  "
},
{
	"uri": "http://localhost/tutorials/asplos-2017/",
	"title": "ASPLOS 2017",
	"tags": [],
	"description": "",
	"content": " Architectural Exploration with gem5\nAbstract This tutorial will give a brief introduction to gem5 for computer engineers who are new to gem5. The attendees will learn what gem5 can and can not do, how to use and extend gem5, as well as how to contribute back to gem5.\nTarget Audience The primary audience is junior computer architecture engineers (e.g., first or second year graduate students, as well as junior engineers) who are planning on using gem5 for future architecture research. We also invite others who want a high-level idea of how gem5 works and its applicability to architecture research.\nThe tutorial is free to attend (no registration fee required), registration is required via ASPLOS.\nPrerequisites: Attendees are expected to have a working knowledge of C++, Python, and computer systems.\nSlides The slides from the tutorial can be downloaded here.\nSchedule The tutorial is scheduled on the Sunday afternoon 9th April 2017 at The Westin Xi\u0026rsquo;an hotel.\n   Topic Time     Introduction 13:00-13:10   Getting started with gem5 13:10-13:30   Advanced configurations 13:30-13:55   Debug \u0026amp; Trace 13:55-14:05   Creating SimObjects 14:05-14:30       Break 14:30-15:00   Introduction to memory subsystems 15:00-15:45   Introduction to CPU models 15:45-16:10   Advanced gem5 features and capabilities 16:10-16:40   How to contribute to gem5 16:40-17:00    Presenters This tutorial is organised by Andreas Sandberg, Stephan Diestelhorst and William Wang of ARM Research\n"
},
{
	"uri": "http://localhost/benchmarks/android-kitkat/",
	"title": "Android KitKat",
	"tags": [],
	"description": "",
	"content": " Building Android KitKat for gem5 Overview The easiest way to build Android for gem5 is to base the system on a the emulated Goldfish device. The main difference between the actual qemu-based Goldfish model and gem5 is a small number of gem5-specific configuration files. These files are mainly related to differences in block device naming and scripts to start use gem5\u0026rsquo;s pseudo-op interface to start experiments.\nBuild Instructions Follow the directions on Android Open Source to download and build Android. Make sure to create a local mirror (see the download page) to speed up things if you ever need to create a new working directory. The following instructions are based on our experience from bringing up Android 4.4.4r2 (KitKat).\nMake sure to quickly read the AOSP build instructions. In particular the section about Java dependencies. KitKat and earlier all depend on particular Sun/Oracle JDK versions. Lollipop and newer seem to be able to build with OpenJDK.\nIf you are building on a dedicated build machine or virtual machine, I\u0026rsquo;d recommend making sure OpenJDK is not installed at all on your machine for building KitKat. Sometimes having both versions can cause issues where some tools are invoked from OpenJDK while others get invoked from the Oracle JDK.\nEither way, make sure that the Oracle bin directory is first in your PATH (and make sure it doesn\u0026rsquo;t have a trailing slash):\nexport PATH=/path/to/java-6-jdk/bin:$PATH\nWhen setting up your build directories, I\u0026rsquo;d suggest a structure as follows:\n /work/android (or some other directory with plenty of free space)  .../repos/aosp-mirro - Mirror of the upstream Android repository .../gem5kitkat/  .../aosp - Clone of the Android repository    Before you start the AOSP build, you will need to make one change to the build system to enable building libion.so, which is used by the Mali driver. Edit the file aosp/system/core/libion/Android.mk to change LOCAL_MODULE_TAGS for libion from \u0026lsquo;optional\u0026rsquo; to \u0026lsquo;debug\u0026rsquo;. Here is the output of repo diff:\n project system/core/  diff --git a/libion/Android.mk b/libion/Android.mk  index 5121fee..1a0af0e 100644  --- a/libion/Android.mk  +++ b/libion/Android.mk  @@ -3,7 +3,7 @@ LOCAL_PATH:= $(call my-dir)  include $(CLEAR_VARS)  LOCAL_SRC_FILES := ion.c  LOCAL_MODULE := libion  -LOCAL_MODULE_TAGS := optional  +LOCAL_MODULE_TAGS := debug  LOCAL_SHARED_LIBRARIES := liblog  include $(BUILD_SHARED_LIBRARY)\nBuild a vanilla AOSP KitKat distribution using the following command:\n. build/envsetup.sh lunch aosp_arm-userdebug make -j8\nThis builds a plain Android for the Goldfish device (an Android specific qemu version). We are going to use this as a base for our gem5 distribution.\nUseful commands:\n         hmm Show a list of build system commands   mm Build the Android module in the current directory   mma Build the Android module in the current directory and its dependencies   emulator Launch Android in qemu    The mm command is especially useful since just running make in a directory with an existing build of Android (i.e., make doesn\u0026rsquo;t need to build anything) can take several minutes.\nPreparing a filesystem for gem5 First, create an empty disk (this example creates a 2GiB image) image:\ndd if=/dev/zero of=myimage.img bs=1M count=2048\nAs root, hook up the disk image to a loopback device (the following assumes /dev/loop0 is free).\nlosetup /dev/loop0 myimage.img\nUsing fdisk, create the following partitions:\n   Part. No Usage Approximate Size     1 / 500MB   2 /data 1GB   3 /cache 500MB    Use common sense when setting up the partitions. The root partition will contain both Android\u0026rsquo;s root file system and the system file system and should be big enough for both of them. The data partition will contain any apps that are not a part of the system (i.e., anything you install).\nAs root, tell the kernel about the partitions and format them to ext4:\npartprobe /dev/loop0 mkfs.ext4 -L AndroidRoot /dev/loop0p1 mkfs.ext4 -L AndroidData /dev/loop0p2 mkfs.ext4 -L AndroidCache /dev/loop0p3\nMount the filesystem and extract the root file system:\nmkdir -p /mnt/android mount /dev/loop0p1 /mnt/android cd /mnt/android zcat AOSP/out/target/product/generic/ramdisk.img | cpio -i mkdir cache\nmkdir -p /mnt/tmp mount -oro,loop AOSP/out/target/product/generic/system.img /mnt/tmp cp -a /mnt/tmp/* system/\nAt this point, the new disk image is a (mostly) vanilla Goldfish image. Add the gem5 pixie dust by copying all the files in the attached tar-ball into the new root file system and adding an m5 binary (see util/m5 in your gem5 work directory) to /sbin. This directory contains a gem5-specific init.rc that is based on the original Goldfish device, with additional tweaks. Specifically, it runs /gem5/postboot.sh when Android has booted. This script is responsible for disabling the screen lock and downloading and executing a run script from gem5. Double check to make sure that both the init.gem5.rc and postboot.sh file have the executable permission set.\nAt this point, everything should just work. Unmount everything and disconnect the loop back device:\numount /mnt/android losetup -d /dev/loop0\nAndroid systems generally does a lot of initialization (JIT compilation etc.) on the first boot. Since gem5 normally mounts the root file system as CoW and stores the file system differences in memory. To speed up future experiments, make sure to follow the guide in BBench-gem5 to make these changes permanent.\nBuilding the kernel You will need a recent ARM cross compiler to build the kernel. If you\u0026rsquo;re using Ubuntu 10.04, install it by running:\nsudo apt-get install gcc-arm-linux-gnueabihf\nCheckout the gem5 kernel from the following git repository:\ngit://linux-arm.org/linux-linaro-tracking-gem5.git\nAlternatively, you can use the github mirror of this same repo (See: http://permalink.gmane.org/gmane.comp.emulators.m5.devel/27631):\ngit clone https://github.com/gem5/linux-arm-gem5.git\nIf you are not planning on doing kernel development, you can add the option --depth 1 to significantly speed up your clone.\nConfigure and build the kernel using:\nmake CROSS_COMPILE=arm-linux-gnueabihf- ARCH=arm vexpress_gem5_defconfig make CROSS_COMPILE=arm-linux-gnueabihf- ARCH=arm vmlinux -j8\nNote: if you want to use Workload Automation, this may be a good time to add the extra kernel options to your .config file. See WA-gem5 for a description and which kernel options should be enabled.\nAndroid Runtime Configuration Simulation-specific kernel boot options options:\n   Kernel Option Function     qemu=1 Enable emulation support   qemu.gles=1 Don\u0026rsquo;t use software rendering (usually enables GLES pass through)   androidboot.hardware=NAME Overrides the device\u0026rsquo;s platform name (used to select configuration files at boot)        A simulated system should normally set qemu=1 on the kernel command line. This enables software rendering and some emulation specific services. In order to load the right boot configuration, set androidboot.hardware to gem5.\nKernel command line to properties mapping:\n   Kernel Option Android Property     androidboot.* ro.boot.*   androidboot.serialno ro.serialno   androidboot.mode ro.bootmode   androidboot.baseband ro.baseband   androidboot.bootloader ro.bootloader   androidboot.hardware ro.hardware \u0026amp; ro.boot.hardware   * ro.kernel.* if running in emulation mode    \\== Emulation Mode Specifics (qemu=1) ==\nOpenGL \u0026amp; Graphics  Setting the ro.kernel.qemu property forces libGLES_android to be used unless ro.kernel.qemu.gles is also set.   qemu.sf.lcd_density instead of ro.sf.lcd_density is used to specify display density.  Resources kitkat-overlay.tar.bz2\nRunning workloads with Kitkat To run workloads we advise the use of Workload Automation. For detailed instructions on how to use this see: WA-gem5\nNoMali To use a NoMali model in gem5, you need to perform the following steps:\n Add it to the configuration file Add the user-space drivers to your Android image Add the kernel-side driver to your kernel Inform the kernel of the device by adding it to the device tree  These steps are described in detail in the sections below.\nOnce you have completed the steps to build the kernel-side driver and modified the DTB, test the kernel and DTB by starting it in gem5. Ensure that you get the following message from the kernel:\nmali 2d000000.gpu: Continuing without Mali clock control mali 2d000000.gpu: GPU identified as 0x0750 r0p0 status 1 mali 2d000000.gpu: Probed as mali0\nOnce the user-space driver has been added, start Android and check the output of the logcat command. You should see something like this when surfaceflinger starts:\nI/[Gralloc]( 885): using (fd=14) I/[Gralloc]( 885): id = hdlcd I/[Gralloc]( 885): xres = 1920 px I/[Gralloc]( 885): yres = 1080 px I/[Gralloc]( 885): xres_virtual = 1920 px I/[Gralloc]( 885): yres_virtual = 1080 px I/[Gralloc]( 885): bpp = 16 I/[Gralloc]( 885): r = 11:5 I/[Gralloc]( 885): g = 5:6 I/[Gralloc]( 885): b = 0:5 I/[Gralloc]( 885): width = 305 mm (159.895081 dpi) I/[Gralloc]( 885): height = 171 mm (160.421051 dpi) I/[Gralloc]( 885): refresh rate = 59.28 Hz E/SurfaceFlinger( 885): hwcomposer module not found W/SurfaceFlinger( 885): getting VSYNC period from fb HAL: 16868241 W/SurfaceFlinger( 885): no suitable EGLConfig found, trying a simpler query I/SurfaceFlinger( 885): EGL informations: I/SurfaceFlinger( 885): vendor : Android I/SurfaceFlinger( 885): version : 1.4 Android META-EGL I/SurfaceFlinger( 885): extensions: EGL_KHR_get_all_proc_addresses EGL_ANDROID_presentation_time EGL_KHR_image EGL_KHR_image_base EGL_KHR_gl_texture_2D_image EGL_KHR_gl_texture_cubemap_image EGL_KHR_gl_renderbuffer_image EGL_KHR_fence_sync EGL_KHR_create_context EGL_EXT_create_context_robustness EGL_ANDROID_image_native_buffer EGL_KHR_wait_sync EGL_ANDROID_recordable  I/SurfaceFlinger( 885): Client API: OpenGL_ES I/SurfaceFlinger( 885): EGLSurface: 5-6-5-0, config=0x78f1f4e0 I/SurfaceFlinger( 885): OpenGL ES informations: I/SurfaceFlinger( 885): vendor : ARM I/SurfaceFlinger( 885): renderer : Mali-T760 I/SurfaceFlinger( 885): version : OpenGL ES 3.1 v1.r8p0-02rel0.785fbb28b57044ab28a90a55a7b06f3f\nConfiguration Changes The NoMali GPU is not instantiated by default in any of the example configurations. To instantiate it, add the following code to your configuration script:\nmy_system.gpu = NoMaliGpu(  gpu_type=\u0026quot;T760\u0026quot;,  ver_maj=0, ver_min=0, ver_status=1,  int_job=114, int_mmu=115, int_gpu=116,  pio_addr=0x2d000000,  pio=membus.master)\nThe code above instantiates a NoMali model of a Mali T760 r0p0-1 at address 0x2d000000. The GPU type and revision must match the driver.\nAndroid User Space Drivers Download the Mali drivers for gem5 from the ARM Mali Midgard User Space Drivers section on MaliDeveloper. Make sure that you download a driver that matches your Android version. Make a note of the driver version number, you\u0026rsquo;ll need a matching kernel-side driver of the same version.\nThe downloaded file will contain at least the following files:\n lib/egl/libGLES_mali.so lib/hw/gralloc.default.so  Copy the entire lib directory into /system/vendor in the target system. The resulting disk image should contain (at least) the following files:\n /system/vendor/lib/egl/libGLES_mali.so /system/vendor/lib/hw/gralloc.default.so  Make sure that everyone can access the files (i.e., files have permission 0644 and directories 0755), or you\u0026rsquo;ll end up with permission errors when booting the system.\nKernel Drivers Download the Mali kernel-side drivers from the ARM Mali Midgard Kernel Drivers section on MaliDeveloper. Make sure that you download a driver version that matches your user space driver version.\nCopy the directory driver/product/kernel/drivers/gpu/arm into drivers/gpu/ in your target kernel. You now need to wire up the driver to the kernel\u0026rsquo;s build system. Do that by adding arm/ to the list of subdirectories in drivers/gpu/Makefile. To add it to the configuration system, add source \u0026quot;drivers/gpu/arm/Kconfig\u0026quot; to drivers/video/Kconfig.\nEnable the driver in the kernel configuration (set MALI_MIDGARD / Device Drivers -\u0026gt; Graphics support -\u0026gt; ARM GPU Configuration -\u0026gt; Mali Midgard series support). The default options should work and enable device tree support. Do not set the NoMali option in the kernel.\nYou will also need to enable the Mali DDK sysfs configuration (set MALI_MIDGARD_DEBUG_SYS / Device Drivers -\u0026gt; Graphics support -\u0026gt; ARM GPU Configuration -\u0026gt; Mali Midgard series support -\u0026gt; Enable sysfs for the Mali Midgard DDK).\nYou will also need to enable the kernel config option CONFIG_ION_DUMMY=y / Device Drivers -\u0026gt; Staging drivers -\u0026gt; Android -\u0026gt; Android Drivers -\u0026gt; Ion Memory Manager -\u0026gt; Dummy Ion driver. This will create the /dev/ion device node that the Mali driver will use.\nIn short, you need to set following configurations in your .config file.\nCONFIG_MALI_MIDGARD=y CONFIG_MALI_MIDGARD_DEBUG_SYS=y CONFIG_ION=y CONFIG_ION_DUMMY=y\nDevice Tree Add the following node to your device tree:\ngpu@0x2d000000 {  compatible = \u0026quot;arm,mali-midgard\u0026quot;;  reg = \u0026lt;0 0x2d000000 0 0x4000\u0026gt;;  interrupts = \u0026lt;0 82 4\u0026gt;, \u0026lt;0 83 4\u0026gt;, \u0026lt;0 84 4\u0026gt;;  interrupt-names = \u0026quot;JOB\u0026quot;, \u0026quot;MMU\u0026quot;, \u0026quot;GPU\u0026quot;; };\nNote that there is an offset between interrupts in gem5 and in the device tree. Linux starts counting SPIs from 0, while gem5 starts from 32.\n"
},
{
	"uri": "http://localhost/docs/doxygen/",
	"title": "Doxygen Code Documentation",
	"tags": [],
	"description": "",
	"content": " Introduction Doxygen allows users to quickly create documentation for our code by extracting the relavent information from the code and comments. It is able to document all the code structures including classes, namespaces, files, members, defines, etc. Most of these are quite simple to document, you only need to place a special documentation block before the declaration. The Doxygen documentation within gem5 is processed every night and the following web pages are generated: Doxygen\nUsing Doxygen The special documentation blocks take the form of a javadoc style comment. A javadoc comment is a C style comment with 2 *\u0026rsquo;s at the start, like this:\n/** * ...documentation... */  The intermediate asterisks are optional, but please use them to clearly delineate the documentation comments.\nThe documentation within these blocks is made up of at least a brief description of the documented structure, that can be followed by a more detailed description and other documentation. The brief description is the first sentence of the comment. It ends with a period followed by white space or a new line. For example:\n/** * This is the brief description. This is the start of the detailed * description. Detailed Description continued. */  If you need to have a period in the brief description, follow it with a backslash followed by a space.\n/** * e.g.\\ This is a brief description with an internal period. */  Blank lines within these comments are interpreted as paragraph breaks to help you make the documentation more readble.\nSpecial commands Placing these comments before the declaration works in most cases. For files however, you need to specify that you are documenting the file. To do this you use the @file special command. To document the file that you are currently in you just need to use the command followed by your comments. To comment a separate file (we shouldn\u0026rsquo;t have to do this) you can supply the name directly after the file command. There are some other special commands we will be using quite often. To document functions we will use @param and @return or @retval to document the parameters and the return value. @param takes the name of the paramter and its description. @return just describes the return value, while @retval adds a name to it. To specify pre and post conditions you can use @pre and @post.\nSome other useful commands are @todo and @sa. @todo allows you to place reminders of things to fix/implement and associate them with a specific class or member/function. @sa lets you place references to another piece of documentation (class, member, etc.). This can be useful to provide links to code that would be helpful in understanding the code being documented.\nExample of Simple Documentation Here is a simple header file with doxygen comments added.\n/** * @file * Contains an example of documentation style. */ #include \u0026lt;vector\u0026gt; /** * Adds two numbers together. */ #define DUMMY(a,b) (a+b) /** * A simple class description. This class does really great things in detail. * * @todo Update to new statistics model. */ class foo { /** This variable stores blah, which does foo and has invariants x,y,z @warning never set this to 0 @invariant foo */ int myVar; /** * This function does something. * @param a The number of times to do it. * @param b The thing to do it to. * @return The number of times it was done. * * @sa DUMMY */ int bar(int a, long b); /** * A function that does bar. * @retval true if there is a problem, false otherwise. */ bool manchu(); };  Grouping Doxygen also allows for groups of classes and member (or other groups) to be declared. We can use these to create a listing of all statistics/global variables. Or just to comment about the memory hierarchy as a whole. You define a group using @defgroup and then add to it using @ingroup or @addgroup. For example:\n/** * @defgroup statistics Statistics group */ /** * @defgroup substat1 Statistitics subgroup * @ingroup statistics */ /** * A simple class. */ class foo { /** * Collects data about blah. * @ingroup statistics */ Stat stat1; /** * Collects data about the rate of blah. * @ingroup statistics */ Stat stat2; /** * Collects data about flotsam. * @ingroup statistics */ Stat stat3; /** * Collects data about jetsam. * @ingroup substat1 */ Stat stat4; };  This places stat1-3 in the statistics group and stat4 in the subgroup. There is a shorthand method to place objects in groups. You can use @{ and @} to mark the start and end of group inclusion. The example above can be rewritten as:\n/** * @defgroup statistics Statistics group */ /** * @defgroup substat1 Statistitics subgroup * @ingroup statistics */ /** * A simple class. */ class foo { /** * @ingroup statistics * @{ */ /** Collects data about blah.*/ Stat stat1; /** Collects data about the rate of blah. */ Stat stat2; /** Collects data about flotsam.*/ Stat stat3; /** @} */ /** * Collects data about jetsam. * @ingroup substat1 */ Stat stat4; };  It remains to be seen what groups we can come up with.\nOther features Not sure what other doxygen features we want to use.\n"
},
{
	"uri": "http://localhost/docs/dyn-inst/",
	"title": "DynInst",
	"tags": [],
	"description": "",
	"content": "The DynInst is used to hold dynamic information about instructions. This is necessary for more detailed models or out-of-order models, both of which may need extra information beyond the StaticInst in order to correctly execute instructions.\nSome of the dynamic information that it stores includes:\n The PC of the instruction The renamed register indices of the source and destination registers The predicted next-PC The instruction result The thread number of the instruction The CPU the instruction is executing on Whether or not the instruction is squashed  Additionally the DynInst provides the ExecContext interface. When ISA instructions are executed, the DynInst is passed in as the ExecContext, handling all accesses of the ISA to CPU state.\nDetailed CPU models can derive from DynInst and create their own specific DynInst subclasses that implement any additional state or functions that might be needed. See src/cpu/o3/alpha/dyn_inst.hh for an example of this.\n"
},
{
	"uri": "http://localhost/deprecated/ubuntu-disk/",
	"title": "Full-system Ubuntu Image",
	"tags": [],
	"description": "",
	"content": " This content is out-of-date.\n This page describes how to build a serial-console filesystem of Ubuntu Linux for ARM ISA simulation after the bare image file is created. An example Ubuntu Natty ARM image is available on the Download page.\nUsing Rootstock and Qemu to build the filesystem One way to create a disk image is to use the rootstock tool provided in older versions of Ubuntu to build an ARM filesystem. To create a base 2GB filesystem that can be booted to the serial console, run the following command:\n%\u0026gt;rootstock --fqdn gem5sim --login gem5 --password 5meg --imagesize 2G --seed build-essential\nOther packages can be added to the \u0026ndash;seed option list to be installed by rootstock. Rootstock will create a tar file containing the file system. Unpack this tar file into the blank disk image. Packages can also be installed at a later date by mounting the filesystem to a loop device, mounting its proc filesystem and chroot\u0026rsquo;ing into the filesystem. Qemu will be used to emulate the binaries within the ARM filesystem to install additional packages using apt-get. For the below to work you need to have installed the rootstock packages so the various qemu emulators are available. For example\nAttention: Rootstock has been deprecated by Ubuntu for making ARM disk images from x86 machines. They now offer core file system image tarballs for download.\nUsing Ubuntu CoreFS and Qemu to build an Ubuntu file system for 12.04LTS+ Ubuntu Core offers pre-compiled base file systems in a tarball to use instead of rootstock. Since v12.04 of Ubuntu, cannonical now offers hardfloat (hf) ABI compiled binaries to make better use of VFP and NEON along with the usual softfloat binaries (armel). Make sure to install the qemu-arm-static package to be able to install packages to your file-system after you have unpacked the tarball to a blank disk image. After unpacking the tarball to the blank image copy the qemu-arm-static binary from /usr/bin from your host system to the usr/bin directory of your disk image. This allows ARM emulation for installing packages directly to the disk image, and even for compiling source packages using an ARM version of GCC. Perform the following operations to get a functional Ubuntu file system that can run hardfloat and softfloat binaries.\n   // if you have not mounted the disk image, do so now, make sure to unpack the core  // fs to this disk image before continuing  mount -oloop,offset=32256 /tmp/Ubuntu-arm.img /mnt  cd /mnt   mount -o bind /proc /mnt/proc  mount -o bind /dev /mnt/dev  mount -o bind /sys /mnt/sys  cp /etc/resolv.conf /mnt/etc/  chroot .    // enable the universe repo in etc/apt/sources.list   apt-get update  apt-get install ubuntu-minimal  apt-get install build-essential  apt-get install vim  apt-get install gcc-multilib  apt-get install g++-multilib  apt-get install \u0026lt;what ever other packages you want\u0026gt;  \nIf you transfer the file system created here to another computer make sure resolv.conf in /etc of the created filesystem is updated to match the resolv.conf of the host system, otherwise apt-get will fail to function properly.\nSetting up Upstart to be Gem5 friendly Instead of the old SysV and init.d, Ubuntu uses Upstart for mounting filesystems and loading the various daemons upon boot. To speed up the process in simulation the following upstart scripts should be removed from the /etc/init folder in the new filesystem as they are not needed:\n console.conf console-setup.conf container-detect.conf cron.conf dmesg.conf hwclock.conf hwclock-save.conf mounted-debugfs.conf mounted-tmp.conf mountall-net.conf network-interface-container.conf network-interface-security.conf plymouth.conf plymouth-log.conf plymouth-splash.conf plymouth-stop.conf plymouth-upstart-bridge.conf rsyslog.conf setvtrgb.conf tty[x].conf (only need tty1.conf) udev-fallback-graphics.conf ureadahead.conf ureadahead-other.conf  The following changes should be made to the mountall.conf script:\n Remove the fsck checks at the beginning of the script declaration Remove the fsck checks in the post-script declaration Remove the exec mountall \u0026ndash;daemon declaration so total control of what filesystems are mounted and when is retained  To the script field on the mountall.conf file, add at least the following:\n  # Mount appropriate file systems from fstab and remount root.  mount /proc  mount /tmp  mount /sys  mount -o remount,rw /dev/sda1 /  swapon /swapfile # if present  # Make sure to emit all events that mountall would have  initctl emit virtual-filesystems  initctl emit local-filesystems  initctl emit remote-filesystems  initctl emit all-swaps  initctl emit filesystem  initctl emit mounting  initctl emit mounted  \nAdditionally, one of the tty.conf scripts should be modified like below to get a login prompt within m5term or load an .rcS job script with multi-user and job-control enabled:\n  script  if [ ! -c /dev/ttyAMA0 ]  then  mknod /dev/ttyAMA0 c 204 64  fi  if [ ! -c /dev/ttySA0 ]  then  if [ ! -L /dev/ttySA0 ]  then  ln -s /dev/ttyAMA0 /dev/ttySA0  fi  fi   /sbin/m5 readfile \u0026gt; /tmp/script  chmod 755 /tmp/script  if [ -s /tmp/script ]  then  exec su root -c '/tmp/script' # gives script full privileges as root user in multi-user mode  exit 0  else  exec /sbin/getty -L ttySA0 38400 vt100 # login prompt  fi  end script  \nBy default a modules.dep file will not be created by rootstock, this file is needed to prevent certain upstart scripts from failing. To create this file, add the following to the beginning of the script declaration in module-init-tools.conf, or simply add your own custom modules.dep file in the appropriate directory for your kernel.\n  if [ ! -e /lib/modules/`uname -r`/modules.dep ]  then  mkdir /lib/modules/`uname -r`  echo \u0026quot;#No modules for this run of Gem5\u0026quot; \u0026gt; /lib/modules/`uname -r`/modules.dep  fi  \nTo allow passwordless login to the filesystem, edit the /etc/shadow file to have the gem5 and root entries look like the following.\n   root::15201:0:99999:7:::  gem5::15201:0:99999:7:::  \nEnsure that the tty that will be used as the login tty is contained within the /etc/securetty file. In most cases this will probably be ttyAMA0. Go through the mounted-proc.conf, mounted-dev.conf, mounted.tmp.conf and mounted-varrun.conf and verify all use the clause\n  start on mounted   \nso the scripts start running. Also make sure to remove dependencies on variables that contain the word \u0026ldquo;container\u0026rdquo;. This appears to be an addition for virtualization but is unnecessary for Gem5.\nThe rc-sysinit.conf file should have most of the script body commented out except for these lines:\n  [ -n \u0026quot;${FROM_SINGLE_USER_MODE}\u0026quot; ] || /etc/init.d/rcS  telinit \u0026quot;${DEFAULT_RUNLEVEL}\u0026quot;  \nFinally the /etc/hosts file must contain the following to get proper behavior for programs that use sockets to do RPC:\n  127.0.0.1 localhost  ::1 localhost ip6-localhost ip6-loopback  fe00::0 ip6-localnet  ff00::0 ip6-mcastprefix  ff02::1 ip6-allnodes  ff02::2 ip6-allrouters  ff02::3 ip6-allhosts  \n"
},
{
	"uri": "http://localhost/developer/future-serialization/",
	"title": "Future Serialization",
	"tags": [],
	"description": "",
	"content": "The current serialization system uses a .ini file format. Each section refers to a specific object instance, and the values within a section are the serialized data for that instance. The section heading is the name() of the object and all hierarchy is essentially flattened.\nDuring the serialization process, the file is directly written out as objects are serialized, without any intermediate buffering. The serialization proceeds by doing the global serialization then by serializing each SimObject. Objects that are to be serialized are simply passed the stream to which they are to output their information. This process leads to several problems:\n It is difficult to serialize user types that are not SimObjects.  On a related note, it is difficult to serialize a contained user type while already serializing another object. (Because we\u0026rsquo;re directly writing the stream.)  Alternative serialization formats can\u0026rsquo;t easily be supported Improvements to the framework require changes all over the place.  Here are some proposed solutions.\n General fixes  Keep a list of serializable objects that is Separate from SimObjects. This allows us to create non SimObject serializable objects. We should probably support serializing python objects.    On the output (serialization) side:  The serialize function should simply take a \u0026ldquo;Checkpoint\u0026rdquo; object as an input, not an ostream. The various paramOut type functions should not be templated the way they are. They should be implemented like ostream::operator\u0026lt;\u0026lt; is. i.e. without a required template argument, arrayParamOut should be no different than paramOut, etc. By doing this, paramOut can be easily overloaded for new types and containers. The Checkpoint object should just build a dictionary of a list of tuples, the outer dict is for each object, and the list of tuples are the serialized keys, values. Python should access the dict and write out the data. Support pointers to other sections (for sub objects)  On the input (unserialization) side:  Don\u0026rsquo;t take both a Checkpoint * and a section name. Instead take a CheckpointSection object which is a dict of the serialized data. CheckpointSection should have a Checkpoint * in it. Fix the paramIn functions as I\u0026rsquo;ve described fixing the paramOut functions.   Questions:\n Do we want to keep the .ini format as the default? It could be a bit difficult to support user defined types this way.  "
},
{
	"uri": "http://localhost/developer/isa/",
	"title": "ISA Description",
	"tags": [],
	"description": "",
	"content": " The gem5 ISA description language is a custom language designed specifically for generating the class definitions and decoder function needed by M5. This section provides a practical, informal overview of the language itself. A formal grammar for the language is embedded in the \u0026ldquo;yacc\u0026rdquo; portion of the parser (look for the functions starting with p_ in isa_parser.py). A second major component of the parser processes C-like code specifications to extract instruction characteristics; this aspect is covered in the section Code parsing. At the highest level, an ISA description file is divided into two parts: a declarations section and a decode section. The decode section specifies the structure of the decoder and defines the specific instructions returned by the decoder. The declarations section defines the global information (classes, instruction formats, templates, etc.) required to support the decoder. Because the decode section is the focus of the description file, we will begin the discussion there.\nThe decode section The decode section of the description is a set of nested decode blocks. A decode block specifies a field of a machine instruction to decode and the result to be provided for particular values of that field. A decode block is similar to a C switch statement in both syntax and semantics. In fact, each decode block in the description file generates a switch statement in the resulting decode function. Let\u0026rsquo;s begin with a (slightly oversimplified) example:\ndecode OPCODE { 0: add({{ Rc = Ra + Rb; }}); 1: sub({{ Rc = Ra - Rb; }}); }  A decode block begins with the keyword decode followed by the name of the instruction field to decode. The latter must be defined in the declarations section of the file using a bitfield definition (see #Bitfield definitions). The remainder of the decode block is a list of statements enclosed in braces. The most common statement is an integer constant and a colon followed by an instruction definition. This statement corresponds to a \u0026lsquo;case\u0026rsquo; statement in a C switch (but note that the \u0026lsquo;case\u0026rsquo; keyword is omitted for brevity). A comma-separated list of integer constants may be used to allow a single decode statement to apply to any of a set of bitfield values.\nInstruction definitions are similar in syntax to C function calls, with the instruction mnemonic taking the place of the function name. The comma-separated arguments are used when processing the instruction definition. In the example above, the instruction definitions each take a single argument, a code literal. A code literal is operationally similar to a string constant, but is delimited by double braces ({{ and }}). Code literals may span multiple lines without escaping the end-of-line characters. No backslash escape processing is performed (e.g., \\t is taken literally, and does not produce a tab). The delimiters were chosen so that C-like code contained in a code literal would be formatted nicely by emacs C-mode.\nA decode statement may specify a nested decode block in place of an instruction definition. In this case, if the bitfield specified by the outer block matches the given value(s), the bitfield specified by the inner block is examined and an additional switch is performed.\nIt is also legal, as in C, to use the keyword default in place of an integer constant to define a default action. However, it is more common to use the decode-block default syntax discussed in the section #Decode block defaults below.\nSpecifying instruction formats When the ISA description file is processed, each instruction definition does in fact invoke a function call to generate the appropriate C++ code for the decode file. The function that is invoked is determined by the instruction format. The instruction format determines the number and type of the arguments given to the instruction definition, and how they are processed to generate the corresponding output. Note that the term \u0026ldquo;instruction format\u0026rdquo; as used in this context refers solely to one of these definition-processing functions, and does not necessarily map one-to-one to the machine instruction formats defined by the ISA. The one oversimplification in the previous example is that no instruction format was specified. As a result, the parser does not know how to process the instruction definitions.\nInstruction formats can be specified in two ways. An explicit format specification can be given before the mnemonic, separated by a double colon (::), as follows:\ndecode OPCODE { 0: Integer::add({{ Rc = Ra + Rb; }}); 1: Integer::sub({{ Rc = Ra - Rb; }}); }  In this example, both instruction definitions will be processed using the format Integer. A more common approach specifies the format for a set of definitions using a format block, as follows:\ndecode OPCODE { format Integer { 0: add({{ Rc = Ra + Rb; }}); 1: sub({{ Rc = Ra - Rb; }}); } }  In this example, the format \u0026ldquo;Integer\u0026rdquo; applies to all of the instruction definitions within the inner braces. The two examples are thus functionally equivalent. There are few restrictions on the use of format blocks. A format block may include only a subset of the statements in a decode block. Format blocks and explicit format specifications may be mixed freely, with the latter taking precedence. Format and decode blocks can be nested within each other arbitrarily. Note that a closing brace will always bind with the nearest format or decode block, making it syntactically impossible to generate format or decode blocks that do not nest fully inside the enclosing block.\nAt any point where an instruction definition occurs without an explicit format specification, the format associated with the innermost enclosing format block will be used. If a definition occurs with no explicit format and no enclosing format block, a runtime error will be raised.\nDecode block defaults Default cases for decode blocks can be specified by default: labels, as in C switch statements. However, it is common in ISA descriptions that unspecified cases correspond to unknown or illegal instruction encodings. To avoid the requirement of a default: case in every decode block, the language allows an alternate default syntax that specifies a default case for the current decode block and any nested decode block with no explicit default. This alternate default is specified by giving the default keyword and an instruction definition after the bitfield specification (prior to the opening brace). Specifying the outermost decode block as follows:\ndecode OPCODE default Unknown::unknown() { [...] }  is thus (nearly) equivalent to adding default: Unknown::unknown(); inside every decode block that does not otherwise specify a default case.\n-\n*Note:* The appropriate format definition (see [\\#Format definitions](#Format_definitions \u0026quot;wikilink\u0026quot;)) is invoked each time an instruction definition is encountered. Thus there is a semantic difference between having a single block-level default and a default within each nested block, which is that the former will invoke the format definition once, while the latter could result in multiple invocations of the format definition. If the format definition generates header, decoder, or exec output, then that output will be included multiple times in the corresponding files, which typically leads to multiple definition errors when the C++ gets compiled. If it is absolutely necessary to invoke the format definition for a single instruction multiple times, the format definition should be written to produce *only* decode-block output, and all needed header, decoder, and exec output should be produced once using `output` blocks (see [\\#Output blocks](#Output_blocks \u0026quot;wikilink\u0026quot;)).  Preprocessor directive handling The decode block may also contain C preprocessor directives. These directives are not processed by the parser; instead, they are passed through to the C++ output to be processed when the C++ decoder is compiled. The parser does not recognize any specific directives; any line with a # in the first column is treated as a preprocessor directive. The directives are copied to all of the output streams (the header, the decoder, and the execute files; see #Format definitions). The directives maintain their position relative to the code generated by the instruction definitions within the decode block. The net result is that, for example, #ifdef/#endif pairs that surround a set of instruction definitions will enclose both the declarations generated by those definitions and the corresponding case statements within the decode function. Thus #ifdef and similar constructs can be used to delineate instruction definitions that will be conditionally compiled into the simulator based on preprocessor symbols (e.g., FULL_SYSTEM). It should be emphasized that #ifdef does not affect the ISA description parser. In an #ifdef/#else/#endif construct, all of the instruction definitions in both parts of the conditional will be processed. Only during the subsequent C++ compilation of the decoder will one or the other set of definitions be selected.\nThe declaration section As mentioned above, the decode section of the ISA description (consisting of a single outer decode block) is preceded by the declarations section. The primary purpose of the declarations section is to define the instruction formats and other supporting elements that will be used in the decode block, as well as supporting C++ code that is passed almost verbatim to the generated output. This section describes the components that appear in the declaration section: #Format definitions, #Template definitions, #Output blocks, #Let blocks, #Bitfield definitions, #Operand and operand type definitions, and #Namespace declaration.\nFormat definitions An instruction format is basically a Python function that takes the arguments supplied by an instruction definition (found inside a decode block) and generates up to four pieces of C++ code. The pieces of C++ code are distinguished by where they appear in the generated output.\n The header output goes in the header file (decoder.hh) that is included in all the generated source files (decoder.cc and all the per-CPU-model execute .cc files). The header output typically contains the C++ class declaration(s) (if any) that correspond to the instruction. The decoder output goes before the decode function in the same source file (decoder.cc). This output typically contains definitions that do not need to be visible to the execute() methods: inline constructor definitions, non-inline method definitions (e.g., for disassembly), etc. The exec output contains per-CPU model definitions, i.e., the execute() methods for the instruction class. The decode block contains a statement or block of statements that go into the decode function (in the body of the corresponding case statement). These statements take control once the bit pattern specified by the decode block is recognized, and are responsible for returning an appropriate instruction object.  The syntax for defining an instruction format is as follows:\ndef format FormatName(arg1, arg2) {{ [code omitted] }};  In this example, the format is named \u0026ldquo;FormatName\u0026rdquo;. (By convention, instruction format names begin with a capital letter and use mixed case.) Instruction definitions using this format will be expected to provide two arguments (arg1 and arg2). The language also supports the Python variable-argument mechanism: if the final parameter begins with an asterisk (e.g., *rest), it receives a list of all the otherwise unbound arguments from the call site.\nNote that the next-to-last syntactic token in the format definition (prior to the semicolon) is simply a code literal (string constant), as described above. In this case, the text within the code literal is a Python code block. This Python code will be called at each instruction definition that uses the specified format.\nIn addition to the explicit arguments, the Python code is supplied with two additional parameters: name, which is bound to the instruction mnemonic, and Name, which is the mnemonic with the first letter capitalized (useful for forming C++ class names based on the mnemonic).\nThe format code block specifies the generated code by assigning strings to four special variables: header_output, decoder_output, exec_output, and decode_block. Assignment is optional; for any of these variables that does not receive a value, no code will be generated for the corresponding section. These strings may be generated by whatever method is convenient. In practice, nearly all instruction formats use the support functions provided by the ISA description parser to specialize code templates based on characteristics extracted automatically from C-like code snippets. Discussion of these features is deferred to the Code parsing page.\nAlthough the ISA description is completely independent of any specific simulator CPU model, some C++ code (particularly the exec output) must be specialized slightly for each model. This specialization is handled by automatic substitution of CPU-model-specific symbols. These symbols start with CPU_ and are treated specially by the parser. Currently there is only one model-specific symbol, CPU_exec_context, which evaluates to the model\u0026rsquo;s execution context class name. As with templates (see #Template definitions), references to CPU-specific symbols use Python key-based format strings; a reference to the CPU_exec_context symbol thus appears in a string as %(CPU_exec_context)s.\nIf a string assigned to header_output, decoder_output, or decode_block contains a CPU-specific symbol reference, the string is replicated once for each CPU model, and each instance has its CPU-specific symbols substituted according to that model. The resulting strings are then concatenated to form the final output. Strings assigned to exec_output are always replicated and subsituted once for each CPU model, regardless of whether they contain CPU-specific symbol references. The instances are not concatenated, but are tracked separately, and are placed in separate per-CPU-model files (e.g., simple_cpu_exec.cc).\nTemplate definitions As discussed in section Format definitions above, the purpose of an instruction format is to process the arguments of an instruction definition and generate several pieces of C++ code. These code pieces are usually generated by specializing a code template. The description language provides a simple syntax for defining these templates: the keywords def template, the template name, the template body (a code literal), and a semicolon. By convention, template names start with a capital letter, use mixed case, and end with \u0026ldquo;Declare\u0026rdquo; (for declaration (header output) templates), \u0026ldquo;Decode\u0026rdquo; (for decode-block templates), \u0026ldquo;Constructor\u0026rdquo; (for decoder output templates), or \u0026ldquo;Execute\u0026rdquo; (for exec output templates). For example, the simplest useful decode template is as follows:\ndef template BasicDecode {{ return new %(class_name)s(machInst); }};  An instruction format would specialize this template for a particular instruction by substituting the actual class name for %(class_name)s. (Template specialization relies on the Python string format operator %. The term %(class_name)s is an extension of the C %s format string indicating that the value of the symbol class_name should be substituted.) The resulting code would then cause the C++ decode function to create a new object of the specified class when the particular instruction was recognized.\nTemplates are represented in the parser as Python objects. A template is used to generate a string typically by calling the template object\u0026rsquo;s subst() method. This method takes a single argument that specifies the mapping of substitution symbols in the template (e.g., %(class_name)s) to specific values. If the argument is a dictionary, the dictionary itself specifies the mapping. Otherwise, the argument must be another Python object, and the object\u0026rsquo;s attributes are used as the mapping. In practice, the argument to subst() is nearly always an instance of the parser\u0026rsquo;s InstObjParams class; see Code parsing#The InstObjParams class. A template may also reference other templates (e.g., %(BasicDecode)s) in addition to symbols specified by the subst() argument; these will be interpolated into the result by subst() as well.\nTemplate references to CPU-model-specific symbols (see #Format definitions) are not expanded by subst(), but are passed through intact. This feature allows them to later be expanded appropriately according to whether the result is assigned to exec_output or another output section. However, when a template containing a CPU-model-specific symbol is referenced by another template, then the former template is replicated and expanded into a single string before interpolation, as with templates assigned to header_output or decoder_output. This policy guarantees that only templates directly containing CPU-model-specific symbols will be replicated, never templates that contain such symbols indirectly. This last feature is used to interpolate per-CPU declarations of the execute() method into the instruction class declaration template (see the BasicExecDeclare template in the Alpha ISA description).\nOutput blocks Output blocks allow the ISA description to include C++ code that is copied nearly verbatim to the output file. These blocks are useful for defining classes and local functions that are shared among multiple instruction objects. An output block has the following format:\noutput \u0026lt;destination\u0026gt; {{ [code omitted] }};  The  keyword must be one of header, decoder, or exec. The code within the code literal is treated as if it were assigned to the header_output decoder_output, or exec_output variable within an instruction format, respectively, including the special processing of CPU-model-specific symbols. The only additional processing performed on the code literal is substitution of bitfield operators, as used in instruction definitions (see Code parsing#Bitfield operators), and interpolation of references to templates.\nLet blocks Let blocks provide for global Python code. These blocks consist simply of the keyword let followed by a code literal (double-brace delimited string) and a semicolon. The code literal is executed immediately by the Python interpreter. The parser maintains the execution context across let blocks, so that variables and functions defined in one let block will be accessible in subsequent let blocks. This context is also used when executing instruction format definitions. The primary purpose of let blocks is to define shared Python data structures and functions for use in instruction formats. The parser exports a limited set of definitions into this execution context, including the set of defined templates (see #Template definitions), the InstObjParams and CodeBlock classes (see Code parsing), and the standard Python string and re (regular expression) modules.\nBitfield definitions A bitfield definition provides a name for a bitfield within a machine instruction. These names are typically used as the bitfield specifications in decode blocks. The names are also used within other C++ code in the decoder file, including instruction class definitions and decode code. The bitfield definition syntax is demonstrated in these examples:\ndef bitfield OPCODE \u0026lt;31:26\u0026gt;; def bitfield IMM \u0026lt;12\u0026gt;; def signed bitfield MEMDISP \u0026lt;15:0\u0026gt;;  The specified bit range is inclusive on both ends, and bit 0 is the least significant bit; thus the OPCODE bitfield in the example extracts the most significant six bits from a 32-bit instruction. A single index value extracts a one-bit field, IMM. The extracted value is zero-extended by default; with the additional signed keyword, as in the MEMDISP example, the extracted value will be sign extended. The implementation of bitfields is based on preprocessor macros and C++ template functions, so the size of the resulting value will depend on the context.\nTo fully understand where bitfield definitions can be used, we need to go under the hood a bit. A bitfield definition simply generates a C++ preprocessor macro that extracts the specified bitfield from the implicit variable machInst. The machine instruction parameter to the decode function is also called machInst; thus any use of a bitfield name that ends up inside the decode function (such as the argument of a decode block or the decode piece of an instruction format\u0026rsquo;s output) will implicitly reference the instruction currently being decoded. The binary machine instruction stored in the StaticInst object is also named machInst, so any use of a bitfield name in a member function of an instruction object will reference this stored value. This data member is initialized in the StaticInst constructor, so it is safe to use bitfield names even in the constructors of derived objects.\nOperand and operand type definitions These statements specify the operand types that can be used in the code blocks that express the functional operation of instructions. See Code parsing#Operand type qualifiers and Code parsing#Instruction operands.\nNamespace declaration The final component of the declaration section is the namespace declaration, consisting of the keyword namespace followed by an identifier and a semicolon. Exactly one namespace declaration must appear in the declarations section. The resulting C++ decode function, the declarations resulting from the instruction definitions in the decode block, and the contents of any declare statements occurring after then namespace declaration will be placed in a C++ namespace with the specified name. The contents of declare statements occurring before the namespace declaration will be outside the namespace.\n"
},
{
	"uri": "http://localhost/developer/todo/",
	"title": "Looking to help? ",
	"tags": [],
	"description": "",
	"content": " This page contains ideas for projects to work on during the gem5 coding sprint held in conjunction with HPCA 2017. For more information on the sprint, see: http://learning.gem5.org/tutorial/index.html.\nBelow are three categories of projects: small, medium, and large. I expect that only the \u0026ldquo;small\u0026rdquo; can be easily completed in an afternoon. For medium projects, you should be able to get a good start on them in an afternoon and finish up with a few extra hours of work after the sprint. Large project ideas are unlikely to be finished within a few days. These ideas are mainly for the community to get together and discuss at the sprint, not to start coding.\nSmall ideas Fix TLB warmup for x86 [Jason] See http://reviews.gem5.org/r/3474/. This patch was a first stab at fixing this problem, but it should be re-written and re-tested based on Steve\u0026rsquo;s feedback.\nThe problem this patch tries to solve is that when switching from the atomic CPU to a timing CPU, the information in the TLB is lost. It is very important to not lose this TLB information when using the atomic CPU to warm up the system. By losing the TLB data, all experiments run with warmup will experience many more TLB misses than expected.\nNote: This requires using FS mode to test.\nModify EventWrapper to understand C++11 lambdas [Jason] The current EventWrapper can only wrap a very simple function that takes no parameters. However, I believe that C++11 supports passing pointers to partially bound function. See the documentation for std::bind: http://en.cppreference.com/w/cpp/utility/functional/bind.\nWith this feature, we should be able to delete most of the Event subclasses throughout the codebase, too. The idea is that most of the time the Event subclasses are only used to pass a single parameter from where the event is created to the event callback function.\nThis project should be quick for someone familiar with C++11 lambdas and early binding of parameters.\nDevelop some ISA instruction tests [Many people] The goal of this project is to find out what is implemented correctly and possibly find some bugs in our ISAs. For instance, a recent GCC change exposed a bug in the x86 ISA.\nThe RISC-V insttest is an example of instruction tests that are already in the codebase. It should be pretty quick to put together a simple test which goes through some of the x86 and/or ARM instructions and explicitly tests each instruction like the RISC-V example above.\nA longer-term goal would be to automate this process so we have some confidence that our implementation of the ISA is correct. Someone with experience validating hardware may have ideas on how to do this.\nClean up serialization code [Andreas S] Increase the code reuse (particularly container helpers).\nCI smoke tests (faster than quick) [Andreas S] Create a separate test classification to quickly check to see if a change breaks anything major. Right now, contributors often just compile gem5 and say \u0026ldquo;close enough\u0026rdquo;. This project would create a new class of tests (probably just move some tests around) so that developers can quickly (less than 5 minutes) test to see if they have made any breaking changes. This is also important for our upcoming CI changes.\nPush in fixes to x86 KVM See http://m5-dev.m5sim.narkive.com/SUFVlG5b/gem5-dev-x86-kvm-status.\nThere are a couple of patches on reviewboard that fix some problems with the x86 KVM code. See http://reviews.gem5.org/r/2557/ and http://reviews.gem5.org/r/2613. It would be great for someone with access to both AMD and Intel hardware (and maybe multiple versions of Linux?) to test these and/or fix them up so they can be merged.\nUnify PS/2 handling [Andreas S] PS2 is implemented by both the i8042 and PL050 models, but almost no code is shared. This should be an easy way to get your hands a little dirty with gem5\u0026rsquo;s current code.\nUnifty PCI config interfaces Many supported architectures (at least x86, ARM, and Alpha) implement PCI. Some of the complexity has been hidden by the new PCI Host abstraction, but we still need to do platform-specific port wiring. We should unify this to make it possible for configuration scripts to reuse more code between architectures.\nOther ideas [Tony]  Adding checkpointing support to the GPU model Properly supporting atomics Add support for event-based scheduling in the GPU model, and FUPool-style functional units  Medium sized projects Config cleanups [Andreas S/Jason] Most of the configs are not very object-oriented. I think these can be restructured in a much better designed system. Jason thinks a good example is his SimpleFS scripts: https://github.com/powerjg/gem5/tree/devel/simplefs/configs/myconfigs Andreas likes this script: https://github.com/gem5/gem5/blob/master/configs/example/arm/fs_bigLITTLE.py\nSome ideas on how to do this:\n move some of config/common/ to a m5.config name space. move the DRAM controller specifics to common/dram create some common CPU configs in common/cpu make some better example scripts  Embed the generated system SVG in a web page [Andreas H] It would be cool if you could interactively navigate through the system and see the simulation results.\nThis should be fairly easy for anyone skilled in client-side scripting. It may even be used to view incremental results while the simulation is running.\nOther ideas  New test binaries based on the LLVM test suite [Andreas S] Mini-DSL for param overrides from the command line [Andreas S] Proper support for pthreads in SE mode [Andreas S] Implement a fast mode in the HDLCD controller to support graphical workloads (e.g., Android) in KVM [Andreas S] Fixing the structure and design of the GPU coalescer [Tony]  Long-term ideas to discuss Syscall emulation on different platforms [Andreas H] See https://www.mail-archive.com/gem5-dev@gem5.org/msg21463.html. When compiling Linux syscall emulation mode on other operating systems (e.g., MacOS, FreeBSD) many of the syscall interfaces are different. This causes compilation errors since the headers are not available on these other systems. It is a good idea to take some time to talk about a long-term solution to this problem.\nRe-write the test infrastructure [Jason] Jason really doesn\u0026rsquo;t like how the current test infrastructure uses SCons. I think it makes in incredibly hard to use. Plus, it makes it difficult to integrate into CI systems (e.g., Jenkins).\nA couple of \u0026ldquo;requirements\u0026rdquo;. This is not an exhaustive list.\n Incremental testing (don\u0026rsquo;t test things for parts of the code that didn\u0026rsquo;t change) Functional-only tests Easy to compare a subset of stats (e.g., all that matters is that the number of cache misses is the same)  Re-write the build system [Jason/Andreas S] Andreas S. has taken some steps to replace SCons with cmake. Another interesting option is bazel: https://bazel.build/, a new build system from Google. This would be a rather huge undertaking, though.\nA few requirements. Again, this list isn\u0026rsquo;t exhaustive.\n Split gem5\u0026rsquo;s components into separate dynamically-linked objects (reduce the size of the gem5 binary) Faster! Play nice with test and CI systems.  A binary-translation CPU [Andreas H] A large-sized project for some crafty person out there. This would be useful for fast-forwarding, much like the KVMCpu, but more portable. It could, for example, be built on top of the Tiny Code Generator (TCG), as it is BSD licensed. Another possible place to look for inspiration is pydgin (https://github.com/cornell-brg/pydgin) which leverages Python (and PyPy/RPython) to build a jit for binary translation.\nQuite a big task, but also a very big contribution to gem5.\n"
},
{
	"uri": "http://localhost/benchmarks/parsec/",
	"title": "PARSEC",
	"tags": [],
	"description": "",
	"content": " PARSEC v2.1 The PARSEC benchmarks have been built to run in the gem5 full-system simulation mode. The following section details references and the process for building and running the suite.\nDownload Pre-built M5 Disk Image PARSEC has been built to run on gem5 with the ALPHA ISA, and disk images are available at Running PARSEC 2.1 on M5 from The University of Texas. You can download a disk image there and unzip it.\nThe original system files that can be downloaded from http://www.m5sim.org/Download will have the following directory structure upon extracting:\nsystem/ binaries/ console ts_osfpal vmlinux disks/ linux-bigswap2.img linux-latest.img  Now, if you are going to run for the ALPHA ISA, then you need to swap the ts_osfpal, vmlinux, linux-latest.img files in the above directory structure with the ones provided at http://www.cs.utexas.edu/~parsec_m5/\nAfter unzipping the file, you will need to specify where gem5 should look for the disk image. In the file ./configs/common/SysPaths.py, specify the path to your disk image in the\nline: path = [ ’/dist/m5/system’, ’\u0026lt;complete path to your disks and binaries directory\u0026gt;’ ]  Next, in ./configs/common/Benchmarks.py, specify the name of your disk image. For example:\nreturn env.get(’LINUX_IMAGE’, disk(’linux-parsec-2-1-m5.img’))  You should now be set up to run with benchmarks that are on the disk image. See the section below about running gem5 for more details on how to execute the simulation.\nRunning PARSEC in gem5 To run PARSEC in gem5, you can specify a run script to gem5 on the command\nline ./build/ALPHA_FS/m5.opt ./configs/example/fs.py --script=./path/to/runscript.rcS  Where the script, runscript.rcS is a shell script that contains commands to execute the benchmark. For example:\n#!/bin/sh # File to run the blackscholes benchmark cd /parsec/install/bin /sbin/m5 dumpresetstats ./blackscholes 64 /parsec/install/inputs/blackscholes/in_64K.txt /parsec/install/inputs/blackscholes/prices.txt echo \u0026quot;Done :D\u0026quot; /sbin/m5 exit  This example changes directories to the location where the binary exists on the disk image, resets the gem5 statistics, runs the blackscholes benchmark and exits.\nReferences For more details on the process for building and running PARSEC for the ALPHA ISA, see Running PARSEC 2.1 on M5 from The University of Texas.\nPARSEC 3.0 ARM In the following setting, 12 out of the 13 PARSEC benchmarks can be built (and run!) successfully:\n PARSEC 3.0-beta-20150206 with this patch Mirror #1 Mirror #2 native compilation on Debian Wheezy armhf (e.g. on qemu-system-arm -M vexpress-a9) the resulting static binaries work in gem5 (if you copy them, do not forget to copy the input files as well)  The following benchmarks work:\n blackscholes bodytrack canneal dedup facesim ferret fluidanimate freqmine streamcluster swaptions vips x264  The following benchmark does not work:\n raytrace (LRT/render.cxx uses SSE intrinsics)  "
},
{
	"uri": "http://localhost/developer/sparc/",
	"title": "SPARC Curiousities",
	"tags": [],
	"description": "",
	"content": " Ancillary State Registers  Floating-Point State Register (FSR) - holds 4 sets of condition codes, exception status, trap enable, rounding direction, etc. Which means we need to rename it, but we need to copy parts of the previous state into it every time.   Y Register - Why ohh why did the make this register. Holds upper 32 bits for some multiple/div operations. For multiply the upper 32 bits of the result is placed in here and the entire result is placed in the dest register. For some divides the upper 32 bits are read from here and then afterwards the state is unpredictable. Depending on how common these depricated operations are, we may be able to flush after each one instead of renaming the register.   Condition Code Register (CCR) - 8 bit register 4 bits for 64bit CC, 4 bits for 32bit CC. We\u0026rsquo;ll have to rename them and in the case of sparc, they are all written at once so we don\u0026rsquo;t have to rename some of them.   Address space identifies (ASI) - Distinguishes between different address space, what kind of translation is done, is the MMU used, also used to map control register for the processor. 255 ASIs 128 of which are unprivileged, the rest required privilidge or hyperpriviledge. Accessing a ASI you don\u0026rsquo;t have privileges for results in a trap. ASI register is used when you issue a load/store alternate instruction. This could be preformance critical and we may want to rename it.   TICK Register - Counts Strand? Ticks, starting at 0 when the processor boots. Upper most bit controls if unprivileged software can read the bit. Make instruction that writes serializing?   Floating-Point Register State (FPRS) - enable/disable the FPU. serializing?   Performace Control Register (PCR) - don\u0026rsquo;t implement?   Performance Instrumentation Conter (PIC) - don\u0026rsquo;t implement?   General State Register (GSR) - holds info for VIS (SIMD) instructions. lower 32 bits flags and such, upper 32 bits set by BMASK instruction and used by BSHUFFLE instruction. Low bits can probably serialize on being written, may want to rename upper bits?   SOFTINT - read current interrupts, write to cause an software interrupt, two bits to enable/disable timer interrupts based of TICK register. Make writes serializing? Has two pseudo registers that writes to them either set or clear bits in the actually register.   Tick Compare (TICK_CMPR) - when lower 63 bits match lower 63 bits of TICK register level 14 interrupt occurs (if everything is enabled) bit 63 + SOFTINT.tim and PIL determine if interrupt actually happens. Writes serializing?   STICK,STICK_CMPR - same as tick, tick compare but synchornized across all processors.  Priviliged State Registers Writes always serializing?\n TPC[1..MAXTL] - PC from the previous trap level, written on trap, based on current TL only one readable. TNPC[1..MAXTL] - NPC from the previous trap level, written on trap, based on current TL only one readable. Trap State (TSTATE)[1..MAXTL] - various bits from registered copied in when a trap occurs. GL, CCR, ASI, PSTATE, and CWP. Trap Type (TT)[1..MAXTL] - Specified what causes the trap, based on current TL only one readable. Trap Base Address (TBA) - virtual address of the trap table. Processor State (PSTATE) - Memory ordering model, address modes, another FPU enable bit, etc TL - controls which of the above registers is accessed, values you can write into it dependent on current mode, autoincremented on trap PIL - Like IPL on alpha GL - chooses which set of global registers are enabled, values you can write into it dependent on current mode, autoincremented on trap Hyperprivileged State (HPSTATE) - various little bits. Hyperprivileged Trap State (HTSTATE) - like Trap state, but for the HPSTATE register Hyperprivileged Interrupt Pending (HINTP) - if an STICK and HSTICK_CMPR are equal. (timer interrupt for Hypervisor). Hyperpriviliged Trap Based Address ( HTBA) - same as TBA for HP state Hyperpriviliged Implementation Version Register (HVER) - read only bits that describes the current version, manufacturer, MAXGL, MAXTL, MAXWIN. Hyperpriviliged Tick Compare (HSTICK_CMPR) - like STICK CMPR for hyperprivileged code.  Instructions  CASA/CASXA - Compare and swap - IF: rs2 == MEM[rs1]: MEM[rs1][31..0] \u0026lt;-\u0026gt; rd[31..0]; rd[63..32] \u0026lt;- 0 ELSE: rd[31..0]\u0026lt;- MEM[rs1][31..0] ; rd[63..32] \u0026lt;- 0 FLUSH - make writes to program memory visable in the icache. (LD|ST)TD - Load/Store twin double word instructions operate on 128bits loading/storing them from two adjacent registers in one atomic operation so you don\u0026rsquo;t have to do any locking for a TLB update. yay.  "
},
{
	"uri": "http://localhost/deprecated/spec-omp/",
	"title": "SPEComp",
	"tags": [],
	"description": "",
	"content": " How to run SpecOMP on M5  build cross-compiler  download crosstool-0.43.tar.gz from http://kegel.com/crosstool/#download (http://kegel.com/crosstool/crosstool-0.43.tar.gz) untar it edit demo-alpha.sh  RESULT_TOP=\u0026ldquo;your_alpha_compiler\u0026rdquo; GCC_LANGUAGE=\u0026ldquo;c,c++\u0026rdquo; \u0026ndash;\u0026gt; GCC_LANGUAGE=\u0026ldquo;c,c++,fortran\u0026rdquo; eval `cat \u0026hellip;dat \u0026hellip;dat` sh all.sh \u0026ndash;\u0026gt; eval `cat alpha.dat gcc-4.2.4-glibc-2.3.6-tls.dat` sh all.sh \u0026ndash;notest  set gcc to gcc-3.4 instead of gcc-4.3 (when I use gcc-4.3 to build the cross-compiler, there is a segmentation fault) run demo-alpha.sh there may be some errors (a header file (.h) missing \u0026ldquo; or some things like that). You can modify the files directly and fix these errors. after you modify the files, you should change demo-alpha.sh again  eval `cat alpha.dat gcc-4.2.4-glibc-2.3.6-tls.dat` sh all.sh \u0026ndash;notest \u0026ndash;\u0026gt; eval `cat alpha.dat gcc-4.2.4-glibc-2.3.6-tls.dat` sh all.sh \u0026ndash;nounpack -notest  --nounpack flag avoids downloading the source again to overwrite you modification   get a new cross-compiler for alpha supports openmp  compile SpecOmp  because runspec may not work in M5, I compile the source directly  install specomp by following its document change directory to benchspec edit Makefile.defaults  LIBS=-Lyour_alpha_compiler/gcc-4.2.4-glibc-2.3.6/alpha-unknown-linux-gnu/lib CC=your_alpha_compiler/gcc-4.2.4-glibc-2.3.6/alpha-unknown-linux-gnu/bin/alpha-unknown-linux-gnu-gcc CFLAGS=-Iyour_alpha_compiler/gcc-4.2.4-glibc-2.3.6/alpha-unknown-linux-gnu/include $(EXTRA_CFLAGS) $(PORTABILITY) $(CPORTABILITY) -fopenmp -O3 CXX=your_alpha_compiler/gcc-4.2.4-glibc-2.3.6/alpha-unknown-linux-gnu/bin/alpha-unknown-linux-gnu-gcc FC=your_alpha_compiler/gcc-4.2.4-glibc-2.3.6/alpha-unknown-linux-gnu/bin/alpha-unknown-linux-gnu-gfortran FFLAGS=-Iyour_alpha_compiler/gcc-4.2.4-glibc-2.3.6/alpha-unknown-linux-gnu/include $(EXTRA_FFLAGS) $(PORTABILITY) $(FPORTABILITY) -fopenmp -O3 F77FLAGS=-Iyour_alpha_compiler/gcc-4.2.4-glibc-2.3.6/alpha-unknown-linux-gnu/include $(EXTRA_FFLAGS) $(PORTABILITY) $(FPORTABILITY) -fopenmp -O3 LD=$(CC) $(CFLAGS) -Lyour_alpha_compiler/gcc-4.2.4-glibc-2.3.6/alpha-unknown-linux-gnu/lib -fopenmp -O3  change directory to OMPM2001/3xx.xxxx/src (e.g., 320.equake/src) make and get the binary for alpha the list which can be compiled without error: wupwise, swim, applu, equake, apsi, gafort (however, there is a segmentation fault during execution), art, ammp  put files into images  copy all files and directories from your_alpha_compiler/gcc-4.2.4-glibc-2.3.6/alpha-unknown-linux-gnu/alpha-unknown-linux-gnu/lib to /lib of the linux image for M5 (e.g., linux-latest.img) copy the executables and input files of SpecOmp to another image and set it as the third disk image of M5  run specomp  run M5 mount the image containing the files of specomp execute specomp executable with correspondent commands.   I am not very sure that I remember everything. However, the operation flow may help others who want to do the same thing. If there is any missing steps or mistakes, please correct me.\n"
},
{
	"uri": "http://localhost/developer/serialization/",
	"title": "Serialization",
	"tags": [],
	"description": "",
	"content": " Saving/Restoring object state Dealing with Events Handling SimObject Pointers Phases of Object Initialization If restoring from a checkpoint, loadState(ckpt) is called on each SimObject. The default implementation simply calls unserialize() if there is a corresponding checkpoint section, so we get backward compatibility for existing objects. However, objects can override loadState() to get other behaviors, e.g., doing other programmed initializations after unserialize(), or complaining if no checkpoint section is found. (Note that the default warning for a missing checkpoint section is now gone.)\nIf not restoring from a checkpoint, we call the new initState() method on each SimObject instead. This provides a hook for state initializations that are only required when *not* restoring from a checkpoint.\n"
},
{
	"uri": "http://localhost/developer/sim-objects/",
	"title": "SimObjects",
	"tags": [],
	"description": "",
	"content": " SimObjects The python side Param types Inheritance Special attribute names Rules for importing - how to get what Pro tips - avoiding cycles, always descend from root, etc. The C++ side create functions Stages of initialization The basic order of C++ SimObject construction and initialization is controlled by the instantiate() and simulate() Python functions in src/python/m5/simulate.py. This process was revised in changeset 3f6413fc37a2. This page documents the process as of that changeset.\nOnce the Python SimObject hierarchy is created by the user\u0026rsquo;s simulation script, that script calls instantiate() to create the C++ equivalent of that hierarchy. The primary steps in this process are:\n Resolve proxy parameters (those specified using Self or Parent). Dump the config.ini file to record the final resolved set of SimObjects and parameter values. Call the C++ constructors for all of the SimObjects. These constructors are called in an order that satisfies parameter dependencies, i.e., if object A is passed as a parameter to object B, then object A\u0026rsquo;s constructor will be called before object B\u0026rsquo;s constructor so that the C++ pointer to A can be passed to B\u0026rsquo;s constructor. Note that this policy means that cyclic parameter dependencies cannot be supported. Instantiate the memory-system port connections. In the case of restoring from a checkpoint, not all ports are connected at this stage, i.e., the switch CPU(s) are not connected. Call init() on each SimObject. This provides the first opportunity to perform initializations that depend on all SimObjects being fully constructed. Call regStats() on each SimObject. Complete initialization of SimObject state from a checkpoint or from scratch.  If restoring from a checkpoint, call loadState(ckpt) on each SimObject. The default implementation simply calls unserialize() if there is a corresponding checkpoint section, so we get backward compatibility with earlier versions of the startup code for existing objects. However, objects can override loadState() to get other behaviors, e.g., doing other programmed initializations after unserialize(), or complaining if no checkpoint section is found. If not restoring from a checkpoint, call initState() on each SimObject instead. This provides a hook for state initializations that are only required when not restoring from a checkpoint.  If restoring from a checkpoint, the switch CPU ports are connected in BaseCPU::takeOverFrom at this stage. Later, the first time that the user script calls simulate(), call startup() on each SimObject. This is the point where SimObjects that do self-initiated processing should schedule internal events, since the value of curTick could change during unserialization.  Initialization Function Call Sequence The figure below shows the function call sequence for the initialization of gem5 when running the \u0026ldquo;Hello, World\u0026rdquo; example given in the Introduction section. Each node in the graph gives the path to the source code file in which the function is located. This representation should aid in the first steps of familiarization with the basic initialization steps of gem5, including how the configuration files are used and how the objects are constructed.\nHeader files to include "
},
{
	"uri": "http://localhost/developer/simple-cpu/",
	"title": "SimpleCPU",
	"tags": [],
	"description": "",
	"content": " The SimpleCPU is a purely functional, in-order model that is suited for cases where a detailed model is not necessary. This can include warm-up periods, client systems that are driving a host, or just testing to make sure a program works.\nIt has recently been re-written to support the new memory system, and is now broken up into three classes.\nBaseSimpleCPU The BaseSimpleCPU serves several purposes:\n Holds architected state, stats common across the SimpleCPU models. Defines functions for checking for interrupts, setting up a fetch request, handling pre-execute setup, handling post-execute actions, and advancing the PC to the next instruction. These functions are also common across the SimpleCPU models. Implements the ExecContext interface.  The BaseSimpleCPU can not be run on its own. You must use one of the classes that inherits from BaseSimpleCPU, either AtomicSimpleCPU or TimingSimpleCPU.\nAtomicSimpleCPU The AtomicSimpleCPU is the version of SimpleCPU that uses atomic memory accesses (see Memory System for details). It uses the latency estimates from the atomic accesses to estimate overall cache access time. The AtomicSimpleCPU is derived from BaseSimpleCPU, and implements functions to read and write memory, and also to tick, which defines what happens every CPU cycle. It defines the port that is used to hook up to memory, and connects the CPU to the cache.\nTimingSimpleCPU The TimingSimpleCPU is the version of SimpleCPU that uses timing memory accesses (see Memory System for details). It stalls on cache accesses and waits for the memory system to respond prior to proceeding. Like the AtomicSimpleCPU, the TimingSimpleCPU is also derived from BaseSimpleCPU, and implements the same set of functions. It defines the port that is used to hook up to memory, and connects the CPU to the cache. It also defines the necessary functions for handling the response from memory to the accesses sent out.\nFlow of execution "
},
{
	"uri": "http://localhost/docs/simpoints/",
	"title": "Simpoints w/gem5",
	"tags": [],
	"description": "",
	"content": " Steps to profile, generate, and use SimPoints with gem5:\nProfiling and Generating BBV To profile a workload and generate a SimPoint BBV file, use the following\ncommand: % build/ARM/gem5.opt \u0026lt;base options\u0026gt; configs/example/fs.py --simpoint-profile [--simpoint-interval \u0026lt;interval length\u0026gt;] \u0026lt;rest of fs.py options\u0026gt;  This will generate a SimPoint Basic Block Vector file (named simpoint.bb.gz) in the gem5 run folder. SimPoint profiling should be run with a single AtomicSimpleCPU configuration and fastmem. Multicore simulation is not supported. is in number of instructions. Default interval length is 10M instructions.\nSimPoint Analysis Generate SimPoint analysis using SimPoint 3.2 from UCSD. See http://cseweb.ucsd.edu/~calder/simpoint/index.htm for details. (SimPoint 3.2 not included in gem5 repository.)\nA sample command to generate the SimPoint analysis based on the BBV file is as\nfollows: % simpoint -loadFVFile simpoint.bb.gz -maxK 30 -saveSimpoints \u0026lt;simpoint_file\u0026gt; -saveSimpointWeights \u0026lt;weight_file\u0026gt; -inputVectorsGzipped  The gem5 flow currently does not support variable-length intervals.\nTaking SimPoint Checkpoints in gem5 To take gem5 checkpoints based on SimPoint analysis, use the following command:\n% build/ARM/gem5.opt \u0026lt;base options\u0026gt; configs/example/fs.py --take-simpoint-checkpoint=\u0026lt;simpoint file path\u0026gt;,\u0026lt;weight file path\u0026gt;,\u0026lt;interval length\u0026gt;,\u0026lt;warmup length\u0026gt; \u0026lt;rest of fs.py options\u0026gt;  and is generated by the SimPoint analysis step above. and are in number of instructions. gem5 checkpoints will be generated instructions before the designated SimPoint starting points.\nResuming from gem5 SimPoint Checkpoints To resume from gem5 SimPoint checkpoints, use the following\ncommand: % build/ARM/gem5.opt \u0026lt;base options\u0026gt; configs/example/fs.py --restore-simpoint-checkpoint -r \u0026lt;N\u0026gt; --checkpoint-dir \u0026lt;simpoint checkpoint path\u0026gt; \u0026lt;rest of fs.py options\u0026gt;   is (SimPoint index + 1). E.g., \u0026ldquo;-r 1\u0026rdquo; will resume from SimPoint #0.\nThis command will resume from a single specified SimPoint checkpoint. After simulating the warmup period (automatically recognized from the checkpoint folder name), gem5 statistics will be dumped and reset. Subsequently, after simulating the actual SimPoint, gem5 will automatically terminate. The simulations for all other SimPoints should be run individually. To project the full execution, the gem5 statistics from individual SimPoints must be weighted accordingly. Remember to use the second gem5 statistics window (not the one from the warmup period).\nQuestions / Feedback For questions or feedback on this flow, please contact Dam Sunwoo (dam.sunwoo@arm.com)\n"
},
{
	"uri": "http://localhost/docs/splash/",
	"title": "Splash",
	"tags": [],
	"description": "",
	"content": " It is possible to run the SPLASH-2 benchmarks on M5 in two different ways, each with their own caveats.\nUsing full-system mode The most robust approach is to compile the benchmarks using a Pthreads implementation of the PARMACS macros, then link with the standard Linux Pthreads library and run this binary on M5 under full-system mode.\nAdvantages:\n Realistic: you\u0026rsquo;re getting the actual Linux thread scheduler to schedule your threads Robust (in contrast to current SE-mode approaches\u0026hellip; see below) You can build a cross-compiler to compile the binaries on non-Alpha platforms (see Using linux-dist to Create Disk Images and Kernels for M5\u0026hellip; note that you don\u0026rsquo;t need to build a kernel, just the cross-compiler).  Disadvantages:\n CPU limits: the Tsunami platform we model only supports 4 CPUs, though we have patches to make that scale to 64 (see Frequently Asked Questions#How many CPUs can M5 run?). Overhead: you\u0026rsquo;ve got to download a disk image, get the binaries onto the disk image, boot Linux under M5, etc. This isn\u0026rsquo;t nearly as bad as it sounds, but it\u0026rsquo;s still extra work.  For a step by step guide on running the benchmarks in full-system mode see this document or follow the next steps:\nDownload Download splash2. Download patches for SPLASH2 from UDEL.\nCompiling Follow the directions from the UDEL site. Most of the benchmarks will compile fine. Some need additional massaging.\nradiosity: descend into glibdumb and glibps and ‘make’.\nvolrend: unarchive and build the libtiff; I had to modify its makefile by adding -DBSDTYPES in the CFLAGS.\nCopying Mount the disk image used in simulation, for example\nsudo mount -o loop,offset=32256 linux-arm-ael.img /mnt/tmp  Executing The following are the basic \u0026ldquo;default\u0026rdquo; ways to run benchmarks. If you compile for your host architecture you should be able to test these directly.\nbarnes: ./BARNES \u0026lt; input\nfmm: uncompress the files in the inputs directory, then ./FMM \u0026lt; inputs/input.16384\nocean: ./OCEAN\nradiosity: ./RADIOSITY -batch\nraytrace: uncompess the files in the inputs directory. I had to run with additional memory (probably a data sizing issue with 64-bit machinery): ./RAYTRACE -m64 inputs/car.env\nvolrend: Uncompress the files in the inputs directory, then ./VOLREND 1 inputs/head (omit the .den extension, it is automatically appended in the code).\nwater-nsquared: ./WATER-NSQUARED \u0026lt; input\nwater-spatial: ./WATER-SPATIAL \u0026lt; input\nUsing syscall-emulation mode Note this section is really out of date; who wants to use Tru64\n There are two approaches to support SPLASH benchmarks in SE mode:\n Use the same Pthreads implementation of PARMACS that you can run in FS mode, and enhance SE mode to handle the necessary syscalls. Use a custom M5-specific PARMACS library, possibly coupled with M5-specific syscalls, to support only the thread management features needed by SPLASH.  Unfortunately for option 1, supporting general Pthreads applications in SE mode is extremely difficult. Under both Tru64 and Linux, Pthreads uses an extra \u0026ldquo;management thread\u0026rdquo; to perform some tasks, which means an N-thread SPLASH application (which is what you\u0026rsquo;d typically want to run on an N-CPU machine) really has N+1 threads, and suddenly you need a thread scheduler in M5 to figure out which threads are runnable, assign them to CPUs, maybe preempt one of them if all N+1 are runnable, etc. Worse yet, the Linux Pthreads library uses a pipe to communicate from the application threads to the management threads, requiring you to implement poll() and add signal support (so you can deliver SIGIO to threads), and lots of other nasty stuff. Frankly it\u0026rsquo;s just not worth the effort, given that the Linux kernel already has excellent implementations of pipes and poll() and signals, and you can just run that under FS mode (see above).\nOption 2 is arguably the \u0026ldquo;right\u0026rdquo; way to support SPLASH applications under SE mode. Your custom PARMACS macro implementation can assume you\u0026rsquo;ll never allocate more threads than CPUs, so you don\u0026rsquo;t need any thread scheduling in M5. This implementation could call existing syscalls where appropriate, or call new M5-specific syscalls that are added specifically for this purpose. This is what most existing simulators that support SPLASH do.\nUnfortunately neither of these environments currently exist in a clean form. What does exist is a historical artifact that resulted from me (Steve) trying to support option 1 for Tru64, i.e. SPLASH applications compiled to the Tru64 Pthreads library. (This code actually predates M5\u0026rsquo;s support for Alpha Linux.) There is a lot of complex code in src/kern/tru64 that attempts to do this. Partway through I came to the realization I mentioned in the paragraph above, that doing a complete job would end up with me writing a full thread scheduler inside M5. At that point I gave up and finished the job by switching to option 2, adding M5-specific implementations of the remaining PARMACS macros that were giving me trouble, using special M5 syscalls I added for that purpose.\nThis code (including binaries) is available here. Feel free to use it, but be aware of the following caveats:\n Because this code is based on Tru64, and it\u0026rsquo;s not possible (to our knowledge) to build a gcc cross-compiler that targets Tru64, you can\u0026rsquo;t compile new binaries without a native Alpha Tru64 system. Because the code partly tries to support the N+1 thread Tru64 Pthreads model, there may be situations where it doesn\u0026rsquo;t work (e.g., if you use large numbers of processors, or unusual inputs). Some of the synchronization primitives use \u0026ldquo;magic\u0026rdquo; M5 system calls, so the synchronization overheads may not be realistic.  If you have trouble, see this email message for more information.\nIn summary, your best bet is to use FS mode. If someone would like to do an Linux-based \u0026ldquo;option 2\u0026rdquo; implementation for SE mode, that would be terrific, and we would be happy to redistribute that with M5. However, to date, that has not happened. Meanwhile, you\u0026rsquo;re welcome to use the Tru64 code, but be aware that it\u0026rsquo;s got some issues.\n"
},
{
	"uri": "http://localhost/developer/static-inst/",
	"title": "StaticInst",
	"tags": [],
	"description": "",
	"content": " This section describes the static instruction objects. These objects are what a decoder function instantiates for each machine instruction fetched during simulation. It has information such as opcode, source and destination register, immediate value, of the machine instruction it was generated from.\nThe definitive documentation for these objects is associated with the source code (particularly in static_inst.hh). The purpose of this section is to provide an overview of the static instruction class hierarchy and how it is used to factor the common portions of instruction descriptions.\nThe instruction objects returned by the decoder during simulation are instances of classes derived from the C++ class StaticInst, defined in static_inst.hh. Because some aspects of StaticInst depend on the ISA, such as the maximum number of source registers, StaticInst is defined as a template class. The ISA-specific characteristics are determined by a class template parameter.\nThe following code snippet is from Alpha\u0026rsquo;s decoder function defined in build/ALPHA{FS|SE}/arch/alpha/decoder.cc. As the instruction decoded is a load address instruction, the decoder initiates an object of class Lda and returns it. Lda is a class derived from class StaticInst, and its class definition is in in build/ALPHA{FS|SE}/arch/alpha/decoder.hh. This object is used throughout the simulation to represent the particular machine instruction at specific PC. Both of these files (decoder.cc and decoder.hh) are generated from The_M5_ISA_description_language by Code_parsing, so they are under subdirectory build, not src.\nStaticInstPtr AlphaISA::decodeInst(AlphaISA::ExtMachInst machInst) { using namespace AlphaISAInst; switch (OPCODE) { case 0x8: // LoadAddress::lda([' Ra = Rb + disp; '],{}) return new Lda(machInst); ...  StaticInst base class The information contained in a StaticInst object includes:\n a vector of flags that indicate the instruction\u0026rsquo;s basic properties, such as whether it\u0026rsquo;s a memory reference; the instruction\u0026rsquo;s operation class, used to assign the instruction to an execution unit in a detailed CPU mode; the number of source and destination registers the number of destination registers that are integer and floating point, respectively the register indices of sources and destinations a function to generate the text disassembly of the instruction functions to emulate the execution of the instruction under various CPU models additional information specific to particular classes of instructions, such as target addresses for branches or effective addresses for memory references.  Some of these items, such as the number of source and destination registers, are simply data fields in the base class. When decoding a machine instruction, the decoder simply initializes these fields with appropriate values. Other items, such as the functions that emulate execution, are best defined as virtual functions, with a different implementation for each opcode. As a result, the decoder typically returns an instance of a different derived class for each opcode, where the execution virtual functions are defined appropriately. Still other features, such as disassembly, are best implemented in a hybrid fashion. Virtual functions are used to provide a small set of different disassembly formats. Instructions that share similar disassembly formats (e.g., integer immediate operations) share a single disassembly function via inheritance.\nStaticInst class hierarchy The class hierarchy is not visible outside the decoder; only the base StaticInst class definition is exported to the rest of the simulator. Thus the structure of the class hierarchy is entirely up to the designer of the decoder module. At one extreme, a single class could be used, containing a superset of the required data fields and code. The execution and disassembly functions of this class would then examine the opcode stored in the class instance to perform the appropriate action, effectively re-decoding the instruction. However, this design would run counter to the goal of paying decode overhead only once.\nIn the end, the structure of the class hierarchy is driven by practical considerations, and has no set relationship to opcodes or to the instruction categories defined by the ISA. For example, in the Alpha ISA, an \u0026ldquo;addq\u0026rdquo; instruction could generate an instance of one of three different classes:\n A typical register-plus-register add generates an instance of the Addq class, which is derived directly from the base AlphaStaticInst class. A register-plus-immediate add generates an instance of AddqImm, which derives from the intermediate IntegerImm class. IntegerImm derives directly from AlphaStaticInst, and is the base class of all integer immediate instructions. It adds an \u0026lsquo;imm\u0026rsquo; data field to store the immediate value extracted from the machine instruction, and overrides the disassembly function to format the immediate value appropriately. An \u0026ldquo;addq\u0026rdquo; instruction that specifies r31 (the Alpha zero register) as its destination is a no-op, and returns an instance of the Nop class, regardless of whether it uses a reg-reg or reg-imm format.  "
},
{
	"uri": "http://localhost/docs/source-code/",
	"title": "Tour of sourcecode",
	"tags": [],
	"description": "",
	"content": " Source Browsing Tools The gem5 source code is browsable online via several methods. You can browse the latest version or the developmen history in our Mercurial repository, search the code using our OpenGrok installation, or look at the Doxygen-generated documentation (note that the class list is perhaps the most useful starting point).\nOnce you have your own local copy of the tree, you use other tools to index and search that copy. Many gem5 developers use cscope, and have included a script in util/cscope-index.py to generate a cscope index.\nTour of the tree  AUTHORS - A list of people who have historically contributed to gem5. LICENSE - The license terms that apply to gem5 as a whole, unless overridden by a more specific license. README - Some very basic information introducing gem5 and explaining how to get started. SConstruct - A part of the build system, as is the build_opts directory. build_opts - holds files that define default settings for build different build configurations. These include X86_FS and MIPS_SE, for instance. configs - Simulation configuration scripts which are written in python, described in more detail later. The files in this directory help make writing configurations easier by providing some basic prepackaged functionality, and include a few examples which can be used directly or as a starting point for your own scripts. ext - Things gem5 depends on but which aren’t actually part of gem5. Specifically, dependencies that are harder to find, not likely to be available, or where a particular version is needed. src - gem5 source code.  arch - ISA implementations.  generic - Common files for use in other ISAs. isa_parser.py - Parser that interprets ISA descriptions. ISA directories - The files associated with the given ISA.  OS directories - Code for supporting an ISA/OS combination, generally in SE mode. isa - ISA description files.   base - General data structures and facilities that could be useful for another project.  loader - Code for loading binaries and reading symbol tables. stats - Code for keeping statistics and writing the data to a file or a database. vnc - VNC support.  cpu - CPU models. dev - Device models.  ISA directories - Device models specific to the given ISA  doxygen - Doxygen templates \u0026amp; output kern - Operating system specific but architecture independent code (e.g. types of data structures).  OS directories - Code specific to the given simulated operating system.  mem - Memory system models and infrastructure.  cache - Code that implements a cache model in the classic memory system. ruby - Code that implements the ruby memory model. protocol - Ruby protocol definitions. slicc - The slicc compiler.  python - Python code for configuration and higher level functions. sim - Code that implements basic, fundamental simulator functionality.  system - Low level software like firmware or bootloaders for use in simulated systems.  alpha - Alpha console and palcode. arm - A simple ARM bootloader.  tests - Files related to gem5’s regression tests.  configs - General configurations used for the tests. test-progs - \u0026ldquo;Hello world\u0026rdquo; binaries for each ISA, other binaries are downloaded separately. quick, long - Quick and long regression inputs, reference outputs, and test specific configuration files, arranged per test.    util - Utility scripts, programs and useful files which are not part of the gem5 binary but are generally useful when working on gem5.  Style rules All of the code in gem5 is expected to follow a set rules described in our style guide. These rules make the code more consistent which make it easier to read, maintain, and extend the code. Specific coding style has been defined for things such as Indentation and Line Breaks, Spacing, Naming of Variables \u0026amp; Classes, and M5 Status Messages. There are also Documentation Guidelines which you should follow which allow documentation to be generated automatically using the Doxygen system.\nGenerated files - where do they end up .m5 config files If you would like to set some gem5 parameters to a value by default you can create a .m5 directory within your home directory and inside place a file called options.py. Within this file you may set any gem5 command line option to a new default. For example placing options.stdout_file=simout in options.py will result in the simulators stdout always being re-directed to a file named simout.\n"
},
{
	"uri": "http://localhost/docs/non-default-python/",
	"title": "Using a non-default Python",
	"tags": [],
	"description": "",
	"content": " The particular installation of the Python interpreter that executes the SCons build scripts is also linked with M5 to execute runtime Python simulation scripts. (This correspondence is necessary since some Python code is shared between the build process and runtime execution.) As a result, you can get M5 to link with a non-default installation of the Python interpreter by using that instance of the interpreter to execute the SCons scripts. Typically this means either:\n rearranging your PATH so that scons finds the non-default \u0026lsquo;python\u0026rsquo; first (assuming that the version you want to use is called \u0026lsquo;python\u0026rsquo;), or explicitly invoking an alternative interpreter on the scons script, e.g.:  % python2.4 `which scons` [scons args]\nIf you\u0026rsquo;re using a standard Python installation that\u0026rsquo;s just not the default version (e.g., Python 2.4 is installed as /usr/bin/python2.4, but you have to invoke it as python2.4 because /usr/bin/python points to Python 2.3) then that\u0026rsquo;s typically all there is to it.\nHowever, if you built Python from source in some user directory, then it gets trickier. You can still use this custom Python to run scons using either of the techniques above, but you must also be sure that the same Python interpreter and Python library modules can be located at runtime too. Details of how to make this work depend on which of the (at least) three different ways of building M5 \u0026amp; Python you select: dynamic Python library with dynamic modules, static Python library with dynamic modules, or static Python library with static modules.\nDynamic (shared) Python library with dynamic modules This approach works well if you\u0026rsquo;ll be running M5 on the same system that you build it on, or on a set of systems that have matching directory structures (e.g., because your home directory is NFS-mounted in the same place on all of them). The steps are:\n When compiling Python from source, make sure you add the --enable-shared argument to configure so that the shared-library version of the interpreter is built. When running M5, you need to make sure the loader can find your Python shared library, typically by adding the directory to your LD_LIBRARY_PATH environment variable.  Note that the paths to the Python library modules are hard-coded into the interpreter when it is built, so once the shared library is found, nothing additional needs to be done to tell Python where those modules are.\nStatic Python library with dynamic modules This approach results in a larger M5 binary (since the Python interpreter is statically linked), but avoids the need to set LD_LIBRARY_PATH. However, Python library modules will still be loaded dynamically, so those files must be present at the same locations on the system where you run M5 as they were on the system where you built it.\nThere are two ways to force the linker to use the static library:\n Don\u0026rsquo;t add the --enable-shared argument to configure when you build Python. The static Python library gets bulit no matter what. If the linker can\u0026rsquo;t find the shared library, it will be forced to use the static version. Use the compiler/linker flag (e.g., -static for gcc/g++) to force all libraries to be linked statically.  You will probably also have to:\n tweak the path to the Python library in the m5/SConstruct file add some linker options so that Python symbols not needed at link time but needed later by dynamically loaded Python library modules don\u0026rsquo;t get stripped out; see 1.  Static Python library with static modules In theory, this approach avoids the need to have any additional Python files present when running M5. It could be useful if the system installation on which you\u0026rsquo;ll be running M5 is drastically different from that of the system where you\u0026rsquo;re building M5.\nThis approach builds on the static Python library approach above, but further includes all of the Python modules needed to run your scripts in the static Python library. This is accompished by editing the Modules/Setup file in the Python source distribution before you build Python. See the comments in that file for further instructions.\nIn practice, we haven\u0026rsquo;t tried this. Please let us know if you try it, whether you succeed or fail. I believe the static linking step will work for compiled-code modules (.so\u0026rsquo;s), but not for library modules written in Python, so it\u0026rsquo;s probably still not a complete solution for getting everything into a single executable file.\n"
},
{
	"uri": "http://localhost/docs/streamline/",
	"title": "Visualizing w/Streamline",
	"tags": [],
	"description": "",
	"content": " What is Streamline? ARM Streamline™ Performance Analyzer, is a component of ARM Development Studio 5, DS-5™, intended to provide in-depth visibilities to the software execution of the system and counters/statistics. Streamline can be used to visualize a gem5 simulation, with Linux/Android process information and gem5 statistics, with the help of some post-processing.\nHow do I get Streamline? ARM provides a Community Edition of Streamline to all active members of the gem5 community, as long as they agree to use it for analyzing gem5 outputs only and not use the license for commercial purposes.\nPlease visit this link for detailed instructions on obtaining Streamline.\nHow do I view my gem5 run in Streamline? Note: The gem5/Streamline integration is currently only supported with the ARM ISA.\n1. Before running\nCurrently Streamline works with Linux and Android guest OSes. The guest kernel needs to have the m5struct.patch applied so that gem5 can access certain information of threads/processes being simulated.\nWhen running the simulation, the enable_context_switch_stats_dump flag in LinuxArmSystem must be enabled. This will ensure that the gem5 statistics will be dumped at every context switch. Additionally, a system.tasks.txt file will be dumped in the output directory, which keeps track of the thread/process information.\n2. After running\nRun the post-processing script, util/streamline/m5stats2streamline.py, to generate a Streamline project folder from gem5 stats. A stat_config.ini file must be provided, which lists the gem5 stats to be included. A couple of sample stat config files are included (util/o3_stat_config.ini and util/atomic_stat_config.ini). Once the .apc folder is generated, it can be viewed using Streamline.\nMore information Slides from the First Annual gem5 User Workshop.\nFor any problem or comment, please email dam.sunwoo@arm.com.\n"
},
{
	"uri": "http://localhost/developer/statistics/",
	"title": "Statistics",
	"tags": [],
	"description": "",
	"content": " If you\u0026rsquo;re looking at this page, we\u0026rsquo;ll assume you sort of know the ins and outs of the simulator and are looking to instrument it in some way that is not already there. In that case, we have a nice stats package built into the simulator for you. One really good place to look for examples of usage is src/unittest/stattest.cc. This has lots of examples of every type of stat. However, you will still want to look at what we have below.\nStats \u0026ldquo;Philosophy\u0026rdquo; \u0026ndash;\u0026gt; Initialization The philosophy of the stats package at the moment is to have a single base class called Stat which is merely a hook into every other aspect of the stat that may be important. Thus, this Stat base class has virtual functions to name, set precision for, set flags for, and initialize size for all the stats. For all Vector based stats, it is very important to do the initialization before using the stat so that appropriate storage allocation can occur. For all other stats, naming and flag setting is also important, but not as important for the actual proper execution of the binary. The way this is set up in the code is to have a regStats() pass in which all stats can be registered in the stats database and initialized. There is a separate pass (regFormulas) to register all Formulas since formulas often depend other stats that must have been initialized already.\nThus, to add your own stats, just add them to the appropriate class\u0026rsquo; data member list, and be sure to initialize/register them in that class\u0026rsquo; regStats function.\nHere is a list of the various initialization functions. Note that all of these return a Stat\u0026amp; reference, thus enabling a clean looking way of calling them all.\n init(various args) //this differs for different types of stats.  Scalar: does not have an init() Average: does not have an init() Vector: init(size_t) //indicates size of vector AverageVector: init(size_t) //indicates size of vector Vector2d: init(size_t x, size_t y) //rows, columns Distribution: init(min, max, bkt) //min refers to minimum value, max the maximum value, and bkt the size of the bkts. In other words, if you have min=0, max=15, and bkt=8, then 0-7 will go into bucket 0, and 8-15 will go into bucket 1. StandardDeviation: does not have an init() AverageDeviation: does not have an init() VectorDistribution: init(size, min, max, bkt) //the size refers to the size of the vector, the rest are the same as for Distributions. VectorStandardDeviation: init(size) //size refers to size of the vector VectorAverageDeviation: init(size) //size refers to size of the vector Formula: does not have an init()  name(const std::string name) //the name of the stat desc(const std::string desc) //a brief description of the stat precision(int p) //p refers to how many places after the decimal point to go. p=0 will force rounding to integers. prereq(const Stat \u0026amp;prereq) //this indicates that this stat should not be printed unless prereq has a non-zero value. (like if there are 0 cache accesses, don\u0026rsquo;t print cache misses, hits, etc.) subname(int index, const std::string subname) //this is for Vector based stats to give a subname to each index of the vector. subdesc(int index, const std::string subname) //also for Vector based stats, to give each index a subdesc. For 2d Vectors, the subname goes to each of the rows (x\u0026rsquo;s). The y\u0026rsquo;s can be named using a Vector2d member function ysubname, see code for details.  flags(FormatFlags f) //these are various flags you can pass to the stat, which i\u0026rsquo;ll describe below.\n none \u0026ndash; no special formatting total \u0026ndash; this is for Vector based stats, if this flag is set, the total across the Vector will be printed at the end (for those stats which this is supported). pdf \u0026ndash; This will print the probability distribution of a stat nozero \u0026ndash; This will not print the stat if its value is zero nonan \u0026ndash; This will not print the stat if it\u0026rsquo;s Not a Number (nan). cdf \u0026ndash; This will print the cumulative distribution of a stat  Below is an example of how to initialize a VectorDistribution:\nvector_dist.init(4,0,5,2) .name(\u0026quot;Dummy Vector Dist\u0026quot;) .desc(\u0026quot;there are 4 distributions with buckets 0-1, 2-3, 4-5\u0026quot;) .flags(nonan | pdf) ;  Stat Types Scalar The most basic stat is the Scalar. This embodies the basic counting stat. It is a templatized stat and takes two parameters, a type and a bin. The default type is a Counter, and the default bin is NoBin (i.e. there is no binning on this stat). It\u0026rsquo;s usage is straightforward: to assign a value to it, just say foo = 10;, or to increment it, just use ++ or += like for any other type.\nAverage This is a \u0026ldquo;special use\u0026rdquo; stat, geared toward calculating the average of something over the number of cycles in the simulation. This stat is best explained by example. If you wanted to know the average occupancy of the load-store queue over the course of the simulation, you\u0026rsquo;d need to accumulate the number of instructions in the LSQ each cycle and at the end divide it by the number of cycles. For this stat, there may be many cycles where there is no change in the LSQ occupancy. Thus, you could use this stat, where you only need to explicitly update the stat when there is a change in the LSQ occupancy. The stat itself will take care of itself for cycles where there is no change. This stat can be binned and it also templatized the same way Stat is.\nVector A Vector is just what it sounds like, a vector of type T in the template parameters. It can also be binned. The most natural use of Vector is for something like tracking some stat over number of SMT threads. A Vector of size n can be declared just by saying Vector\u0026lt;\u0026gt; foo; and later initializing the size to n. At that point, foo can be accessed as if it were a regular vector or array, like foo[7]++.\nAverageVector An AverageVector is just a Vector of Averages.\nVector2d A Vector2d is a 2 dimensional vector. It can be named in both the x and y directions, though the primary name is given across the x-dimension. To name in the y-dimension, use a special ysubname function only available to Vector2d\u0026rsquo;s.\nDistribution This is essentially a Vector, but with minor differences. Whereas in a Vector, the index maps to the item of interest for that bucket, in a Distribution you could map different ranges of interest to a bucket. Basically, if you had the bkt parameter of init for a Distribution = 1, you might as well use a Vector.\nStandardDeviation This stat calculates standard deviation over number of cycles in the simulation. It\u0026rsquo;s similar to Average in that it has behavior built into it, but it needs to be updated every cycle.\nAverageDeviation This stat also calculates the standard deviation but it does not need to be updated every cycle, much like Average. It will handle cycles where there is no change itself.\nVectorDistribution This is just a vector of distributions.\nVectorStandardDeviation This is just a vector of standard deviations.\nVectorAverageDeviation This is just a vector of AverageDeviations.\nFormula This is a Formula stat. This is for anything that requires calculations at the end of the simulation, for example something that is a rate. So, an example of defining a Formula would be:\nFormula foo = bar + 10 / num;\nThere are a few subtleties to Formula. If bar and num are both stats(including Formula type), then there is no problem. If bar or num are regular variables, then they must be qualified with constant(bar). This is essentially cast. If you want to use the value of bar or num at the moment of definition, then use constant(). If you want to use the value of bar or num at the moment the formula is calculated (i.e. the end), define num as a Scalar. If num is a Vector, use sum(num) to calculate its sum for the formula. The operation \u0026ldquo;scalar(num)\u0026rdquo;, which casts a regular variable to a Scalar, does no longer exist.\n"
},
{
	"uri": "http://localhost/developer/utility/",
	"title": "Utility Code",
	"tags": [],
	"description": "",
	"content": " Bitfield functions BitUnion classes FastAlloc FastAlloc has been removed from the code base since late May 2012. Instead the build scripts will pick up tcmalloc (a malloc() version used also at Google), the observed performance gain of replacing tcmalloc over fastalloc is around 6%.\nIntMath panic, fatal, warn, inform, hack: which? There are error and warning functions defined in src/base/misc.hh: panic(), fatal(), warn(), inform(), and hack(). The first two functional have nearly the same effect (printing an error message and terminating the simulation), and the latter three print a message and continue. Each function has a distinct purpose and use case. The distinction is documented in the comments in the header file, but is repeated here for convenience because people often get confused and use the wrong one.\n panic() should be called when something happens that should never ever happen regardless of what the user does (i.e., an actual m5 bug). panic() calls abort() which can dump core or enter the debugger. fatal() should be called when the simulation cannot continue due to some condition that is the user\u0026rsquo;s fault (bad configuration, invalid arguments, etc.) and not a simulator bug. fatal() calls exit(1), i.e., a \u0026ldquo;normal\u0026rdquo; exit with an error code. warn() should be called when some functionality isn\u0026rsquo;t necessarily implemented correctly, but it might work well enough. The idea behind a warn() is to inform the user that if they see some strange behavior shortly after a warn() the description might be a good place to go looking for an error. hack() should be called when some functionality isn\u0026rsquo;t implemented nearly as well as it could or should be but for expediency or history sake hasn\u0026rsquo;t been fixed. inform() Provides status messages and normal operating messages to the console for the user to see, without any connotations of incorrect behavior. For example it\u0026rsquo;s used when secondary CPUs being executing code on ALPHA.  The reasoning behind these definitions is that there\u0026rsquo;s no need to panic if it\u0026rsquo;s just a silly user error; we only panic if m5 itself is broken. On the other hand, it\u0026rsquo;s not hard for users to make errors that are fatal, that is, errors that are serious enough that the m5 process cannot continue.\nrandom number generation reference counting pointers "
},
{
	"uri": "http://localhost/cpu-models/o3/visualization/",
	"title": "Visualization",
	"tags": [],
	"description": "",
	"content": " This page contains information about different types of information visualization that is integrated or can be used with gem5.\nO3 Pipeline Viewer The o3 pipeline viewer is a text based viewer of the out-of-order CPU pipeline. It shows when instructions are fetched (f), decoded (d), renamed (n), dispatched (p), issued (i), completed \u0026copy;, and retired \u0026reg;. It is very useful for understanding where the pipeline is stalling or squashing in a reasonable small sequence of code. Next to the colorized viewer that wraps around is the tick the current instruction retired, the pc of that instruction, it\u0026rsquo;s disassembly, and the o3 sequence number for that instruction.\nTo generate output line you see above you first need to run an experiment with the o3 cpu:\n./build/ARM/gem5.opt --debug-flags=O3PipeView --debug-start= --debug-file=trace.out configs/example/se.py --cpu-type=detailed --caches -c  -m Then you can run the script to generate a trace similar to the above (500 is the number of ticks per clock (2GHz) in this case):\n./util/o3-pipeview.py -c 500 -o pipeview.out --color m5out/trace.out\nYou can view the output in color by piping the file through less:\nless -r pipeview.out\nWhen CYCLE_TIME (-c) is wrong, Right square brackets in output may not aligned to the same column. Default value of CYCLE_TIME is 1000. Be careful.\nThe script has some additional integrated help: (type ‘./util/o3-pipeview.py \u0026ndash;help’ for help).\n"
},
{
	"uri": "http://localhost/cpu-models/tracecpu/",
	"title": "TraceCPU",
	"tags": [],
	"description": "",
	"content": " Overview The Trace CPU model plays back elastic traces, which are dependency and timing annotated traces generated by the Elastic Trace Probe attached to the O3 CPU model. The focus of the Trace CPU model is to achieve memory-system (cache-hierarchy, interconnects and main memory) performance exploration in a fast and reasonably accurate way instead of using the detailed but slow O3 CPU model. The traces have been developed for single-threaded benchmarks simulating in both SE and FS mode. They have been correlated for 15 memory-sensitive SPEC 2006 benchmarks and a handful of HPC proxy apps by interfacing the Trace CPU with classic memory system and varying cache design parameters and DRAM memory type. In general, elastic traces can be ported to other simulation environments.\nPublication\n\u0026ldquo;Exploring System Performance using Elastic Traces: Fast, Accurate and Portable\u0026rdquo; Radhika Jagtap, Stephan Diestelhorst, Andreas Hansson, Matthias Jung and Norbert Wehn SAMOS 2016\nTrace generation and replay methodology\nElastic Trace Generation The Elastic Trace Probe Listener listens to Probe Points inserted in O3 CPU pipeline stages. It monitors each instruction and creates a dependency graph by recording data Read-After-Write dependencies and order dependencies between loads and stores. It writes the instruction fetch request trace and the elastic data memory request trace as two separate files as shown below.\nTrace file formats The elastic data memory trace and fetch request trace are both encoded using google protobuf.\n         required uint64 seq_num Instruction number used as an id for tracking dependencies   required RecordType type RecordType enum has values: INVALID, LOAD, STORE, COMP   optional uint64 p_addr Physical memory address if instruction is a load/store   optional uint32 size Size in bytes of data if instruction is a load/store   optional uint32 flags Flags or attributes of the access, ex. Uncacheable   required uint64 rob_dep Past instruction number on which there is order (ROB) dependency   required uint64 comp_delay Execution delay between the completion of the last dependency and the execution of the instruction   repeated uint64 reg_dep Past instruction number on which there is RAW data dependency   optional uint32 weight To account for committed instructions that were filtered out   optional uint64 pc Instruction address, i.e. the program counter   optional uint64 v_addr Virtual memory address if instruction is a load/store   optional uint32 asid Address Space ID    Elastic Trace fields in protobuf format\nA decode script in Python is available at util/decode_inst_dep_trace.py that outputs the trace in ASCII format.\nExample of a trace in ASCII\n        1,356521,COMP,8500::   2,35656,1,COMP,0:,1:   3,35660,1,LOAD,1748752,4,74,500:,2:   4,35660,1,COMP,0:,3:   5,35664,1,COMP,3000::,4   6,35666,1,STORE,1748752,4,74,1000:,3:,4,5   7,35666,1,COMP,3000::,4   8,35670,1,STORE,1748748,4,74,0:,6,3:,7   9,35670,1,COMP,500::,7    Each record in the instruction fetch trace has the following fields.\n         required uint64 tick Timestamp of the access   required uint32 cmd Read or Write (in this case always Read)   required uint64 addr Physical memory address   required uint32 size Size in bytes of data   optional uint32 flags Flags or attributes of the access   optional uint64 pkt_id Id of the access   optional uint64 pc Instruction address, i.e. the program counter    Instruction fetch trace fields in protobuf format\nThe decode script in Python at util/decode_packet_trace.py can be used to output the trace in ASCII format.\nCompile dependencies\nYou need to install google protocol buffer as the traces are recorded using this.\nsudo apt-get install protobuf-compiler sudo apt-get install libprotobuf-dev\nScripts and options  SE mode  build/ARM/gem5.opt [gem5.opt options] -d bzip_10Minsts configs/example/se.py [se.py options] \u0026ndash;cpu-type=arm_detailed \u0026ndash;caches \u0026ndash;cmd=$M5_PATH/binaries/arm_arm/linux/bzip2 \u0026ndash;options=$M5_PATH/data/bzip2/lgred/input/input.source -I 10000000 \u0026ndash;elastic-trace-en \u0026ndash;data-trace-file=deptrace.proto.gz \u0026ndash;inst-trace-file=fetchtrace.proto.gz \u0026ndash;mem-type=SimpleMemory    FS mode: Create a checkpoint for your region of interest and resume from the checkpoint but with O3 CPU model and tracing enabled  build/ARM/gem5.opt \u0026ndash;outdir=m5out/bbench ./configs/example/fs.py [fs.py options] \u0026ndash;benchmark bbench-ics build/ARM/gem5.opt \u0026ndash;outdir=m5out/bbench/capture_10M ./configs/example/fs.py [fs.py options] \u0026ndash;cpu-type=arm_detailed \u0026ndash;caches \u0026ndash;elastic-trace-en \u0026ndash;data-trace-file=deptrace.proto.gz \u0026ndash;inst-trace-file=fetchtrace.proto.gz \u0026ndash;mem-type=SimpleMemory \u0026ndash;checkpoint-dir=m5out/bbench -r 0 \u0026ndash;benchmark bbench-ics -I 10000000   Replay with Trace CPU The execution trace generated above is then consumed by the Trace CPU as illustrated below.\nThe Trace CPU model inherits from the Base CPU and interfaces with data and instruction L1 caches. A diagram of the Trace CPU explaining the major logic and control blocks is shown below.\nScripts and options  A trace replay script in the examples folder can be used to play back SE and FS generated traces  build/ARM/gem5.opt [gem5.opt options] -d bzip_10Minsts_replay configs/example/etrace_replay.py [options] \u0026ndash;cpu-type=trace \u0026ndash;caches \u0026ndash;data-trace-file=bzip_10Minsts/deptrace.proto.gz \u0026ndash;inst-trace-file=bzip_10Minsts/fetchtrace.proto.gz \u0026ndash;mem-size=4GB   "
},
{
	"uri": "http://localhost/cpu-models/o3/",
	"title": "O3",
	"tags": [],
	"description": "",
	"content": " The O3CPU is our new detailed model for the v2.0 release. It is an out of order CPU model loosely based on the Alpha 21264. This page will give you a general overview of the O3CPU model, the pipeline stages and the pipeline resources. We have made efforts to keep the code well documented, so please browse the code for exact details on how each part of the O3CPU works.\nPipeline stages The O3CPU has the following pipeline stages:\n Fetch  Fetches instructions each cycle, selecting which thread to fetch from based on the policy selected. This stage is where the DynInst is first created. Also handles branch prediction.  Decode  Decodes instructions each cycle. Also handles early resolution of PC-relative unconditional branches.  Rename  Renames instructions using a physical register file with a free list. Will stall if there are not enough registers to rename to, or if back-end resources have filled up. Also handles any serializing instructions at this point by stalling them in rename until the back-end drains.  Issue/Execute/Writeback  Our simulator model handles both execute and writeback when the execute() function is called on an instruction, so we have combined these three stages into one stage. This stage (IEW) handles dispatching instructions to the instruction queue, telling the instruction queue to issue instruction, and executing and writing back instructions.  Commit  Commits instructions each cycle, handling any faults that the instructions may have caused. Also handles redirecting the front-end in the case of a branch misprediction.   Pipeline resources Additionally it has the following structures:\n Branch predictor  Allows for selection between several branch predictors, including a local predictor, a global predictor, and a tournament predictor. Also has a branch target buffer and a return address stack.  Reorder buffer  Holds instructions that have reached the back-end. Handles squashing instructions and keeping instructions in program order.  Instruction queue  Handles dependencies between instructions and scheduling ready instructions. Uses the memory dependence predictor to tell when memory operations are ready.  Load-store queue  Holds loads and stores that have reached the back-end. It hooks up to the d-cache and initiates accesses to the memory system once memory operations have been issued and executed. Also handles forwarding from stores to loads, replaying memory operations if the memory system is blocked, and detecting memory ordering violations.  Functional units  Provides timing for instruction execution. Used to determine the latency of an instruction executing, as well as what instructions can issue each cycle.  Memory dependence prediction using store sets  Informs the IQ which memory instructions it predicts as ready to issue (in terms of memory ordering). In the Alpha models, memory operations have been atomic operations where the address calculation and memory access are bundled as one instruction. Because the effective addresses are not calculated separately, memory dependence prediction is necessary in order to give some idea of the order in which memory operations can execute.   Execute-in-execute model For the O3CPU, we\u0026rsquo;ve made efforts to make it highly timing accurate. In order to do this, we use a model that actually executes instructions at the execute stage of the pipeline. Most simulator models will execute instructions either at the beginning or end of the pipeline; SimpleScalar and our old detailed CPU model both execute instructions at the beginning of the pipeline and then pass it to a timing backend. This presents two potential problems: first, there is the potential for error in the timing backend that would not show up in program results. Second, by executing at the beginning of the pipeline, the instructions are all executed in order and out-of-order load interaction is lost. Our model is able to avoid these deficiencies and provide an accurate timing model.\nTemplate Policies The O3CPU makes heavy use of template policies to obtain a level of polymorphism without having to use virtual functions. It uses template policies to pass in an \u0026ldquo;Impl\u0026rdquo; to almost all of the classes used within the O3CPU. This Impl has defined within it all of the important classes for the pipeline, such as the specific Fetch class, Decode class, specific DynInst types, the CPU class, etc. It allows any class that uses it as a template parameter to be able to obtain full type information of any of the classes defined within the Impl. By obtaining full type information, there is no need for the traditional virtual functions/base classes which are normally used to provide polymorphism. The main drawback is that the CPU must be entirely defined at compile time, and that the templated classes require manual instantiation. See src/cpu/o3/impl.hh and src/cpu/o3/cpu_policy.hh for example Impl classes.\nISA independence The O3CPU has been designed to try to separate code that is ISA dependent and code that is ISA independent. The pipeline stages and resources are all mainly ISA independent, as well as the lower level CPU code. The ISA dependent code implements ISA-specific functions. For example, the AlphaO3CPU implements Alpha-specific functions, such as hardware return from error interrupt (hwrei()) or reading the interrupt flags. The lower level CPU, the FullO3CPU, handles orchestrating all of the pipeline stages and handling other ISA-independent actions. We hope this separation makes it easier to implement future ISAs, as hopefully only the high level classes will have to be redefined.\nInteraction with ThreadContext The ThreadContext provides interface for external objects to access thread state within the CPU. However, this is slightly complicated by the fact that the O3CPU is an out-of-order CPU. While it is well defined what the architectural state is at any given cycle, it is not well defined what happens if that architectural state is changed. Thus it is feasible to do reads to the ThreadContext without much effort, but doing writes to the ThreadContext and altering register state requires the CPU to flush the entire pipeline. This is because there may be in flight instructions that depend on the register that has been changed, and it is unclear if they should or should not view the register update. Thus accesses to the ThreadContext have the potential to cause slowdown in the CPU simulation.\nAnatomy of the pipeline Fetch Handling DynInsts Pipeline, PC/predicted target, dynamic information\nWires/delay Squashing Load/store handling Renaming etc. "
},
{
	"uri": "http://localhost/arch/arm/linux_kernel/",
	"title": "Linux Kernel",
	"tags": [],
	"description": "",
	"content": " This page contains instructions for building up-to-date kernels for gem5 running on ARM.\nPrerequisites These instructions are for running headless systems. That is a more \u0026ldquo;server\u0026rdquo; style system where there is no frame-buffer. The description has been created using the latest known-working tag in the repositories linked below, however the tables in each section list previous tags that are known to work. To built the kernels on an x86 host you\u0026rsquo;ll need ARM cross compilers and the device tree compiler. If you\u0026rsquo;re running a reasonably new version of Ubuntu or Debian you can get required software through\napt: apt-get install gcc-arm-linux-gnueabihf gcc-aarch64-linux-gnu device-tree-compiler  If you can\u0026rsquo;t use these pre-made compilers the next easiest way to obtain the required compilers from Linaro.\nDepending on the exact source of your cross compilers, the compiler names used below will required small changes.\nTo actually run the kernel, you\u0026rsquo;ll need to download or compile gem5\u0026rsquo;s bootloader. See the bootloaders section in this documents for details.\nLinux 4.x Newer gem5 kernels for ARM (v4.x and later) are based on the vanilla Linux kernel and typically have a small number of patches to make them work better with gem5. The patches are optional and you should be able to use a vanilla kernel as well. However, this requires you to configure the kernel yourself. Newer kernels all use the VExpress_GEM5_V1 gem5 platform for both AArch32 and AArch64. The required DTB files to describe the hardware to the OS ship with gem5. To build them, execute this command:\nmake -C system/arm/dt  Kernel Checkout To checkout the kernel, execute the following command:\ngit clone https://gem5.googlesource.com/arm/linux  The repository contains a tag per gem5 kernel releases and working branches for major Linux revisions. Check the project page for a list of tags and branches. The clone command will, by default, check out the latest release branch. To checkout the v4.4 branch, execute the following in the repository:\ngit checkout -b gem5/v4.4  AArch32 To compile the kernel, execute the following commands in the repository:\nmake ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- gem5_defconfig make ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- -j `nproc`  Testing the just built\nkernel: ./build/ARM/gem5.opt configs/example/fs.py --kernel=/tmp/linux-arm-gem5/vmlinux --machine-type=VExpress_GEM5_V1 \\ --dtb-file=$PWD/system/arm/dt/armv7_gem5_v1_1cpu.dtb  AArch64 To compile the kernel, execute the following commands in the repository:\nmake ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- gem5_defconfig make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- -j `nproc`  Testing the just built\nkernel: ./build/ARM/gem5.opt configs/example/fs.py --kernel=/tmp/linux-arm-gem5/vmlinux --machine-type=VExpress_GEM5_V1 \\ --dtb-file=$PWD/system/arm/dt/armv8_gem5_v1_1cpu.dtb --disk-image=linaro-minimal-aarch64.img  Legacy kernels (pre v4.x) Older gem5 kernels for ARM (pre v4.x) are based on Linaro\u0026rsquo;s Linux kernel for ARM. These kernels use either the VExpress_EMM (AArch32) or VExpress_EMM64 (AArch64) gem5 platform. Unlike the newer kernels, there is a separate AArch32 and AArch64 kernel repository and the device tree files are shipped with the kernel.\n32 bit kernel (AArch32) These are instructions to generate a 32-bit ARM Linux binary.\nTo checkout the aarch32 kernel, execute the following command:\ngit clone https://gem5.googlesource.com/arm/linux-arm-legacy  The repository contains a tag per gem5 kernel release. Check the project page for a list of branches and release tags. To checkout a tag, execute the following in the repository:\ngit checkout -b TAGNAME  To compile the kernel, execute the following commands in the\nrepository: make ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- vexpress_gem5_server_defconfig make ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- -j `nproc`  Testing the just built\nkernel: ./build/ARM/gem5.opt configs/example/fs.py --kernel=/tmp/linux-arm-gem5/vmlinux \\ --machine-type=VExpress_EMM --dtb-file=/tmp/linux-arm-gem5/arch/arm/boot/dts/vexpress-v2p-ca15-tc1-gem5.dtb  64 bit kernel (AArch64) These are instructions to generate a 64-bit ARM Linux binary.\nTo checkout the aarch64 kernel, execute the following command:\ngit clone https://gem5.googlesource.com/arm/linux-arm64-legacy  The repository contains a tag per gem5 kernel release. Check the project page for a list of branches and release tags. To checkout a tag, execute the following in the repository:\ngit checkout -b TAGNAME make ARCH=arm64 CROSS_COMPILE=aarch64-none-elf- gem5_defconfig make ARCH=arm64 CROSS_COMPILE=aarch64-none-elf- -j4  Testing the just built\nkernel: ./build/ARM/gem5.opt configs/example/fs.py --kernel=/tmp/linux-arm64-gem5/vmlinux --machine-type=VExpress_EMM64 \\ --dtb-file=/tmp/linux-arm64-gem5/arch/arm64/boot/dts/aarch64_gem5_server.dtb --disk-image=linaro-minimal-aarch64.img  Bootloaders There are two different bootloaders for gem5. One of 32-bit kernels and one for 64-bit kernels. They can be compiled using the following command:\nmake -C system/arm/simple_bootloader make -C system/arm/aarch64_bootloader/  Once you have compiled the binaries, put them in the binaries directory in your M5_PATH.\n"
},
{
	"uri": "http://localhost/docs/useful-external/",
	"title": "Useful external docs",
	"tags": [],
	"description": "",
	"content": " External ARM Documents  ARM Assembly Reference Sheet Thumb Assembler Reference Sheet GNU assembler Reference Sheet  External X86 documentation  AMD documentation Intel documentation Sandpile.org \u0026ndash; Handy collection of documentation about x86 INT 15h,AX=E820 \u0026ndash; Information about the INT 15h,AX=E820 BIOS service routine  External SPARC documentation  SPARCV9.pdf \u0026ndash; SPARC v9 architecture manual UA2005-current-draft-HP-EXT.pdf \u0026ndash; UltraSPARC Architecture 2005, Hyperprivileged edition UA2005-current-draft-P-EXT.pdf \u0026ndash; UltraSPARC Architecture 2005, Privileged edition T1 Specific Docs UA2007-current-draft-HP-EXT.pdf \u0026ndash; UltraSPARC Architecture 2007, Hyperprivileged edition T2 Specific Docs psABI3rd.pdf \u0026ndash; SYSTEM V APPLICATION BINARY INTERFACE - SPARC Processor Supplement - Third Edition 64.psabi.1.35.ps \u0026ndash; SYSTEM V APPLICATION BINARY INTERFACE - SPARC Version 9 Processor Supplement  "
},
{
	"uri": "http://localhost/docs/software_not_in_git/",
	"title": "Software not in git",
	"tags": [],
	"description": "",
	"content": " Full-System Stuff You would need one or more of the following files to full system simulations using gem5. If you download these files, read this page for instructions on how to install these files.\n ARM  ARM Full-System Files \u0026ndash; Pre-compiled kernel and disk images for 32-bit and 64-bit ARM simulation. Updated October 2014. There kernels all support PCIe devices and the 64-bit kernels support \u0026gt;2GB of DRAM. Legacy ARM Full System Files \u0026ndash; A collection of previous ARM files that have been distributed. Anyone getting started with ARM and gem5 should use the above link. BBench for gem5 \u0026ndash; Full-system Android files and BBench, a web-browser benchmark. AsimBench for gem5 \u0026ndash; Full-system Android files for AsimBench, a benchmark suite containing various types of mobile applications.  X86  Full System Files \u0026ndash; The kernel used for regressions, an SMP version of it, and a disk image config files \u0026ndash; Config files for both of the above kernels, 2.6.25.1 and 2.6.28.4  (The mkblankimage.sh script to create a blank disk image that used to be downloadable here is now included in the m5 repository, in the util directory.) Alpha  Full System Files \u0026ndash; Pre-compiled Linux kernels, PALcode/Console code, and a filesystem  Unchanged since M5 2.0 beta 3. If you already have these you don\u0026rsquo;t need them again.  linux-dist \u0026ndash; Everything you need to create your own disk image and compile everything in it from scratch   Benchmarks  For information about running Android on gem5 and using the web browser benchmark, see BBench-gem5. For information about running the AsimBench benchmark on Android with gem5, see AsimBench for more information. For information about using the DaCapo benchmarks on gem5 see the DaCapo benchmarks page for more information. SPLASH benchmarks \u0026ndash; See the Splash benchmarks page for more information.  Pre-compiled Cross-compilers Externally supplied cross compilers:\n Ubuntu users can simply install ARM compilers with the crossbuild-essential-armhf and libc6-dev-armhf-armel-cross packages for 32-bit ARM and crossbuild-essential-arm64 and libc6-dev-arm64-cross for 64-bit ARM. MIPS cross compilers from CodeSourcery  All generated with crosstool for x86 linux hosts/linux targets\n Alpha: gcc-3.4.3, gcc-4.3.2, glibc-2.6.1 (NPTL,x86/64), gcc-4.3.2, glibc-2.6.1 (NPTL,x86/32) SPARC64  "
},
{
	"uri": "http://localhost/docs/alternate_source/",
	"title": "Alternate Sources",
	"tags": [],
	"description": "",
	"content": " The latest gem5 source code is available via our Git repository host at https://gem5.googlesource.com. It is strongly recommend that you get a copy of gem5 by using git. You can get more info about the repository and git here. In additional to the main git repository, there is a mirror on GitHub (we can\u0026rsquo;t currently accept pull requests on GitHub) and a Mercurial mirror. Keep in mind that the mirrors are read only. New code can only be submitted to the main git repository.\nMercurial mirror NOTE: The Mercurial mirror is read-only.\n Install Mercurial if you don\u0026rsquo;t have it already. This is available in the mercurial package on Ubuntu and OSX Brew.   Clone the repository: hg clonehttp://repo.gem5.org/gem5   After you clone the repository you can update it by typing hg pull and hg update.  TAR dumps If you want to download gem5 without installing Mercurial, you can get a tarball. But it will be more difficult to merge in changes when you need to update to new version. Tagged stable versions can be downloaded from GitHub.\n"
},
{
	"uri": "http://localhost/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " About The gem5 simulator is a modular platform for computer-system architecture research, encompassing system-level architecture as well as processor microarchitecture.\nFeatures Multiple interchangeable CPU models. gem5 provides four interpretation-based CPU models: a simple one-CPI CPU; a detailed model of an in-order CPU, and a detailed model of an out-of-order CPU. These CPU models use a common high-level ISA description. In addition, gem5 features a KVM-based CPU that uses virtualisation to accelerate simulation.\nEvent-driven memory system. gem5 features a detailed, event-driven memory system including caches, crossbars, snoop filters, and a fast and accurate DRAM controller model, for capturing the impact of current and emerging memories, e.g. LPDDR3/4/5, DDR3/4, GDDR5, HBM1/2/3, HMC, WideIO1/2. The components can be arranged flexibly, e.g., to model complex multi-level non-uniform cache hierarchies with heterogeneous memories.\nMultiple ISA support gem5 decouples ISA semantics from its CPU models, enabling effective support of multiple ISAs. Currently gem5 supports the Alpha, ARM, SPARC, MIPS, POWER, RISC-V and x86 ISAs. However, all guest platforms aren\u0026rsquo;t supported on all host platforms (most notably Alpha requires little-endian hardware).\nHomogeneous and heterogeneous multi-core The CPU models and caches can be combined in arbitrary topologies, creating homogeneous, and heterogeneous multi-core systems. A MOESI snooping cache coherence protocol keeps the caches coherent.\nFull-system capability  Alpha: gem5 models a DEC Tsunami system in sufficient detail to boot unmodified Linux 2.4\u0026frasl;2.6, FreeBSD, or L4Ka::Pistachio. We have also booted HP/Compaq\u0026rsquo;s Tru64 5.1 operating system in the past, though we no longer actively maintain that capability. ARM: gem5 can model up to 64 (heterogeneous) cores of a Realview ARM platform, and boot unmodified Linux and Android with a combination of in-order and out-of-order CPUs. The ARM implementation supports 32 or 64-bit kernels and applications. SPARC: The gem5 simulator models a single core of a UltraSPARC T1 processor with sufficient detail to boot Solaris in a similar manner as the Sun T1 Architecture simulator tools (building the hypervisor with specific defines and using the HSMID virtual disk driver). x86: The gem5 simulator supports a standard PC platform  Application-only support In application-only (non-full-system) mode, gem5 can execute a variety of architecture/OS binaries with Linux emulation.\nMulti-system capability Multiple systems can be instantiated within a single simulation process. In conjunction with full-system modeling, this feature allows simulation of entire client-server networks.\nPower and energy modeling gem5’s objects are arranged in OS-visible power and clock domains, enabling a range of experiments in power- and energy-efficiency. With out-of-the-box support for OS-controller Dynamic Voltage and Frequency (DVFS) scaling, gem5 provides a complete platform for research in future energy-efficient systems. See how to run your own DVFS experiments.\nA trace-based CPU CPU model that plays back elastic traces, which are dependency and timing annotated traces generated by a probe attached to the out-of-order CPU model. The focus of the Trace CPU model is to achieve memory-system (cache-hierarchy, interconnects and main memory) performance exploration in a fast and reasonably accurate way instead of using the detailed CPU model.\nCo-simulation with SystemC. gem5 can be included in a SystemC simulation, effectively running as a thread inside the SystemC event kernel, and keeping the events and timelines synchronized between the two worlds. This functionality enables the gem5 components to interoperate with a wide range of System on Chip (SoC) component models, such as interconnects, devices and accelerators. A wrapper for SystemC Transaction Level Modelling (TLM) is provided.\nA NoMali GPU model. gem5 comes with an integrated NoMali GPU model that is compatible with the Linux and Android GPU driver stack, and thus removes the need for software rendering. The NoMali GPU does not produce any output, but ensures that CPU-centric experiments produce representative results.\nLicensing The gem5 simulator is released under a Berkeley-style open source license. Roughly speaking, you are free to use our code however you wish, as long as you leave our copyright on it. For more details, see the LICENSE file included in the source download. Note that the portions of gem5 derived from other sources are also subject to the licensing restrictions of the original sources.\nPublications A list of publications using the gem5 simulator is also available. Please append to the list if you publish a paper using gem5.\nIf you use gem5 in your research, we would appreciate a citation to, The gem5 Simulator, from the May 2011 issue of ACM SIGARCH Computer Architecture News in any publications you produce. In addition, please cite the specific features of gem5 that you are using as part of your research.\nAcknowledgments The gem5 simulator has been developed with generous support from several sources, including the National Science Foundation, AMD, ARM, Hewlett-Packard, IBM, Intel, MIPS, and Sun. Individuals working on gem5 have also been supported by fellowships from Intel, Lucent, and the Alfred P. Sloan Foundation. This material is based upon work supported by the National Science Foundation under the following grants: CCR-0105503, CCR-0219640, CCR-0324878, EAI/CNS-0205286, and CCR-0105721.\nAny opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation (NSF) or any other sponsor.\n"
},
{
	"uri": "http://localhost/deprecated/code-locations/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Full system code locations are as follows:\n /sim/system - Architecture and OS independent code /arch//system.(cc|hh) - Architecture dependent, OS independent code /arch///system.(cc|hh) - - Architecture dependent, OS dependent code /kern//* - Architecture independent, OS dependent code  Very little of the code that was in /kern/ was actually architecture independent. Even things that at first glance appeared to be, actually patched architecture dependent parts of the kernel. Since there wasn\u0026rsquo;t much shared code, I didn\u0026rsquo;t feel any any large amount of effort should be put into sharing litterally 3 lines so unless we find a place where there is a lot of code, os dependent architecture independent code should go in it\u0026rsquo;s proper directory in a namespace for that OS and simply be called by the source architecture and os dependend files.\n"
},
{
	"uri": "http://localhost/developer/io/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " title: \u0026ldquo;I/O Base Classes\u0026rdquo; date: 2018-05-13T18:51:37-04:00\ndraft: false The base classes in src/dev/io_device.* allow devices to be created with reasonable ease. The classes and virtual functions that must be implemented are listed below. Before reading the following it will help to be familiar with the Memory_System.\nPioPort The PioPort class is a programmed I/O port that all devices that are sensitive to an address range use. The port takes all the memory access types and roles them into one read() and write() call that the device must respond to. The device must also provide the addressRanges() function with which it returns the address ranges it is interested in. An extra sendTiming() function is implemented which takes an delay. In this way the device can immediately call sendTiming(pkt, time) after processing a request and the request will be handled by the port even if the port bus the device connects to is blocked. Because of this a PIO device should not call Port::sendTiming() only PioPort::sendTiming() should be used. sendTiming() causes a new PioPort::SendEvent is created and when the event time is reached the packet is sent. If the send is not successful then the packet is placed on a the transmit list and resent when recvRetry() is called.\nIf desired a device could have more than one PIO port. However in the normal case it would only have one port and return multiple ranges when the addressRange() function is called. The only time multiple PIO ports would be desirable is if your device wanted to have separate connection to two memory objects.\nPioDevice This is the base class which all devices senstive to an address range inherit from. There are three pure virtual functions which all devices must implement addressRanges(), read(), and write(). The magic to choose which mode we are in, etc is handled by the PioPort so the device doesn\u0026rsquo;t have to bother.\nParameters for each device should be in a Params struct derived from PioDevice::Params.\nBasicPioDevice Since most PioDevices only respond to one address range BasicPioDevice provides an addressRanges() and parameters for the normal pio delay and the address to which the device responds to. Since the size of the device normally isn\u0026rsquo;t configurable a parameter is no used for this and anything that inherits from this class is expected to write it\u0026rsquo;s size into pioSize in its constructor.\nDmaPort The DmaPort is used only for device mastered accesses. The recvFunctional(), and recvAtomic() methods are defined to panic as the device should never receive a request on this port. the recvTiming() method must be available to responses (nacked or not) to requests it makes. The port has two public methods dmaPending() which returns if the dma port is busy (e.g. It is still trying to send out all the pieces of the last request). All the code to break requests up into suitably sized chunks, collect the potentially multiple responses and respond to the device is accessed through dmaAction(). A command, start address, size, completion event, and possibly data is handed to the function which will then execute the completion events process() method when the request has been completed. Internally the code uses DmaRequestState to manage what blocks it has received and to know when to execute the completion event.\nDmaDevice This is the base class from which a DMA non-pci device would inherit from, however none of those exist currently within M5. The class does have some methods dmaWrite(), dmaRead() that select the appropriate command from a DMA read or write operation.\nPCI devices Explanation of platforms and systems, how they’re related, and what they’re each for "
},
{
	"uri": "http://localhost/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]